\documentclass[a4paper,twoside]{article}
\usepackage[latin1]{inputenc}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SciTePress}
\usepackage[small]{caption}
\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\newcommand{\metodo}{L-Co-R}

\begin{document}

\title{The {\metodo} co-evolutionary algorithm: a comparative analysis in medium-term time-series forecasting problems}

\author{\authorname{E. Parras-Gutierrez\sup{1}, V.M. Rivas\sup{1} and J.J. Merelo\sup{2}}
\affiliation{\sup{1}Department of Computer Sciences, University of Jaen, Campus Las Lagunillas s/n, 23071, Jaen, Spain}
\affiliation{\sup{2}Department of Computers, Architecture and Technology, University of Granada, C/ Periodista Daniel Saucedo s/n, 18071, Granada, Spain}
\email{\{eparrasg, vrivas\}@vrivas.es, jmerelo@geneura.ugr.es}
}


\keywords{Time series forecasting, Co-evolutionary algorithms, Neural Networks, Significant lags}

\abstract{This paper presents an experimental study in which the
  effectiveness of the {\metodo} method is tested.
 {\metodo} is a
  co-evolutionary algorithm to time series forecasting that evolves, on
  one hand, RBFNs building an appropriate architecture of net, and on
  the other hand, sets of time lags that represents the time series in
  order to perform the forecasting using, at the same time, its own
  forecasted values. This coevolutive approach makes possible to
  divide the main problem into two subproblems where every individual
  of one population cooperates with the individuals of the other. The goal of this work is to analyze the results obtained
  by {\metodo} comparing with other methods from the time series
  forecasting field. For that, 20 time series and 5 different methods
  found in the literature have been selected, and 3 distinct quality
  measures have been used to show the results. Finally, a statistical
  study confirms the good results of {\metodo} in most cases.} 


\onecolumn \maketitle \normalsize \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

Formally defined, a time series is a set of observed values from a
variable along time in regular periods (for instance, every day, every
month or every year) \cite{Pena2005}. Accordingly, the work of
forecasting in a time series can be defined as the task of predicting
successive values of the variable in time spaced based on past and
present observations. 

For many decades, different approaches have been used for to
modelling and forecasting time series. These techniques can be
classified into three different areas: descriptive traditional
technologies, linear and nonlinear modern models, and soft computing
techniques. From all developed method, ARIMA, proposed by Box and
Jenkins \cite{BoxJenk}, is possibly the most widely known and used. 
Nevertheless, it yields simplistic linear models, 
being unable to find subtle patterns in the time series data. 

New methods based on artificial neural networks, such as the one used
in this paper, on the other hand, can generate more complex
models that are able to grasp those subtle variations. 

The {\metodo} method \cite{Parras2012Softcomputing}, developed inside
the field of ANNs, makes jointly use of Radial Basis Function Networks
(RBFNs) and EAs to automatically
forecast any given time series.
 Moreover, {\metodo} designs adequate neural networks and selects the
 time lags that will be used in the prediction, in a coevolutive
 \cite{Castillo2003} 
 approach that allows to separate the main problem in two dependent
 subproblems. The algorithm evolves two subpopulations based on a
 cooperative scheme in which every individual of a subpopulation
 collaborates with individuals from the other subpopulation in order
 to obtain good solutions. 

While previously work \cite{Parras2012Softcomputing} was focused on 1-step ahead prediction, the main goal of this one is to analyze the effectiveness of the {\metodo} method in the medium-term horizon, using the own previously predicted values to perform next predictions. For this reason, {\metodo} has been tested over 20 databases, taken from real world, or used in well-known research publications and time series competition. As section \ref{sec:experimentation} shows, the method has been compared against 5 time series forecasting methods.

The rest of the paper is organized as follows: section
\ref{sec:preliminaries} introduces some preliminary topics related to
this research; section \ref{sec:method} describes the method
{\metodo}; section \ref{sec:experimentation} presents the
experimentation and the statistical study carried out, while section \ref{sec:conclusions} presents some conclusions of the work.


\section{\uppercase{Preliminaries}}
\label{sec:preliminaries}

\noindent 


Approaches proposed in time series forecasting can be mainly grouped as linear and nonlinear models. Methods like exponential smoothing methods \cite{Winters1960}, simple exponential smoothing, Holt's linear methods, some variations of the Holt-Winter's methods, State space models \cite{Snyder1985}, and ARIMA models \cite{BoxJenk}, have stand out from linear methods, used chiefly for modelling time series. Nonlinear models arose because linear models were insufficient in many real applications; between nonlinear methods it can be found regime-switching models, which comprise the wide variety of existing threshold autoregressive models \cite{Tong1978}.
% as: self-exciting models \cite{Tong1983}, smooth transition models \cite{Chan1986}, and continuous-time models \cite{Brockwell1992157}, among others. 
Nevertheless, soft computing approaches were developed in order to save disadvantages of nonlinear models like the lack of robustness in complex model and the difficulty to use \cite{Clements2004}.

ANNs have also been successfully applied  \cite{Jain2007}
and recognized as an important tool for time-series forecasting. 
Within ANNs, the utilization of RBFs as activation functions were
considered by works as \cite{Broomhead88} and \cite{Rivas04}, 
% and applied to time series by, for instance, Carse and Fogarty \cite{Carse1996}, 
% and Whitehead and Choate \cite{Whitehead96}. Later works like the ones by
while Harpham and Dawson \cite{Harpham06} or  Du \cite{Du2008} focused on
RBFNs for time series forecasting. 


On the other hand, an issue that must be taken into account when
working with time series is the correct choice of the time lags for
representing the series. Takens' theorem \cite{Takens1980} establishes
that if $d$, a $d$-dimensional space where $d$ is the minimum
dimension capable of representing such a relationship, is sufficiently
large is possible to build a state space using the correct time lags
and if this space is correctly rebuilt also guarantees that the
dynamics of this space is topologically identical to the dynamics of
the real systems state space. 

Many methods are based in Takens' theorem (like \cite{Luko2010}) but, in general, the approaches found in the literature consider the lags selection as a pre or post-processing or as a part of the learning process \cite{Araujo2010a},\cite{Maus2011}. In the {\metodo} method the selection of the time lags is jointly faced along with the design process, thus it employs co-evolution to simultaneously solve these problems.

Cooperative co-evolution \cite{Potter94} has also been used in order to train ANNs to design neural network ensembles \cite{Garcia-Pedrajas05} and RBFNs \cite{Li08}. But in addition, cooperative co-evolution is utilized in time series forecasting in works as the one by Xin \cite{Xin10}. 


\section{\uppercase{Description of the method}}
\label{sec:method}

\noindent This section describes {\metodo} \cite{Parras2012Softcomputing}, a co-evolutionary algorithm developed to minimize the error obtained for automatically time series forecasting. The algorithm works building at the same time RBFNs and sets of lags that will be used to predict future values. For this task, {\metodo} is able to simultaneously evolve two populations of different individual species, in which any member of each population can cooperate with individuals from the other one in order to generate good solutions, that is, each individual represents itself a possible solution to the subproblem. Therefore, the algorithm is composed of the following two populations:

\begin{itemize}
  \item Population of RBFNs: it consists of a set of RBFNs which evolves to design a suitable architecture of the network. This population employs real codification so every individual represent a set of neurons (RBFs) that composes the net. Each neuron of the net is defined by a center (a vector with the same dimension as the inputs) and a radius. The exact dimension of the input space is given by an individual of the population of lags (the one chosen to evaluate the net). During the evolutionary process neurons can grow or decrease since the number of neurons is variable, and centers and radius can also be modified by means of muatation.

  \item Population of lags: it is composed of sets of lags evolves to forecast future values of the time series. The population uses a binary codification scheme thus each gene indicates if that specific lag in the time series will be utilized in the forecasting process. The length of the chromosome is set at the beginning corresponding with the specific parameter, so that it cannot vary its size during the execution of the algorithm.
\end{itemize}


As the fundamental objective, {\metodo} forecasts any time series for any horizon and builds appropriate RBFNs designed with suitable sets of lags, reducing any hand made preprocessing step. Figure \ref{generalscheme} describes the general scheme of the algorithm {\metodo}.

\begin{figure}
\centering
\begin{tabular}{|l|}
\hline
\\
Trend preprocessing \\
t = 0;  \\
\emph{initialize} P\_lags(t); \\
\emph{initialize} P\_RBFNs(t);  \\
\emph{evaluate} individuals in P\_lags(t); \\
\emph{evaluate} individuals in P\_RBFNs(t); \\
while termination condition not satisfied do \\
begin  \\
\ \ \ t = t+1;  \\
\ \ \ /* Evolve population of lags */ \\
\ \ \ for i=0 to max\_gen\_lags do \\
\ \ \ begin  \\
\ \ \ \ \ set threshold;  \\
\ \ \ \ \ \emph{select} P\_lags'(t) from P\_lags(t); \\
\ \ \ \ \ \emph{apply} genetic operators in P\_lags'(t); \\
\ \ \ \ \ /* Evaluate P\_lags'(t) */ \\
\ \ \ \ \ \ \ \emph{choose} collaborators from P\_RBFNs(t); \\
\ \ \ \ \ \ \ \emph{evaluate} individuals in P\_lags'(t); \\
\ \ \ \ \ \emph{replace} individuals P\_lags(t) with P\_lags'(t); \\
\ \ \ \ \ if threshold $<$ 0  \\
\ \ \ \ \ begin  \\
\ \ \ \ \ \ \ \ \emph{diverge} P\_lags(t); \\
\ \ \ \ \ end \\
\ \ \ end  \\
\ \ \ /* Evolve population of RBFNs */ \\
\ \ \ for i=0 to max\_gen\_RBFNs do \\
\ \ \ begin \\
\ \ \ \ \ \emph{select} P\_RBFNs'(t) from P\_RBFNs(t); \\
\ \ \ \ \ \emph{apply} genetic operators in P\_RBFNs'(t); \\
\ \ \ \ \ /* Evaluate P\_RBFNs'(t) */  \\
\ \ \ \ \ \ \ \emph{choose} collaborators from P\_lags(t); \\
\ \ \ \ \ \ \ \emph{evaluate} individuals in P\_RBFNs'(t); \\
\ \ \ \ \ \emph{replace} individuals with P\_RBFNs'(t);  \\
\ \ \ end  \\
end  \\

\emph{train} models and select the best one \\
forecast test values with the final model \\
Trend postprocessing \\
\hline
\end{tabular}
\caption{General scheme of method {\metodo}.}
\label{generalscheme}
\end{figure}

{\metodo} performs a process to automatically remove the trend of the times series to work with, if necessary. This procedure is divided into two main phases: preprocessing, which takes places at the beginning of the algorithm, and post-processing, at the end of co-evolutionary process. Basically, the algorithm checks if the time series includes trend and, in affirmative case, the trend is removed.

The performance of {\metodo} starts with the creation of the two initial populations, randomly generated for the first generation; then, each individual of the populations is evaluated. The {\metodo} algorithm uses a sequential scheme in which only one population is active, so the two population take turns in evolving. Firstly, the evolutionary process of the population of lags occurs: the individuals which will belong to the subpopulation are selected; following the CHC scheme \cite{Eshelman}, genetic operators are applied; the collaborator for every individual is chosen from the population of RBFNs; and the individuals are evaluated again and assigned the result as fitness. After that, the best individuals from the subpopulation will replace the worst individuals of the population. During the evolution, the population of lags checks that al least one gene of the chromosome must be set to one because necessarily the net needs one input to obtained the forecasted value.

In the second place, the population of RBFNs starts the evolutionary process. For the first generation, every net in the population has a number of neurons randomly chosen which may not exceed a maximum number previously fixed. As in population of lags, the individuals for the subpopulation are selected, the genetic operators are applied, every individual chooses the collaborator from the population of lags, and then, the individuals are evaluated and the result is assigned as fitness. Fitness function is defined by the inverse of the root mean squared error
At the end of the co-evolutionary process, two models formed by a set of lags (from the first population) and a neural network (from the second population) are obtained. On the one hand, a model is composed of the best set of lags and its best collaborator, and on the other hand, the other model is composed of the best net found and its best collaborator. Then, the two models are trained again and the final model chosen is the one that obtains the best fitness. This final model obtains the future values of the time series used for the prediction, and then, forecasted data will be used to find next values.

The collaboration scheme used in {\metodo} is the best collaboration scheme \cite{Potter94}. 
Thus, every individual in any population chooses the best collaborator from the other population. Only at the beginning of the co-evolutionary process, the collaborator is selected randomly because the population has not been evaluated yet.

The method has a set of specific operators specially developed to work with individuals from every population. 
The operators used by {\metodo} are the followings:

\begin{itemize}
  \item{Population of RBFNs:} tournament selection, x\_fix crossover, four operators to mutate randomly chosen (C\_random, R\_random, Adder, and Deleter) and replacement of the worst individuals by the best ones of the subpopulation.
    
  \item{Population of lags:} elitist selection, HUX crossover operator, replacement of the worst individuals, and diverge (the population is restarted when it is blocked).
    
\end{itemize}


\section{\uppercase{Experimentation and statistical study}}
\label{sec:experimentation}

\noindent The main goal of the experiments is to study the behavior of the algorithm {\metodo} comparing with other 5 methods found in the literature and for 3 different quality measures. 

\subsection{Experimental methodology}
\label{experimentalmethodoly}

\noindent As in \cite{Parras2012Softcomputing}, the experimentation has been carried out using 20 data bases, most of then taken from the INE\footnote{National Statistics Institute (http://www.ine.es/)}. The data represent observations from different activities and have different nature, size, and characteristics. The data bases have been labeled as: Airline, WmFrancfort, WmLondon, WmMadrid, WmMilan, WmNewYork, WmTokyo, Deceases, SpaMovSpec, Exchange, Gasoline, MortCanc, MortMade, Books, FreeHouPrize, Prisoners, TurIn, TurOut, TUrban, and HouseFin. The number of samples in every database is between 43 (for MortCanc) and 618 (for Gasoline, a database used in the NN3 competition).

To compare the effectiveness of {\metodo}, 5 additional methods have been used, all of them found
within the field of time series forecasting: Exponential smoothing
method (ETS), Croston, Theta, Random Walk (RW), and ARIMA \cite{HyndmanKhandakar:2008}. 

In order to test and compare the generalization capabilities of every method, databases have been split into training and test sets. Training sets have been given the first $75\%$ of the data, while test sets are composed by the remaining $25\%$ samples.

An open question when dealing with time series is the measure to be used in order to calculate the accuracy of the obtained predictions. Mean Absolute Percentage Error (MAPE) \cite{Bowerman2004} was intensively used until many other measures as Geometric Mean Relative Absolute Error, Median Relative Absolute Error, Symmetric Median and Median Absolute Percentage Error (MdAPE), or Symmetric Mean Absolute Percentage Error were proposed \cite{Makridakis2000}. However, a disadvantage was found in these measures, they were not generally applicable and can be infinite, undefined or can produce misleading results, as Hyndman and Koehler explained in their work \cite{Hyndman2006}. Thus, they proposed Mean Absolute Scaled Error (MASE) that is less sensitive to outliers, less variable on small samples, and more easily interpreted.

In this work, the measures used are MAPE (i.e., $mean(\mid p_t\mid)$), MASE (defined as $mean(\mid q_t\mid)$), and MdAPE (as $median(\mid p_t\mid)$ ), taking into account that $Y_t$ is the observation at time $t = {1,...,n}$; $F_t$ is the forecast of $Y_t$; $e_t$ is the forecast error (i.e. $e_t= Y_t - F_t$); $p_t = 100e_t/Y_t$ is the percentage error, and $q_t$ is determined as:

 \begin{center}
 \smallskip
 $q_t = \displaystyle\frac{e_t}{\displaystyle\frac{1}{n-1} \sum_{i=2}^n \mid Y_i - Y_{i-1} \mid }$
 \end{center}




Due to its stochastic nature, the results yielded by {\metodo} have been calculated as the average errors over 30 executions with every time series. For each execution, the following parameters are
used in the {\metodo} algorithm: lags population size=50, lags
population generations=5, lags chromosome size=10\%, RBFNs population
size=50, RBFNs population generations=10, validation rate=0.25,
maximum number of neurons of first generation=0.05, tournament size=3,
replacement rate=0.5, crossover rate=0.8, mutation rate=0.2, and total
number of generations=20. 

% Resultados

Tables \ref{tab:resultsMAPE}, \ref{tab:resultsMASE}, and \ref{tab:resultsMDAPE} show the results of the {\metodo} and the utilized methods to compare (ETS, Croston, Theta, RW, and ARIMA), for measures MAPE, MASE, and MdAPE, respectively (best results are emphasized with the character *). As mentioned before, every result indicated in the tables represent the average of 30 executions for each time series. With respect to MAPE, the {\metodo} algorithm obtains the best results in 15 of 20 time series used, as can be seen in table \ref{tab:resultsMAPE}. Regarding MASE, {\metodo} stands out yielding the best results for 5 time series; ETS, Croston and Theta for 3 time series; RW only for 2; and ARIMA for 4 time series; as can be observed in table \ref{tab:resultsMASE}. Concerning MdAPE, {\metodo} acquires better results than the other methods in 12 of 20 time series, as table \ref{tab:resultsMDAPE} shows. Thus, the {\metodo} algorithm is able to achieve a more accurate forecast in the most time series for any of the quality measures considered.


\begin{table*}[h]
\caption{Results of the methods {\metodo}, ETS, Croston, Theta, RW, and ARIMA, with respect to MAPE. Best result per database is marked with character *.}
\label{tab:resultsMAPE}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  Time series & {\metodo} & ETS & Croston & Theta & RW & ARIMA \\
  \hline Airline	&	30.380 *	&	274.770	&	72.606	&	141.452	&	137.986	&	53.636\\
  \hline WmFrancfort	&	16.423	&	17.393	&	40.544	&	22.745	&	25.169	&	12.136 *\\
  \hline WmLondon	&	2.860 *	&	5.383	&	27.682	&	10.136	&	13.397	&	5.212\\
  \hline WmMadrid	&	20.101	&	27.035	&	44.285	&	25.505	&	27.034	&	12.930 *\\
  \hline WmMilan	&	30.529 *	&	34.858	&	49.750	&	34.078	&	34.823	&	34.823\\
  \hline WmNewYork	&	8.259	&	7.182 *	&	30.297	&	14.669	&	18.073	&	7.536\\
  \hline WmTokyo	&	4.764 *	&	12.807	&	20.556	&	10.575	&	12.591	&	12.591\\
  \hline Deceases	&	5.981 *	&	8.002	&	7.472	&	7.264	&	8.040	&	8.040\\
  \hline SpaMovSpec	&	53.788 *	&	217.978	&	78.648	&	70.500	&	78.935	&	88.197\\
  \hline Exchange	&   43.044	&   46.025	&   31.121 *	&   39.138	&   33.631	&   45.254\\
  \hline Gasoline	&	1.654 *	&	7.986	&	9.587	&	6.701	&	7.974	&	9.359\\
  \hline MortCanc	&	1.137 *	&	12.979	&	32.489	&	5.889	&	6.256	&	5.440\\
  \hline MortMade	&	3.931 *	&	13.526	&	46.362	&	40.272	&	12.800	&	31.000\\
  \hline Books	&	13.787 *	&	23.588	&	23.122	&	22.360	&	22.640	&	23.476\\
  \hline FreeHouPrize	&	3.424 *	&	8.540	&	29.271	&	5.215	&	9.220	&	10.227\\
  \hline Prisoners	&   8.392	&   3.103 *	&   14.220	&   6.888	&   9.474   &	3.150\\
  \hline TurIn	&	1.357 *	&	7.074	&	11.234	&	7.084	&	7.110	&	6.377\\
  \hline TurOut	&	8.133 *	&	13.261	&	12.159	&	15.238	&	13.226	&	9.634\\
  \hline TUrban	&	2.734 *	&	11.957	&	9.067	&	8.949	&	10.116	&	9.291\\
  \hline HouseFin	&	16.452 *	&	22.296	&	21.548	&	19.947	&	22.887	&	19.555\\
  \hline
\end{tabular}
\end{table*}


\begin{table*}[h]
\caption{Results of the methods {\metodo}, ETS, Croston, Theta, RW, and ARIMA, with respect to MASE. Best result per database is marked with character *.}
\label{tab:resultsMASE}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  TS & {\metodo} & ETS & Croston & Theta & RW & ARIMA \\
  \hline Airline	&	1.913	&	12.707	&	2.738	&	5.853	&	5.664	&	1.441 *\\
  \hline WmFrancfort	&	3.578 *	&	3.608	&	7.984	&	4.673	&	5.159	&	7.988\\
  \hline WmLondon	&	1.648	&	1.603 *	&	8.410	&	3.099	&	4.119	&	3.484\\
  \hline WmMadrid	&	4.442 *	&	5.686	&	9.126	&	5.362	&	5.685	&	8.625\\
  \hline WmMilan	&	5.967 *	&	6.684	&	9.263	&	6.534	&	6.678	&	19.327\\
  \hline WmNewYork	&	2.667	&	1.837 *	&	7.982	&	3.942	&	4.879	&	6.228\\
  \hline WmTokyo	&	2.791	&	2.443	&	3.935	&	2.129	&	2.402	&	1.628 *\\
  \hline Deceases	&	1.059	&	1.059	&	0.952 *	&	0.955	&	1.064	&	1.144\\
  \hline SpaMovSpec	&	1.027	&	2.027	&	1.009 *	&	1.023	&	1.010	&	1.933\\
  \hline Exchange	&	41.181	&	44.039	&	30.448 *	&	37.807	&	32.825	&	70.734\\
  \hline Gasoline	&	1.198 *	&	1.543	&	1.864	&	1.274	&	1.541	&	1.698\\
  \hline MortCanc	&	0.646	&	1.618	&	4.098	&	0.725	&	0.796	&	0.277 *\\
  \hline MortMade	&	1.314	&	1.303 *	&	4.500	&	3.869	&	1.315	&	1.712\\
  \hline Books	&	0.762	&	0.965	&	0.936	&	0.894	&	0.759 *	&	1.147\\
  \hline FreeHouPrize	&	3.339 *	&	5.642	&	19.468	&	3.487	&	6.183	&	6.805\\
  \hline Prisoners	&	14.482	&	5.485	&	23.979	&	11.934	&	16.305	&	4.031 *\\
  \hline TurIn	&	1.903	&	1.902	&	3.151	&	1.824 *	&	1.916	&	1.950\\
  \hline TurOut	&	2.005	&	2.000	&	2.088	&	2.239	&	1.996 *	&	2.241\\
  \hline TUrban	&	0.886	&	0.978	&	0.772	&	0.744 *	&	0.887	&	0.897\\
  \hline HouseFin	&	1.319	&	1.283	&	1.234	&	1.095 *	&	1.322	&	1.502\\
  \hline
\end{tabular}
\end{table*}

\begin{table*}[h]
\caption{Results of the methods {\metodo}, ETS, Croston, Theta, RW,
  and ARIMA, with respect to MdAPE. Best result per database is marked
  with character *.}

\label{tab:resultsMDAPE}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  Time series & {\metodo} & ETS & Croston & Theta & RW & ARIMA \\
  \hline Airline	&	15.057 *	&	233.934	&	54.657	&	119.754	&	118.090	&	15.212\\
  \hline WmFrancfort	&	14.610	&	14.603	&	39.259	&	19.960	&	22.750	&	11.026 *\\
  \hline WmLondon	&	3.498 *	&	5.430	&	30.550	&	10.474	&	15.722	&	5.099\\
  \hline WmMadrid	&	22.718	&	28.116	&	45.817	&	26.787	&	28.116	&	11.446 *\\
  \hline WmMilan	&	30.476 *	&	34.685	&	50.040	&	33.872	&	34.643	&	34.643\\
  \hline WmNewYork	&	9.114	&	4.598 *	&	35.253	&	16.505	&	23.137	&	5.712\\
  \hline WmTokyo	&	5.517 *	&	9.864	&	18.782	&	9.075	&	9.556	&	9.556\\
  \hline Deceases	&	4.267 *	&	5.464	&	6.121	&	4.440	&	5.458	&	5.458\\
  \hline SpaMovSpec	&	17.669 *	&	107.283	&	51.653	&	53.104	&	51.568	&	54.033\\
  \hline Exchange	&	44.368	&	46.597	&	34.121 *	&	38.832	&	36.521	&	45.961\\
  \hline Gasoline	&	1.792 *	&	7.587	&	9.045	&	6.429	&	7.563	&	8.923\\
  \hline MortCanc	&	11.25	&	9.694	&	30.568	&	4.047 *	&	5.339	&	5.116\\
  \hline MortMade	&	3.459 *	&	12.111	&	45.704	&	41.989	&	15.629	&	28.374\\
  \hline Books	&	4.868 *	&	18.111	&	17.230	&	16.566	&	11.567	&	18.093\\
  \hline FreeHouPrize	&	1.803 *	&	5.222	&	29.683	&	5.201	&	9.748	&	6.572\\
  \hline Prisoners	&	6.766	&	1.512 *	&	12.651	&	5.287	&	7.817	&	1.621\\
  \hline TurIn	&	2.945 *	&	6.627	&	11.696	&	4.779	&	6.669	&	4.605\\
  \hline TurOut	&	5.289 *	&	11.331	&	11.518	&	10.873	&	11.392	&	7.689\\
  \hline TUrban	&	5.290	&	8.262	&	6.822	&	4.922 *	&	8.900	&	6.374\\
  \hline HouseFin	&	18.286	&	22.623	&	21.279	&	18.845	&	23.684	&	17.297 *\\
  \hline
\end{tabular}
\end{table*}




\subsection{Analysis of the results}
\label{analysisresults}

\noindent To analyze in more detail the results and check whether the observed differences are significant, two main steps are performed: firstly, identifying whether exist differences in general between the methods used in the comparison; and secondly, determining if the best method is significant better than the rest of the methods. To do this, first of all it has to be decided if is possible to use parametric o non-parametric statistical techniques. An adequate use of parametric statistical techniques reaching three necessary conditions: independency, normality and homoscedasticity \cite{Sheskin2004}. 

Owing to the former conditions are not fulfilled, the Friedman and Iman-Davenport non-parametric tests have been used. Tables \ref{tab:Friedman} and \ref{tab:ImanDavenport} 
  shows the results for MAPE, MASE and MdAPE, for these tests. From left to right, tables show the Friedman and Iman-Davenport values ($\chi^2$ and $F_F$, respectively), the corresponding critical values for each distribution by using a level of significance $\alpha$ = 0.05, and the \emph{p-value} obtained for the measures utilized.

    \begin{table}[h]
        \caption{Results of the Friedman, showing signficant differences as $p-values<0.05$.}
        \label{tab:Friedman}
        \centering
        \begin{tabular}{|c|c|c|c|}
         \hline Measure & F. Value & Value in $\chi^2$ & $p$ value \\
         \hline MAPE	&	39.364	&	5	&	2.101E-10	\\
         \hline MASE	&	18.893	&	5	&	2.012E-03	\\
         \hline MdAPE	&	38.350	&	5	&	3.209E-07	\\
         \hline
        \end{tabular}
    \end{table}

    \begin{table}[h]
        \caption{Results of the Iman-Davenport test, showing signficant differences as $p-values<0.05$. }
        \label{tab:ImanDavenport}
        \centering
        \begin{tabular}{|c|c|c|c|}
         \hline Measure &  I-D. Value & Value in $F_F$ & $p$ value \\
         \hline MAPE	&	12.283	&	5 and 95	&	3.416E-09\\
         \hline MASE	&	4.426	&	5 and 95	&	1.146E-03\\
         \hline MdAPE	&	11.819	&	5 and 95	&	6.717E-09\\
         \hline
        \end{tabular}
    \end{table}
      

    As can be observed, the critical values of Friedman and Iman-Davenport are smaller than the statistic, it means that there are significant differences among the methods in all cases.
    In addition, Friedman provides a ranking of the algorithms, so that the method with a lowest result is taken as the control algorithm. For this reason, and according to table \ref{tab:ranking}, the {\metodo} algorithm results to be the control algorithm for the three quality measures.

    \begin{table}[h]
        \caption{Friedman's test ranking. Control algorithms are located in first row.}
        \label{tab:ranking}
        \centering
        \begin{tabular}{|c|c|c|c|c|c|}
         \hline \multicolumn{2}{|c|}{MAPE}   &  \multicolumn{2}{c|}{MASE}  &   \multicolumn{2}{c|}{MdAPE} \\
         \hline {\metodo} & 1.50   &   {\metodo} & 2.53   &   {\metodo} & 1.85 \\
         \hline Theta	&	3.15   &	Theta   &   2.70   &   ARIMA   &   2.93\\
         \hline ARIMA	&	3.18   &	RW      &   3.45   &   Theta   &   2.95\\
         \hline RW      &   4.13   &   ETS     &   3.48   &   RW      &   4.05\\
         \hline ETS     &   4.25   &   Croston &   4.40   &   ETS     &   4.08\\
         \hline Croston &   4.80   &   ARIMA   &   4.45   &   Croston &   5.15\\
         \hline
        \end{tabular}
    \end{table}

In order to check if the control algorithm has statistical differences regarding the other methods used, the Holm procedure \cite{Holm1979} is used. Table \ref{tab:holm} presents the results of the Holm's procedure since shows the adjusted $p$ values from each comparison between the algorithm control and the rest of the methods for MAPE, MASE, and MdAPE, considering a level of significance of $alpha=0.05$.

      \begin{table*}[h]
        \caption{Adjusted $p$ values of Holm's procedure between the control algorithm ({\metodo}) and the other methods for MAPE, MASE, and MdAPE. Values lower than $alpha=0.05$ indicate significant differences between {\metodo} and the corresponding algorithm.}
        \label{tab:holm}
        \centering
        \begin{tabular}{|c|c|c|c|c|c|}
         \hline \multicolumn{2}{|c|}{MAPE}   &  \multicolumn{2}{c|}{MASE}  &   \multicolumn{2}{c|}{MdAPE} \\
         \hline Croston &   2.433E-08   &   ARIMA   &   1.138E-03   &   Croston &   2.432E-08\\
         \hline ETS     &   3.346E-06   &   Croston &   1.528E-03   &   ETS     &   1.692E-04\\
         \hline RW      &   9.120E-06   &   ETS     &   1.083E-01   &   RW      &   2.002E-04\\
         \hline ARIMA	&	4.636E-03   &	RW      &   1.179E-01   &   Theta   &   6.298E-02\\
         \hline Theta	&	5.287E-03   &	Theta   &   7.673E-01   &   ARIMA   &   6.920E-02\\
         \hline
        \end{tabular}
    \end{table*}

    As can be seen in table \ref{tab:holm}, there are significant differences among {\metodo} and all the rest methods for MAPE. With respect to MASE, there exist significant differences between the {\metodo} algorithm and ARIMA and Croston, although it is not appropriate to assure that with methods ETS, RW, and Theta. Regarding MdAPE, {\metodo} has significant differences with methods Croston, ETS, and RW.

    In conclusion, it is possible to confirm that the {\metodo} method is able to achieve a better forecast in majority of cases comparing with the other 5 methods utilized and concerning to 3 different quality measures.


\section{Conclusions} 
\label{sec:conclusions}

\noindent In this contribution, the behavior of the {\metodo} method, a recent algorithm developed for minimizing the error when predicting future values of any time series given, for automatic time series forecasting is studied. 

The algorithm has been tested with 20 different time series and
contrasted with a set of 5 representative methods. In
addition, 3 distinct quality measures have been
used to check the results. {\metodo} obtains the best results in the
majority of the cases tested for every measure considered.

A statistic study has been done in order to confirm the results achieved.
With respect to MAPE, {\metodo} is significantly better than the rest
of the method; regarding MASE, it has significant differences
with ARIMA and Croston; and with respect to MdAPE, it obtains
significantly better results than Croston, ETS and RW. 

Thus, it can be concluded that the {\metodo} algorithm yields better
results in most time series used than the other methods utilized. 
\section*{\uppercase{Acknowledgements}}

\noindent This work has been supported by the regional projects
TIC-3928 and -TIC-03903 (Feder Funds), the Spanish projects TIN
2012-33856 (Feder Founds), and TIN 2011-28627-C04-02 (Feder Funds). The authors would also like to thank the FEDER of European Union for financial support via project {"Sistema de Información y Predicción de bajo coste y autónomo para conocer el Estado de las Carreteras en tiempo real mediante dispositivos distribuidos" (SIPEsCa)} of the {"Programa Operativo FEDER de Andalucía 2007-2013"}. We also thank all {Agency of Public Works of Andalusia Regional Government} staff and researchers for their dedication and professionalism.

\begin{figure}
\begin{center}
\epsfig{file=logos_SIPESCA_2.png,width=6cm}
\end{center}
\end{figure}

\bigskip


\bibliographystyle{apalike}
{\small
\bibliography{ECTA-IJCCI}}




\vfill
\end{document}

