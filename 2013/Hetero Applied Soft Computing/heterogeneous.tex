%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%%
%%
%% $Id: elsarticle-template-num.tex 4 2009-10-24 08:22:58Z rishi $
%%
%%
%%%%%\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%%  \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
 \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%%\documentclass[final,5p,times,twocolumn]{elsarticle}%%DOS COLUMNAS

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{url}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfigure}
\usepackage{color}
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


\journal{Applied Soft Computing}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Population size adaptation in distributed evolutionary algorithms on heterogeneous clusters}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[ugr]{Pablo Garc\'ia-S\'anchez}
\ead{pgarcia@atc.ugr.es}
\author[ugr]{Jes\'us Gonz\'alez}
\ead{jesusgonzalez@ugr.es}
\author[ugr]{Antonio Miguel Mora}
\ead{amorag@geneura.ugr.es}
\author[ugr]{Maribel Garc\'ia Arenas}
\ead{maribel@ugr.es}
\author[ugr]{Pedro A. Castillo}
\ead{pedro@atc.ugr.es}
\author[laseeb]{Carlos Fernandes}
\ead{cfernandes@laseeb.org}
\author[ugr]{Juan Juli\'an Merelo}
\ead{jmerelo@geneura.ugr.es}


\address[ugr]{Department of Computer Architecture and Computer Technology and CITIC-UGR, University of Granada, Granada, Spain. Tel: +34958241778. Fax: +34958248993}
\address[laseeb]{LaSEEB-ISR-IST, Technical University of Lisbon (IST), Lisbon, Portugal}%


\begin{abstract}
In order to increase performance, distributed algorithms on heterogeneous clusters should be adapted to take advantage of the available resources. In distributed Evolutionary Algorithms the population size is one of the factors that mainly affects the execution time, because it directly influences the iterations necessary to find the solution. In this paper, we present a study to adapt this parameter to computational power of the nodes of a heterogeneous cluster, in order to reduce execution time. Two size adaptation schemes have been proposed: an offline and an online parameter setting, and two problems with different characteristics and computational demands (MMDP and OneMax) have been tested. Results show that setting the population size according to the computational power of the nodes in the heterogeneous cluster decreases the time and evaluations required to obtain the optimum. Meanwhile, the same set of different size values could not improve the time in a homogeneous cluster, so the improvement is due to the interaction of the different resources with the algorithm. In addition, a study of the influence of the different population sizes on each stage of the algorithm is presented. This opens a new research line on the adaptation (offline or online) of parameters to the computational power of the devices.

%Quitado: The total number of individuals is divided taking into account the computational power of each node.
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
%Service Oriented Architecture \sep OSGi \sep Java \sep Context Management \sep e-health
Evolutionary Algorithms \sep Genetic Algorithms \sep Heterogeneous computation \sep Distributed computing \sep Parameter Tuning \sep Parameter Control
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{sec:intro}

Adapting algorithm parameters to available computational resources leads to improved performance \cite{AutomaticallyConfiguringStyles12}. An easy way to take advantage of the available resources is the load balancing \cite{PARALLELIMPLEMENTATION} to distribute workloads across multiple elements. However, assigning equal tasks  to each node on heterogeneous clusters may result in suboptimal performance \cite{LoadBalancingBohn02}. Parameters of an algorithm could also be adapted to increase the performance of the whole system. For example, the population size in Evolutionary Algorithms (EAs) is the key to obtain good performance, because it have effect on the quality of the solution and the time spent during the run \cite{ShrinkageLaredo09}. This parameter has been studied as a fixed \cite{SizingHarik99} or adaptive parameter during runtime \cite{AdaptiveLobo07,SelfRegulatedSizeFernandes06}, but without taking into account the computational power of each machine in a heterogeneous cluster. In this paper we have investigated whether adapting the population size of the islands of a distributed Evolutionary Algorithm (dEA) \cite{MULTIKULTI} can leverage the capability of a heterogeneous cluster.

Evolutionary Algorithms are a general technique for solving optimization and search problems based on the evolution of species and natural selection. These algorithms are formed by a population of possible solutions, called {\em individuals}, that compete using their {\em fitness} (quality of adaptation) with the rest of solutions. Every iteration of the algorithm (or {\em generation}) the population evolves by means of selection and recombination/mutation to create a new set of candidates, until a {\em stop criterion} (e.g. number of generations) is met. Fitness function is a quality function that gives the grade of adaptation of an individual respect the others. This function usually describes the problem to solve.

% Metes muchos the al principio de plural, es incorrecto - JJ - FERGU: Ya veo. Lo cambio en el resto del paper (por ejemplo, en el párrafo de abajo)
New trends in distributed computing such as Cloud Computing \cite{CLOUD}, GRID
\cite{OPENSCIENCEGRID} or Service Oriented Science \cite{GLOBUS} are
leading to heterogeneous computational devices, including for instance, laptops,
tablets or desktop PCs, working in the same
environment. Thus, many laboratories, which do not count with classic
clusters but the usual workstations used by scientists, can leverage
this motley set as a heterogeneous cluster. Distributed Evolutionary
Algorithms \cite{MULTIKULTI,PARALLELGRIDHETEROGENEOUS} have been tested successfully in this
type of systems \cite{HETEROGENEOUSHARD} and they have become very popular because their implementation is
not complex.  % ¿seguro? - JJ FERGU: Esto está sacado del artículo ese de abajo xD (no plagiado, claro)
The most extended model exploits a coarse grained parallelism with sporadic % o
                                % sporadic? -JJ FERGU: Corregido
 communications, being fit to be executed in distributed architectures
 such as clusters or GRIDs \cite{PLATO}.%Nos puedes citar a nosotros mismos: asynchronous,
          %Dropbox... - JJ - FERGU: Done 

In distributed EAs a set of nodes executes simultaneously the EA, working with different sub-populations (or islands) at the same time. Every certain number of generations one or more individuals are interchanged (migrated) between sub-populations, which are connected following a specific topology. Figure \ref{fig:islands} shows this model with a ring topology.  % Revisa el inglés! - JJ FERGU: Ya, el problema es que no detecto los fallos de forma evidente :(





%There are different ways to parallelize the EAs, being the most extended:
% estas no son todas las maneras: se puede paralelizar basado en pool o en P2P.  FERGU: lo dejo abierto 
%\begin{itemize}
%\item {\em Farming model (centralized EAs)}: A central node coordinate several slave nodes. The central node executes the EA in a sequential way, but distributes the individuals of the population to the slaves just for being evaluated. An example can be seen in \cite{NUCLEAR}, where slave nodes evaluates fitness function for simullation of nuclear devices.
%\item {\em Island model (distributed EAs)}: A number of nodes executes simultaneously the EA, working with different sub-populations at the same time. Each certain number of generations is interchanged (migrated) between populations. Figure \ref{fig:islands} shows this model with a ring topology.
%\item {\em Cellular EAs (fine grain EAs)}: Each node has one individual of the population, and selection and reproduction is limited with the individuals of the neighbourhood of the node \cite{CELLULAR}. Usually a bi-dimensional grid is used for topology. 
% Puedes añadir un cuarto item: "métodos no convencionales", por ejemplo - JJ FERGU: Arriba
%\end{itemize}

\begin{figure}[htb]
\centering
\epsfig{file=1.eps, width = 7cm}
\caption{Island model scheme using a neighborhood ring topology.}
\label{fig:islands}
% Explica qué utilidad tiene en el artículo. FERGU: La cito luego en la sección experimentos
\end{figure}







Distributed EAs can be executed on homogeneous clusters with the same parameters in all nodes (homogeneous dEAs), or with different parameters or nodes' features (heterogeneous dEAs).
It  has also been showed \cite{HETEROGENEOUSHARD} that dEAs with the same parameter configuration are even
more efficient in time and evaluations on heterogeneous hardware configurations than on clusters with
homogeneous devices. This can be explained by different reasons, such
as different memory access times, cache sizes, % cache qué? - JJ - Fergu: sizes
or even implementation
languages or compilers in each machine, leading to a different
exploitation/exploration rate of the search space. %vaya salto de características
                                %físicas a algorítmicas. No sólo
                                %explotación, también exploración,
                                %¿no? - JJ
% Insisto: esa explicación queda muy ramplona- JJ - FERGU: Es que está copiada (adaptada, claro) del artículo de arriba tal cual. Pero añado lo de exploration
Heterogeneous parameter
configuration  has also been shown to be more  efficient time-wise than a fixed
set % CUIDADO CON LA GRAMÁTICA!!! - JJ - Fergu: Uhm, no veo cual era el error en la frase original, pero bueno, supongo que esta está mejor
% cambiado - JJ
of parameters for different problems
\cite{HETEROGENEOUSPARAMETERS}.   % efficiente en qué sentido? - JJ - Fergu: in time

% enlázalo con párrafo anterior. Un artículo es una historia. Por
% ejemplo: These proofs have motivated us to propose in this paper... FERGU: Cambiado
These facts have motivated us to study a combination of both ideas in this paper: dEAs on a heterogeneous set of nodes with different parameter values adapted to each node. In this study, the parameter to adapt to the computational power of each node has been the sub-population size of each island.

%Our motivation in this work is to combine both ideas % ¿cuáles ideas?
                                % a estas altura no se sabe de qué
                                % estás hablando. Si es usar
                                % configuración heterogénea en
                                % dispositivos heterogéneos, debes
                                % dejar bien claro que nadie lo ha
                                % hecho hasta ahora - JJ - FERGU: Reescrito arriba
%and adapt the population size of the islands according to the computational power of each node. 
%To calculate the computational power, the algorithm is executed in
%each machine; then, the total size of individuals is distributed
%according to the number of generations attained in each node per unit
%time. Two different problems (MMDP \cite{goldberg92massive} and
%OneMax \cite{ONEMAX}) have been used as a benchmark. 
% Por qué has comentado esto? - JJ - FERGU: La he quitado para que la gente no piense que es obligatorio ejecutarlo antes con igual tamaño para luego cambiar los tamaños (parece una metodología, pero no, es una hipótesis, y en este caso lo hemos adaptado así, pero se pueden usar otras cosas). Lo explico más adelante


In this work, a heterogeneous distributed system has been used to give an insight to the following questions:
\begin{itemize}
 \item Can a distributed EA be adapted to leverage the capability of a
   heterogeneous cluster? % Si este es el objetivo principal, deberías
                          % hacer énfasis desde el principio de la
                          % intro y demás. FERGU2: Dicho al principio de la intro
 \item How the adaptation of the sub-population size to the computational power affects the execution time and number of evaluations? % computational power ==
                                % performance? - JJ FERGU: Reescrita
                                % la pregunta
   % el efecto sobre qué? ¿El número de evaluaciones el algoritmo? ¿El
   % tiempo de ejecución? - JJ FERGU2: Cambiada la pregunta, again
 \item Is there any difference between using the same sub-population sizes in a homogeneous and a heterogeneous cluster?
 \item How is each stage of the algorithm (selection, recombination, mutation, replacement and migration) affected by the different
   configurations? % stage? - JJ FERGU: Etapa del algoritmo, Antonio y Jesús me han dicho que lo cambie a stage. Añado los paréntesis para aclararlo
% no se usan puntos suspensivos en escribura formal. No son fases del algoritmo, son diferentes operadores. FERGU2: puestas todas
\end{itemize}


The rest of the work is structured as follows: after a presentation of
the state of
the art in the algorithm parameter adaptation to computational substrate in dEAs, %en qué área? aquilátalo bien y que quede muy clarito - JJ - FERGU: Hecho FERGU2: Y cambiado de nuevo
 we present the developed algorithms and experimental setting (Section \ref{sec:experiments}). 
Then, the results of the experiments are shown (Section \ref{sec:results}), followed by conclusions and suggestions for future work lines.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SOA  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{State of the art}
\label{sec:soa}

One of the problems in parameter adaptation in heterogeneous clusters is 
the computational load representation. It depends of the algorithm, size of the problem, 
language, compiler or hardware characteristics, and the results obtained from artificial 
benchmarks (such as  Linpack \cite{LinpackEndo10}) should not be extolled as identificative 
of the system performance \cite{LinpackDongarra03}. For example, in the work of Garamendi 
et al. \cite{PARALLELIMPLEMENTATION},  a small benchmark was executed in all nodes at the beginning
of the algorithm in order to distribute individuals of an Evolutionary Strategy
 (ES), following a master-slave model. The computational load by artificial benchmarks may not accurately 
 represent the correct load of the algorithm, so, as proposed in this paper, information 
 about the algorithm itself should be used for calibration.

In other works, there are not direct relation with the algorithm parameters and 
computational resources of the nodes. For example, Dom\'inguez et al. \cite{HYDROCM} 
divided the available devices in ``faster'' and ``slower'' nodes to create a distributed hybrid 
meta-heuristic that combines two different EAs: Genetic Algorithms (GAs) and Simulated
Annealing (SA). Their system executes the heavy (in computational
terms) algorithms (GAs) in the faster nodes (computational devices), and
simpler meta-heuristics (SA) in the slower ones, obtaining better results
than other configurations.  Gong et al. in \cite{HETEROGENEOUSTOPOLOGY} also ordered 
the nodes by their computational power to test different topology configurations in a distributed EA.
Besides from ordering the nodes taking into account 
only their previously known computational resources, the results of the previous works were not compared in a homogeneous 
cluster to validate if the adaptation takes advantage of the heterogeneity 
of the cluster, as proposed in this paper.

The heterogeneous computational performance of nodes or network speed can affect the performance of an algorithm. In \cite{HETEROGENEOUSHARD},
 Alba et al. compared a distributed Genetic Algorithm (dGA), one of
 the sub-types of EAs, on homogeneous and heterogeneous clusters. % ¿Cómo considera las prestaciones un parámetro este trabajo en particular? - JJ FERGU2: number of iterations, añadido
 Super-linear performance in terms of iterations was obtained in the heterogeneous ones,
 being more efficient than the same algorithm running on homogeneous
 machines. However, the parameter setting was the same in both
 clusters and they not adapted the parameters to the machines used. %FERGU2: comparado


%In the field of  Evolutionary Computation (EC) there are two different approaches about the algorithm parameter setting: {\em parameter control} and {\em parameter tuning} \cite{PARAMETERTUNING}. The first one refers to setting up a number of parameters of an Evolutionary Algorithm  and changing these values in running time (online). The parameter tuning consists in establishing a good set of parameters before the run (offline), and do not change them during the execution.

 %Computational performance of nodes or network speed can also be
 %inherent parameters of an algorithm. In \cite{HETEROGENEOUSHARD},
 %Alba et al. compared a distributed Genetic Algorithm (dGA), one of
 %the sub-types of EAs, on homogeneous and heterogeneous clusters. % ¿Cómo considera las prestaciones un parámetro este trabajo en particular? - JJ FERGU2: iterations
 %Super-linear performance was obtained in the heterogeneous ones,
 %being more efficient than the same algorithm running on homogeneous
 %machines, although the parameter setting was the same in both
 %clusters. % Entonces no tiene nada que ver con lo de los parámetros
           % "inherentes". Si tiene que ver, no está explicado. Y en
           % todo caso este artículo está más en el contexto de "dEA
           % heterogéneos", que no es algo a lo que has dicho que ibas
           % a aludir. Si es eso de lo que vas a hablar, podría ser un
           % ejemplo negativo: "Los DEAs heterogéneos han obtenido
           % buenos resultados sin necesidad de adaptación de
           % parámetros". 
%Tampoco haces ninguna referencia a la asincronía. Tú tienes
%algoritmos heterogéneos síncronos, mientras que estos me parece
%recordar que son asíncronos y si hablas de ellos tienes estos dos
%papers:
% http://scholar.google.com/citations?view_op=view_citation&hl=en&user=gFxqc64AAAAJ&pagesize=100&citation_for_view=gFxqc64AAAAJ:k8Z6L05lTy4C
% y
% http://scholar.google.com/citations?view_op=view_citation&hl=en&user=gFxqc64AAAAJ&pagesize=100&citation_for_view=gFxqc64AAAAJ:9ZlFYXVOiuMC
% también estos
% http://scholar.google.com/citations?view_op=view_citation&hl=en&user=gFxqc64AAAAJ&cstart=100&pagesize=100&citation_for_view=gFxqc64AAAAJ:lSLTfruPkqcC
% 
%FERGU2: el mío sí es asíncrono, lo pone más adelante, pero añado uno de estos a la sección "Framework"

%Some authors have expanded this idea % qué idea? La de los algoritmos FERGU2: Todos estos trabajos han sido reordenados y puestos arriba
                                % heterogéneos? - JJ
%by adapting the algorithm to be executed: Dom\'inguez et
%al. \cite{HYDROCM} presented a distributed hybrid meta-heuristic that
%combines two different EAs: Genetic Algorithms (GAs) and Simulated
%Annealing (SA). Their system executes the heavy (in computational
%terms) algorithms (GAs) in faster nodes (computational devices), and
%simpler meta-heuristics (SA) in slower ones, obtaining better results
%than other configurations. As in previous works, the parameters were
%not adapted to the node. % ni falta que hacía, ya adaptabas el
                         % algoritmo. En todo caso, no tiene nada que
                         % ver con el anterior, salvo que es
                         % heterogéneo - JJ FERGU2: movido arriba, comparado con lo nuestro.
 %Gong et al. in \cite{HETEROGENEOUSTOPOLOGY} studied different
 %configurations % configurations de qué? parameter? Algoritmo? de qué?
                % Tienes que relacionarlo con el anterior, recuerda
                % "la narrativa" - JJ
 %of heterogeneous machines for a tree topology. % para qué necesitaban
                                % el árbol? - JJ fERGU2: árbol quitado
 %However, the heterogeneity was simulated in a homogeneous cluster
 %with programs to add computational load. %eso es irrelevante (como lo
                                %del árbol) - JJ FERGU2: quitado lo de simulated
 %Load-balancing was also applied taking into account the computational
 %load of the nodes % en ese caso, la carga era una forma de medir con
                   % precisión las prestaciones de cada uno, algo que
                   % no se puede hacer tan bien con nodos heterogéneos
                   % - JJ 
 %in the work of Garamendi et al. \cite{PARALLELIMPLEMENTATION} a small
 %benchmark was executed in all nodes at the beginning of the algorithm
 %in order to distribute individuals of an Evolutionary Strategy
 %(ES). However, there was no communication between the nodes and the
 %algorithm parameters were not adapted.  % Hala, seguimos con "este
                                % tío hace esto". "Tal como en el
                                % anterior se conocía la carga
                                % computacional a base de crearla
                                % artificialmente (lo que es no
                                % bueno), el problema de representar
                                % la capacidad computacional en cada
                                % nodo se resuelve  por parte de
                                % Garamendi via..." "nosotros, por
                                % nuestra parte, haremos... "
%O mucho mejor. "Uno de los problemas en la adaptación de parámetros FERGU2: Puesto al principio del SOA
%en clusters heterogéneos es la representación de la carga
%computacional. Se puede resolver de varias formas: una, el usar una
%carga conocida como hizo no sé quién. Otra, usar un benchmark como
%hizo no sé quién " (y no me creo que este tío haya sido el único). 
% Además, este artículo tiene toda la pinta de un proyecto fin de
% carrera y es bastante cutre, aunque apunta a diferentes técnicas de
% adaptación de carga en clusters heterogéneos que es lo que deberías
% mirar, en general. 
% Por ejemplo, tienes este artículo, el primero que sale en "load FERGU2: Citado y comentado
% balancing heterogeneous clusters"
% http://www.sciencedirect.com/science/article/pii/S0167739X01000589
% Te dice: "This research addresses techniques by which the size of
% the task assigned to a processor is a suitable match. Thus, the more
% powerful processors do more work and the less powerful processors
% perform less work."
% Una primera aproximación al "tamaño" de la tarea es cambiar el FERGU2: esta parte puesta arriba.
% tamaño de la población, aunque en realidad lo que se cambia es el
% "tamaño" de la computación necesaria para calcular una
% generación. Las conclusiones vienen bastante bien para justificar,
% al menos a priori, este artículo. Mira también lo que dice sobre los
% resultados dependiendo de la disimilitud de las prestaciones de los
% nodos - JJ

Adapting algorithm parameters to computational nodes derives in heterogeneous parameter sets. These sets can improve the results in homogeneous hardware, for example, setting a random set of parameters in each homogeneous node can also increase the
performance of a distributed Genetic Algorithm, as explained by Gong
and Fukunaga in \cite{HETEROGENEOUSPARAMETERS}. That model
outperformed a tuned canonical dGA with the same parameter values in
all islands. Also, adapting the migration rate produced better
results than homogeneous periods, as explained by Salto and Alba in
\cite{HETEROGENEOUSMIGRATION}. This indicates that heterogeneous parameters
 may lead to an increase of performance, so it is necessary to valid if the 
 performance is due to the parameter set or by the heterogeneous devices combination.
% Todo esto está bien porque vas a probar lo mismo en el paper, pero FERGU2: Cambiada la intro de esta parte y explicado que hay que hacer comparativas
% ¿por qué diablos lo haces? Si es para establecer una baseline de
% prestaciones, no hace falta que lo justifiques, porque no es un
% resultado del paper. Si es para justificar algo, di qué es lo que
% quieres justificar y como encaja en la estrategia. 


 %Our work presents a combination of some of the previous ideas: % si
                                % son solo some, ¿para qué presentas
                                % las otras? ¿Cómo encajan en la
                                % justificación de tu trabajo?
                                % Finalmente, lo más importante, ¿cuál
                                % es el estado del arte sobre el cuál
                                % vas a probar que has establecido una
                                % mejora?  FERGU2: Quitado lo de las ideas
In our work, a parameter (size of the sub-populations) of a dEA is adapted (offline and online) to the computational power of each machine, using the information obtained from the algorithm itself, and compared in different hardware systems.
 To the best of our knowledge, there are no works that
 modify parameters of the EA (such as the size) depending of the
 node where the island is being executed, and taking into account information provided by the execution of the algorithm. 
% pero explica por qué esa combinación es interesante y puede
% aprovechar mejor la capacidad computacional del sistema - JJ FERGU:
% He descomentado el párrafo de To our knowledge y he metido más info
% Jolines, si acabas de poner el trabajo del estudiante de la URJC que
% hace lo mismo!!!! Al menos habrá uno, pero seguro que hay más.FERGU2: Cambiado este párrafo entero. El de la URJC no modifica un parámetro hace un master-slave, lo he aclarado



%\section{Service Oriented Evolutionary Algorithms}
%\label{sec:soaea}



%As discussed in \cite{SOASOCO} the evolutionary algorithms research area is a propitious environment to migrate to SOA for several reasons: SOA fits with the genericity advantages in the development of software for EAs \cite{GENERICITY05} and adds new features, such as language independence and  distribution mechanisms. Moreover, there are a wide number of frameworks for EAs mostly incompatible with others, due to different programming languages, operating systems or communication protocols (see \cite{SURVEYMOFS} for a survey). In addition, new research trends, like self-adaptation \cite{SELFSTAR}, require many changes and modifications in the algorithms behaviour in real-time. And finally, the increase of technologies such as GRID and Cloud Computing \cite{CLOUD,LOADBALANCINGCLOUD}, where the computation elements are distributed in different machines, with many operating systems and programming languages.

%In order to deal with the operating system and architecture heterogeneity, the OSGiLiath framework \cite{SOASOCO}, based in Java, has been used in this work. This is a service-oriented evolutionary framework that automatically configures the services to be used in a local network. In this case, each node offers a migration buffer to accept foreign individuals. Also, in order to reduce bottlenecks in distributed executions, asynchronous communication has been provided to avoid idle time using reception buffers (that is, the algorithm does not wait until new individuals arrive, but the buffers cannot be used until again until the reception is done). This kind of communication offers an excellent performance when working with different nodes and operating systems, as demonstrated in \cite{HETEROGENEOUSHARD}. The transmission mechanism is based in ECF Generic server (over TCP)\footnote{\url{http://www.eclipse.org/ecf/}}. 
% ¿y qué? ¿eso es mejor o peor que un socket o un SOAP? - JJ. FERGU: en realidad no digo que sea mejor que otros, lo pongo para que se puedan reproducir los resultados, es como si dijera que está hecho en C++
% The source code of
% the algorithms used in this work is available in
% \url{http://www.osgiliath.org} under a GPL V3 License. 


%%%%%%%%%%%%%%%%%%  Experiments  %%%%%%%%%%%%%%%%%%%
\section{Experimental setup}
\label{sec:experiments}
In this section we propose two different parameter adaptation schemes to test if adapting the sub-population sizes of a dEA to the nodes of a  heterogeneous clusters reduces time. In the field of  Evolutionary Computation (EC) there are two different approaches about the algorithm parameter setting: {\em parameter tuning} and {\em parameter control} \cite{PARAMETERTUNING}. The first one consists in establishing a good set of parameters before the run (offline), and do not change them during the execution. The parameter control refers to setting up a number of parameters of the EA  and changing these values in running time (online). For the first approach, a dEA has been executed in the heterogeneous cluster. The obtained results have been used, to distribute the number of individual among the nodes (that is, offline). The same sizes set is used in the homogeneous cluster to validate if the changes in performance are dued to the parameters or the adaptation to the nodes. Finally, an online parameter setting that extract relative information of the performance of the nodes has been tested to validate our approach.




\subsection{Algorithm used}
The experimentation is centered in a distributed GA, one of the most used EAs \cite{GeneticAlgorithmsEiben03}. Figure \ref{fig:EA} shows the pseudo-code of the used algorithm. %We have consider the usual set of parameters used in the literature for this type of algorithms. Parameters are described in Table \ref{table:parameters}. 
The algorithm is steady-state, i.e. every generation the offspring is mixed with the parents and the worst individuals are removed. The used neighborhood topology for migration between islands (nodes) is a ring (see Figure \ref{fig:islands}). The best individual is sent to the neighbour in the ring, after a fixed number of generations in each island. The algorithm stops when the optimum (the solution to the problem) is found.  %Two different parameter configurations have been used. The Homogeneous Size (HoSi) uses 64 individuals per node. In the Heterogeneous Size (HeSi) approach this number is proportional to the average number of generations attained by each node in this first homogeneous size execution.

%Tienes que justificarlo absolutamente todo. ¿Si se usa otro
%algoritmo, cambiarían los resultados? ¿Por qué se ha usado
%precisamente este? - JJ


\begin{figure}[htb]

\begin{algorithmic}
\STATE population $\gets$ initializePopulation()
\WHILE {stop criterion not met}
    \STATE parents $\gets$ selection(population)
    \STATE offspring $\gets$ recombination(parents)
    \STATE offspring $\gets$ mutation(offspring)
    \STATE population $\gets$ population + offspring
    \IF {time to migrate}
      \STATE migrants $\gets$ selectMigrants(population)
      \STATE remoteBuffer.send(migrants)
    \ENDIF
    \IF {localBuffer.size $\neq$ zero}
      \STATE population $\gets$ population + localBuffer.read()
    \ENDIF
    \STATE population $\gets$ removeWorst(population)
\ENDWHILE

\end{algorithmic}
\caption{Pseudo-code of the used dEA: a distributed Genetic Algorithm (dGA).}
\label{fig:EA}
\end{figure}




\subsection{Problems}
%No empieces con "the problems to evaluate". Di que los resultados
%deberían ser más o menos independientes del problema, pero se han
%elegido estos por tal y cual. Tienes que justificar que con estos es
%suficientes, para que no te digan el clásico "Prueba otro algoritmo"
%- JJ

The problems to evaluate are the Massively Multimodal Deceptive
Problem (MMDP) \cite{goldberg92massive} and the OneMax problem
\cite{ONEMAX}. Each one requires different actions/abilities by the GA
at the level of population sizing, individual selection and
building-blocks mixing. The MMDP

 is designed to be difficult for an EA, due to
its multimodality and deceptiveness. Deceptive problems are functions where low-order building-blocks do not combine to form higher order building-blocks. Instead, low-order building-blocks may mislead the search towards local optima, thus challenging search mechanisms. MMDP it is composed of $k$ subproblems of 6 bits each one ($s_i$). Depending of
the number of ones (unitation) $s_i$ takes the values shown in Table \ref{table:mmdpvalues}.  

\begin{table}

\centering
{\scriptsize
\caption{ Basic deceptive bipolar function ($s_i$) for MMDP.}
\label{table:mmdpvalues}
\begin{tabular}{|c|c|}
\hline
Unitation&Subfunction value\\
\hline
0 & 1.000000 \\
\hline
1 & 0.000000 \\
\hline
2 & 0.360384 \\
\hline
3 & 0.640576\\
\hline
4 & 0.360384\\
\hline
5 & 0.000000\\
\hline
6 & 1.000000\\
\hline

\end{tabular}
}


\end{table}
%%%%%%%%%%%%%%%%%%



The fitness value is defined as the sum of the $s_i$ subproblems with an optimum of $k$ (Equation \ref{eq:mmdp}).
The search space is composed of $2^{6k}$ combinations from which there
are only $2^k$ global solutions with $22^k$ deceptive
attractors. Hence, a search method have to find a global solution
out of $2^{5k}$ additionally to deceptiveness. In this work $k=25$. 

\begin{equation}\label{eq:mmdp}
f_{MMDP}(\vec s)= \sum_{i=1}^{k} fitness_{s_i}
\end{equation}

OneMax is a simple linear problem that consists in maximising the number of ones in a binary string. That is, maximize the expression:
\begin{equation}
f_{OneMax}(\vec{x}) = \sum_{i=1}^{N}{x_{i}}
\end{equation}

\subsection{Hardware and parameter configurations}

<<<<<<< HEAD
Five configurations have been tested: % ¿Por qué? ¿Cubren el objetivo
                                % del artículo? Si no, siempre llegará
                                % el revisor que te diga "Prueba, no
                                % sé, "migración de poblaciones
                                % completas" o vaya usté a saber... -
                                % JJ 
=======
Five configurations of hardware and parameter settings have been tested:
>>>>>>> 820357f3e027f2313f2ba89182bc486b395ab386

\begin{itemize}
\item HoSi/HeHa: Homogeneous Size/Heterogeneous Hardware. The same sub-population size in each island on a heterogeneous cluster.
\item HeSi/HeHa: Heterogeneous Size/Heterogeneous Hardware. Different sub-population sizes in each island on a heterogeneous cluster.
\item HoSi/HoHa: Homogeneous Size/Homogeneous Hardware. The same sub-population size in each island on a homogeneous cluster.
\item HeSi/HoHa: Heterogeneous Size/Homogeneous Hardware. Different sub-population sizes (the obtained for HeSi/HeHa) in each island on a homogeneous cluster.

\item AdSi/HeHa: Adaptive Size/Heterogeneous Hardware. Online adaptation of sub-population sizes in each island on a heterogeneous cluster.
\end{itemize}

Two different computational systems have been used: a {\em heterogeneous cluster} and a {\em homogeneous cluster}. The first one is formed by four different computers of our lab with different processors, operating systems and memory size. The latter is a dedicated scientific cluster formed by homogeneous nodes. Table \ref{tabcomputers} shows the features of each system and the name of the nodes.

\begin{table*}
\centering{\scriptsize
\caption{Details of the clusters used: a homogeneous cluster (Ho), and a heterogeneous cluster (He)}
\begin{tabular}{|c|c|c|c|c|} \hline
Name     & Processor  & Memory  & Operating System  & Network  \\ \hline
\multicolumn{5}{|c|}{Homogeneous cluster} \\ \hline
HoN[1-4] &  Intel(R) Xeon(R) CPU   E5320  @ 1.86GHz       & 4GB & CentOS 6.7    &   Gigabit Ethernet    \\ \hline
\hline
\multicolumn{5}{|c|}{Heterogeneous cluster} \\ \hline
HeN1  &  Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz    & 4GB   & Ubuntu 11.10 (64 bits)  & Gigabit Ethernet      \\ \hline
HeN2  &  Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz    & 4GB   & Ubuntu 11.04 (64 bits)  & Gigabit Ethernet      \\ \hline
HeN3  &  AMD Phenom(tm) 9950 Quad-Core Processor @ 1.30Ghz    & 3GB   & Ubuntu 10.10 (32 bits)  & 100MB Ethernet      \\ \hline
HeN4  &  Intel (R) Pentium 3 @ 800MHz               & 768 MB  & Ubuntu 10.10 (32 bits)  &   10MB Ethernet     \\ \hline
\end{tabular}
\label{tabcomputers}
}
\end{table*}

\subsubsection{Homogeneous Size configuration}

In this configuration, each node has 256 individuals (so, the total amount is 1024). After executing the algorithm 40 times per problem on the heterogeneous cluster, we have obtained the average number of generations in each node, as it can be seen in Table \ref{table:generations}. Note how the generations attained (and their proportion in every node) to reach the optimum depends on the problem considered (besides the hardware).

\begin{table*}
\centering{
\caption{Average number of generations in each node needed to find the
  optimum on the heterogeneous cluster with heterogeneous size.}
% Y la desviación? - JJ
\begin{tabular}{|c|c|c|c|c|} \hline
Node        & HeN1     & HeN2      & HeN3     & HeN4   \\ \hline
\multicolumn{5}{|c|}{MMDP problem} \\ \hline
Generations & 10990.25 & 10732.075 &  7721.15 & 717.95 \\ \hline
Proportion  & 36.43    & 35.58    & 25.59    & 2.38    \\ \hline
\multicolumn{5}{|c|}{OneMax problem} \\ \hline
Generations & 2430.27 & 2353.77 & 1423.77 & 91.5 \\ \hline
Proportion  & 38.58   & 37.36   & 22.6   & 1.45 \\ \hline
\end{tabular}
\label{table:generations}
}
\end{table*}


\subsubsection{Heterogeneous Size configuration}

Our aim consists in validating the following hypothesis: adapting the sub-population size to the computational power of the heterogeneous cluster nodes presents an improvement in execution time. In this work, for a possible offline manner, we have used the average number of generations obtained in the HoSi/HeHa configuration for both problems to determine the computational power of the heterogeneous machines. This comparison takes into account all the evolutionary process in a fair manner (proportional to the memory, processor and network usage), instead a traditional benchmark that usually relies only on the CPU speed. Although this is not obviously the best way, it is a possible way to establish the computational power for the experiments of this work and to determine if changing the sub-population size according the computational power reduces the computing time of the whole approach. It should be considered that the contribution of this work is not the way we have computed these sizes, but compare the algorithm with parameters adapted to their power.

Thus, we have used the obtained average number of generations in the previous sub-section (Table \ref{table:generations}) to set proportionally the sizes in the HeSi/HeHa and HeSi/HoHa configurations, by dividing the total number of individuals (1024). Note that, even having two nodes with the same processors and memory (HeN1 and HeN2), they could have different computational power: this may be produced by different operating systems, virtual machine versions, or number of processes being executed (inside a node).



\subsubsection{Adaptive Size configuration}

Finally, in order to validate the hypothesis that adapt the sub-population sizes to computational resources of a heterogeneous cluster leads to decrease of time for obtain the solution, we propose a third configuration. In this experiment, the adaptation of the sub-population size to the computational power of the islands (nodes) is performed during runtime (online).  Each time a node ($N$) receives an individual, it compares its current number of generations ($Gen_{N}$) with the ones of the node who sent the individual (node $N-1$ in the ring). Then, the sub-population size is adapted proportionally to the difference in the number of generations, following the next equation:

\begin{equation}
size'_{N}=\dfrac{Gen_{N}}{Gen_{N-1}}size_{N}
\end{equation}

If the new size is larger than the actual size, new individuals are added to the sub-population cloning random existent ones. Otherwise, the sub-population must be reduced and thus, the worst are removed.

With this possible online adaptation scheme, each node only requires to receive information of one of the neighbours and not from the whole system. Thus, each node tends to have a number of individuals proportional to their computational power with respect to the other nodes. Experiments on homogeneous cluster do not alter the sub-population sizes, as the number of current generations are equal in all nodes during runtime.

Table \ref{table:parameters} summarizes all the parameters used in the experiments.

\begin{table}
\centering
\caption{Parameters used in all configurations.}
\begin{tabular}{|c|c|} \hline
Name & Value\\ \hline

Crossover type & Two-points crossover \\ \hline
Crossover rate & 0.5\\ \hline
Mutation rate & 1/individual size\\ \hline
Selection & 2-tournament \\ \hline
Replacement & Steady-state\\ \hline
Generations to migrate & 64 \\ \hline
Number of individuals to migrate & 1 \\ \hline
Stop criterion & Optimum found \\ \hline
Individual size for MMDP & 150 \\ \hline
Individual size for OneMax & 5000 \\ \hline
Runs per configuration & 40 \\ \hline
\hline
Total individuals in HoSi and HeSi & 1024\\ \hline \hline
Sub-population size in each node in HoSi & 256  \\ \hline
Sub-population sizes in HeSi for MMDP & 374, 364, 262 and 24 (from N1 to N4)\\ \hline
Sub-population sizes in HeSi for OneMax & 396,  382, 232 and 14 (from N1 to N4)\\ \hline
\hline
Maximum island size in AdSi & 1024 \\ \hline
Minimum island size in AdSi & 16 \\ \hline
Initial island size in AdSi & 256 \\ \hline 
\end{tabular}
\label{table:parameters}
\end{table}

\subsection{Framework}
In order to deal with the operating system and architecture heterogeneity (different operating systems, processors, compilers, etc.), the OSGiLiath framework \cite{SOASOCO}, based in Java, has been used in this work. This is a service-oriented evolutionary framework that automatically configures the services to be used in a local network. In this case, each node offers a migration buffer to accept foreign individuals. Also, in order to reduce bottlenecks in distributed executions, asynchronous communication has been provided to avoid idle time using reception buffers (that is, the algorithm does not wait until new individuals arrive, but the buffers cannot be used again until the reception is done). This kind of communication offers an excellent performance when working with different nodes and operating systems, as demonstrated in \cite{HETEROGENEOUSHARD,AsynchronousMerelo08}. The transmission mechanism is based on ECF Generic server (over TCP)\footnote{\url{http://www.eclipse.org/ecf/}}.  The source code of the algorithms used in this work is available in \url{http://www.osgiliath.org} under a LGPL V3 License. 

%%%%%%%%%%%%%%%%%%  Results  %%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}

The three main objectives of parallel programming are to tackle large computational problems, increase the performance of algorithms in a finite time, or reduce computational time to solve the problem (reaching the optimum). In this work, we focus in the last objective.
As claimed by Alba and Luque in \cite{EVALUATIONPARALLEL}, assessing the performance of a parallel EA by the number of fitness function evaluations required to attain a solution may be misleading. In our case, for example, the evaluation time is different in each node of the heterogeneous cluster, so the real algorithm speed could not be reflected correctly. However, the number of evaluations has been included in this section to better understand the results. The total number of generations carried out by all nodes, and the maximum number of generations required by the faster node in each configuration are also shown. It is difficult to compare the performance of HoHa and HeHa for the same reason: the evaluation time is different in each system (and even in each node). Thus, in this work, our aim is not making the heterogeneous cluster comparable or better in time than the homogeneous one (because they are, obviously, different), but showing that the same parameter configuration can improve performance in time on heterogeneous clusters and could not have an effect on homogeneous ones.

\subsection{MMDP results}
% No tiene mucho sentido hablar de los resultados de los dos problemas
% por separado, porque el objetivo no es resolver ese problema, sino
% probar cómo funciona en algoritmo. Se pueden comentar los dos juntos
% y se debería hacer - JJ 

Table \ref{tab:resultsMMDP} shows the results for the MMDP problem. These results are also shown in the boxplots of Figure \ref{fig:timeMMDP} (time) and Figure \ref{fig:evalsMMDP} (evaluations). Table \ref{tab:significanceMMDP} shows the statistical significance of the results. First, a Kolmogorov-Smirnov test is performed to assess the normality of the distributions. As all distributions are not normal, we use non-parametric tests. To compare between two methods (HoSi and HeSi in the homogeneous cluster) a Wilcoxon test has been applied. For a three methods comparison (HoSi, HeSi and AdSi on heterogeneous cluster) a Kruskal-Wallis test has been used. %If the results fit a normal distribution, then a Student's T-Test is calculated. Otherwise, the non-parametric Wilcoxon test is applied (see \cite{TUTORIAL} for a tutorial for comparing EAs).

 In the HeHa system, adapting offline the sub-population to the computational
 power of each node makes the algorithm finish significantly earlier,
<<<<<<< HEAD
 and need a lower number of evaluations to reach the solution. On the
 other hand, in the HoHa system, % nunca coma antes de predicado - JJ 
 setting the same population sizes makes no difference in time and
=======
 and also, needing a lower number of evaluations to reach the solution. On the other hand, in the HoHa system,
 setting the same sub-population sizes makes no difference in time and
>>>>>>> 820357f3e027f2313f2ba89182bc486b395ab386
 evaluations, that is, changing this parameter has no influence in the
 algorithm's performance (p-value=0.52 for time and 0.08 for evaluations).


\begin{table*}
\centering
\caption{Results for the MMDP problem.}
% Es la desviación típica o el error de la media? Deberías publicar el
% error de la media, esos números tan grandes (mayores que la media)
% no tiene sentido - JJ 
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|} \hline
Configuration & Max. generations      & Total generations     &   Total evaluations     & Time (ms) \\ \hline
HoSi/HeHa & 11194.8 $\pm$ 18810.08   & 30161.42 $\pm$  50722.03 & 7723372.8 $\pm$  12984841.71   & 27871.075 $\pm$  44583.14 \\ \hline
HeSi/HeHa   & 2506.1  $\pm$5308.872    & 8683.9    $\pm$ 18459.58 &  2453677 $\pm$5217896.18  &  8110.9 $\pm$ 17162.86 \\ \hline
AdSi/HeHa   & 2407.10 $\pm$3938.43     & 8376.35 $\pm$ 14140.55   & 2948946.15  $\pm$  5165324.99 &  10235.89  $\pm$ 17193.98\\ \hline  \hline
HoSi/HoHa   & 2614    $\pm$5889.93     & 10259.22  $\pm$ 23153.23 &  2628409.6 $\pm$   5927278.22 & 11560.8 $\pm$ 26072.14 \\ \hline
HeSi/HoHa   & 5411.92 $\pm$15608.81    & 10689.15  $\pm$  30790.7 & 1844908.1 $\pm$  5314771.88 &  9520.325 $\pm$   27237.35 \\ \hline

\end{tabular}
}
\label{tab:resultsMMDP}
\end{table*}




\begin{figure}[ht]
\centering

\subfigure[Heterogeneous cluster]{
   \includegraphics[scale =0.35] {3a.eps}
   \label{fig:subfig1}
 }
\subfigure[Homogeneous cluster]{
   \includegraphics[scale =0.35] {3b.eps}
   \label{fig:subfig2}
 }
\caption{Time to obtain the optimum in the MMDP problem
  (milliseconds).}
%No se ve nada, deberías usar log-y - JJ 
\label{fig:timeMMDP}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure[Heterogeneous cluster]{
   \includegraphics[scale =0.35] {4a.eps}
   \label{fig:subfig1}
 }
\subfigure[Homogeneous cluster]{
   \includegraphics[scale =0.35] {4b.eps}
   \label{fig:subfig2}
 }
\caption{Number of evaluations for MMDP problem.}
\label{fig:evalsMMDP}
\end{figure}



To see the differences on how the evolution is being performed, the average fitness in each node of HeHa is shown in Figures \ref{fig:hosiheha} and \ref{fig:hesiheha}. As it can be seen, with the HeSi (Figure \ref{fig:hesiheha}), the local optima are overtaken in less time than HoSi (Figure \ref{fig:hosiheha}).  This can be explained because in HeSi, the migration from HeN4 to HeN1 is performed faster, adding more heterogeneity to the whole system. Gaps in the figures correspond to the time spent in the nodes for sending the migrant individual to other nodes (not while they are receiving them). In the HoHa configurations, the evolution of sub-population is performed at the same time, being the average fitness similar in all nodes during all run. % The natural migration period variation from a processor to another is also giving more diversity to the populations that migrating at the same time of the homogeneous


\begin{figure}[htb]
\centering
\epsfig{file=5.eps, angle=-90, width = 13cm}
\caption{Average fitness in the first 1000 milliseconds of execution of the four nodes of the heterogeneous cluster with the same sub-population sizes (HoSi/HeHa) for the MMDP problem.}
\label{fig:hosiheha}
\end{figure}

\begin{figure}[htb]
\centering
\epsfig{file=6.eps, angle=-90, width = 13cm} %Era 9
\caption{Average fitness in the first 1000 milliseconds of execution of the four nodes of the heterogeneous cluster with different sub-population sizes (HeSi/HeHa) for the MMDP problem.}
\label{fig:hesiheha}
\end{figure}

Regarding to AdSi/HeHa, results are significantly  equal (p-value 0.139) to HeSi/HeHa (and, therefore, better than HoSi/HeHa), but this time no previous tuning has been required.  Average sub-population sizes in each node are shown in Table \ref{table:sizesMMDP}. The proportions of size are similar to the proportions in Table \ref{table:generations}. Figure \ref{fig:sizesMMDP} plots all the possible sizes in each node during all the runs. This figure shows that the variation of the sub-population sizes lies proportionally to the computational power of each node. The outliers in boxplots are produced during the size changing, as it can be seen in Figure \ref{fig:sizesMMDP1ejec}. As N4 is the slower node with difference it keeps its size always close to the minimum (16 individuals).

\begin{figure}[htb]
\centering
\epsfig{file=7.eps, width = 9cm}
\caption{Boxplots of the sub-population sizes in each node during all the runs for the MMDP problem.}
\label{fig:sizesMMDP}
\end{figure}

\begin{figure}[htb]
\centering
\epsfig{file=8.eps, width = 13cm}
\caption{Population size in each node during one execution to solve the MMDP problem.}
\label{fig:sizesMMDP1ejec}
\end{figure}

\begin{table*}
\centering{
\caption{Average sub-population size in each node on the heterogeneous cluster with adaptive size (MMDP).}
\begin{tabular}{|c|c|c|c|c|} \hline
Node        & HeN1     & HeN2      & HeN3     & HeN4   \\ \hline
Size &  556.31 & 504.30  & 321.15 & 19.81 \\ \hline
Proportion  & 39.69 &  35.98 & 22.91 & 1.41   \\ \hline
\end{tabular}
\label{table:sizesMMDP}
}
\end{table*}


<<<<<<< HEAD
\textcolor{red}{Summarizing, adapting the population sizes to the
  computational power of each machine (offline and online) has reduced
  the time to obtain the optimum. The same heterogeneous fixed sizes
  in the homogeneous cluster does not produces a significant decrease
  of running time, so the improvement is produced by the heterogeneity
  and not for the different island sizes. Also, the AdSi proposal is
  not applicable here because there are no differences of generations
  during runtime.} % no entiendo lo de que no haya "diferencias de
                   % generaciones" - JJ 

=======
Summarizing, adapting the sub-population sizes to the computational power of each machine (offline and online) has reduced the time to obtain the optimum. The same heterogeneous fixed sizes in the homogeneous cluster does not produces a significant decrease of running time, so the improvement is produced by the heterogeneity and not due to the different island sizes. Moreover, the AdSi proposal is not applicable in HoHa because there are not differences of generations during runtime.
>>>>>>> 820357f3e027f2313f2ba89182bc486b395ab386

% este junto con el anterior y un párrafo al final de los dos a
% comentar conjuntamente los resultados obtenidos. - JJ

\subsection{OneMax results}

Results for this problem are shown in Table \ref{tab:onemaxresults} and Figures  \ref{fig:timeOneMax} and \ref{fig:evalsOneMax}. In this case, adapting offline the sub-population sizes significantly decreases  the running time for solving it in the heterogeneous cluster, but this time, the number of evaluations is increased (see statistical significance in Table \ref{tab:significanceONEMAX}). In the homogeneous system, the effect of changing the sub-population sizes is clearer, and this time the number of evaluations (and therefore, the time) are reduced (both significantly). 

The efficiency on OneMax problem depends mainly on the ability to mix
the building-blocks, and less on the genetic diversity and size of the
population (as with MMDP). No genetic diversity is particularly
required. When properly tuned, a simple Genetic Algorithm is able to
solve OneMax in linear time. Sometimes, problems like OneMax are used
as control functions, in order to check if very efficient algorithms
on hard functions fail on easier ones. As it can be seen in Figure
\ref{fig:gensonemaxhomosize}, the average fitness of all sub-populations
are increasing in linear way in the HoSi/HeHa configuration. However,
the slower node evaluates extremely fewer times.  On the other
% qué diablos es un lower processor? slower processor? FERGU: cambiado a slower node y luego más adelante también
% por favor revisa muy bien todo esto, que tienes muchos errores
% gramaticales - JJ Fergu: el párrafo anterior lo escribió Carlos, así que creo que está bien, lo siguiente lo hice yo. He cambiado pronombres erróneos y eses en plurales
side, in Figure \ref{fig:gensonemaxheterosize}, smaller sub-population
sizes make that slower nodes increase the number of evaluations,
but the average fitness is also maintained in linear way (and in
smaller increase rate) between migrations. Nevertheless, the other
nodes still perform a higher number of evaluations. That is the
reason why the number of evaluations is higher in HeHa, and lower in
HoHa. Computational time is more efficiently spent in faster nodes,
having a higher chance to cross the individuals. In addition, due to
the larger size of  individuals in the OneMax problem (5000 bits
vs. 150 of the MMDP), the transmission time is larger, (white gaps in the
figures). It also implies that HeN4 sends its best individual to
HeN1 in an extremely large amount of time when using HoSi (every 64
generations). 

\begin{table*}
\centering
\caption{Results for the OneMax problem.}
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|} \hline
Configuration & Max. generations      & Total generations     &   Total evaluations     & Time (ms) \\ \hline
HoSi/HeHa   & 2430.34 $\pm$ 70.16  & 6299.31 $\pm$ 250.87 & 1614673.45  $\pm$  64223.09  &  160713.65 $\pm$   8873.46 \\ \hline
HeSi/HeHa   & 2643.34 $\pm$150.82  & 7969.58 $\pm$214.92 & 1802321.65  $\pm$  30511.96  &  151822.75  $\pm$4764.95 \\ \hline 
AdSi/HeHa   & 3698.30 $\pm$ 494.56 & 9465.25 $\pm$ 635.07 & 1149277.43  $\pm$ 58887.13 &  103919.33  $\pm$ 6296.39 \\ \hline \hline
HoSi/HoHa   & 1791.32 $\pm$   31.64& 7111.05 $\pm$125.11 & 1822476.8   $\pm$32029.78  &  141176.1    $\pm$2493.72\\ \hline
HeSi/HoHa   & 13698.12 $\pm$ 406.85 & 16012.625 $\pm$  482.61 & 895698.2 $\pm$   29520.99  &  77898.85  $\pm$  2935.57 \\ \hline
\end{tabular}
}
\label{tab:onemaxresults}
\end{table*}



\begin{figure}[ht]
\centering

\subfigure[Heterogeneous cluster]{
   \includegraphics[scale =0.35] {9a.eps}
   \label{fig:subfig1}
 }
\subfigure[Homogeneous cluster]{
   \includegraphics[scale =0.35] {9b.eps}
   \label{fig:subfig2}
 }
\caption{Time to obtain the optimum in the OneMax problem (milliseconds).}
\label{fig:timeOneMax}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure[Heterogeneous cluster]{
   \includegraphics[scale =0.35] {10a.eps}
   \label{fig:subfig1}
 }
\subfigure[Homogeneous cluster]{
   \includegraphics[scale =0.35] {10b.eps}
   \label{fig:subfig2}
 }
\caption{Number of evaluations for OneMax problem.}
\label{fig:evalsOneMax}
\end{figure}



\begin{figure}[htb]
\centering
\epsfig{file=11.eps, angle=-90, width = 13cm}
\caption{Average fitness in the first 15000 milliseconds of execution of the four nodes of the heterogeneous cluster with the same sub-population sizes (HoSi/HeHa) for the OneMax problem.}
\label{fig:gensonemaxhomosize}
\end{figure}

\begin{figure}[htb]
\centering
\epsfig{file=12.eps, angle=-90, width = 13cm}
\caption{Average fitness in the first 15000 milliseconds of execution of the four nodes of the heterogeneous cluster with different sub-population sizes (HeSi/HeHa) for the OneMax problem.}
\label{fig:gensonemaxheterosize}
\end{figure}


In the AdSi/HeHa configuration significantly better results in terms of execution time (and number of evaluations) are also attained, and even better than those obtained with HeSi. Average sizes (Table \ref{table:sizesONEMAX}) and boxplots (in Figure \ref{fig:sizesONEMAX}) during all the runs also show proportionality to the computational power of each machine. As in MMDP case, some oscillations (outliers in boxplots) may appear during the execution (as it can be seen in Figure \ref{fig:sizesONEMAX1ejec}).

\begin{figure}[htb]
\centering
\epsfig{file=13.eps, width = 9cm}
\caption{Boxplots of the sub-population sizes in each node during all the runs for the OneMax problem.}
\label{fig:sizesONEMAX}
\end{figure}

\begin{figure}[htb]
\centering
\epsfig{file=14.eps, width = 13cm}
\caption{Sub-population size in each node during one execution to solve the OneMax problem.}
\label{fig:sizesONEMAX1ejec}
\end{figure}

\begin{table*}
\centering{
\caption{Average sub-population size in each node on the heterogeneous cluster with adaptive size (OneMax).}
\begin{tabular}{|c|c|c|c|c|} \hline
Node        & HeN1     & HeN2      & HeN3     & HeN4   \\ \hline
Size &   267.09 & 158.63 &  74.20  & 16.29 \\ \hline
Proportion  &  51.73 &  30.72 &   14.37 &  3.15  \\ \hline
\end{tabular}
\label{table:sizesONEMAX}
}
\end{table*}



\begin{table}
\centering
\caption{Statistical significance of the results for MMDP.}
\begin{tabular}{|c|c|c|c|c|c|} \hline
\multicolumn{6}{|c|}{Time} \\ \hline
\multicolumn{6}{|c|}{Kruskal-Wallis chi-squared = 20.3042, df = 2, p-value = 3.899e-05} \\ \hline
Configuration       & Test  & obs.dif   & critical.dif  & p-value & difference \\ \hline
AdSi/HeHa-HeSi/HeHa      & K-W   & 13.19231  &    18.38851   & 0.1390  &  FALSE \\ \hline
AdSi/HeHa-HoSi/HeHa      & K-W   & 21.11538  &    18.38851   & 0.0067  & TRUE \\ \hline
HeSi/HeHa-HoSi/HeHa & K-W   & 34.30769  &    18.38851   & 9\e{-5} & TRUE \\ \hline \hline
HoSi/HoHa-HeSi/HoHa & Wilcoxon & -      & -             & 0.52    & FALSE \\ \hline \hline


\multicolumn{6}{|c|}{Evaluations}  \\ \hline
\multicolumn{6}{|c|}{Kruskal-Wallis chi-squared = 11.9676, df = 2, p-value = 0.002519} \\ \hline
AdSi/HeHa-HeSi/HeHa      & K-W  & 2.794872   & 18.38851      &  1.0          & FALSE \\ \hline
AdSi/HeHa-HoSi/HeHa      & K-W  & 21.487179  & 18.38851      &  0.0207        & TRUE\\ \hline
HeSi/HeHa-HoSi/HeHa & K-W  & 24.282051  & 18.38851      &  0.0028        & TRUE \\ \hline \hline
HoSi/HoHa-HeSi/HoHa &Wilcoxon & -       & -             & 0.08           & FALSE \\ \hline 

\end{tabular}
\label{tab:significanceMMDP}
\end{table}


\begin{table}
\centering
\caption{Statistical significance of the results for OneMax.}
\begin{tabular}{|c|c|c|c|c|c|} \hline
\multicolumn{6}{|c|}{Time} \\ \hline
\multicolumn{6}{|c|}{Kruskal-Wallis chi-squared = 66.4965, df = 2, p-value = 3.635e-15} \\ \hline
Configuration       & Test  & obs.dif   & critical.dif  & p-value & difference \\ \hline
AdSi/HeHa-HeSi/HeHa      & K-W   &  33.27586 &    15.87987   & 2.3\e{-10}  &  TRUE \\ \hline
AdSi/HeHa-HoSi/HeHa      & K-W   &  53.56897 &   15.87987  & $<$2\e{-16}  & TRUE \\ \hline
HeSi/HeHa-HoSi/HeHa & K-W   &   20.29310&   15.87987  & 4.2\e{-6}  & TRUE \\ \hline \hline
HoSi/HoHa-HeSi/HoHa & Wilcoxon & -      & -             & 3\e{-8}   & TRUE \\ \hline \hline


\multicolumn{6}{|c|}{Evaluations}  \\ \hline
\multicolumn{6}{|c|}{Kruskal-Wallis chi-squared = 75.7342, df = 2, p-value $<$ 2.2e-16} \\ \hline
AdSi/HeHa-HeSi/HeHa      & K-W  &  57.72414   &  15.87987     & $<$2\e{-16}          & TRUE \\ \hline
AdSi/HeHa-HoSi/HeHa      & K-W  &  29.27586   &   15.87987    & $<$2\e{-16}         & TRUE\\ \hline
HeSi/HeHa-HoSi/HeHa & K-W  &  28.44828   &  15.87987     &  $<$1.3\e{-14}        & TRUE \\ \hline \hline
HoSi/HoHa-HeSi/HoHa &Wilcoxon & -       & -              &  3\e{-8}          & TRUE \\ \hline 

\end{tabular}
\label{tab:significanceONEMAX}
\end{table}



\subsection{Running time analysis}

This sub-section analyses the time spent by each node of the clusters in every stage of the EA for each configuration with fixed sizes (HoSi and HeSi). Tables \ref{tab:mmdptimes} and \ref{tab:onemaxtimes} show the average and standard deviation of the time spent in each stage of the algorithm (He=Heterogeneous cluster, Ho=Homogeneous cluster). Figures \ref{fig:MMDPbars} and \ref{fig:ONEMAXbars} graphically compare these results. As it can be seen, the migration is the most time consuming operation in all configurations, being the migration in HeHa more expensive than in HoHa. This happens because we are using the multi-purpose laboratory network to communicate the nodes, instead of the specific one used in the HoHa system. Note that the standard deviation of the migration is larger in the HeHa cluster because the network is having real conditions of traffic during the experiment. In the MMDP problem (Table \ref{tab:mmdptimes}) changing the sub-population size does not affect the migration time, but it affects the rest of the algorithm's stages. However, with larger data communications (individuals of 5000 elements of the OneMax problem), the sub-population size affects the migration time of all nodes. This might be due to the synchronization of migration buffers: if the slowest machine is sending/receiving, bottlenecks can be propagated (as it can be seen in Figure \ref{fig:gensonemaxhomosize}). 

Results also show how the stages of the algorithms depends on the node
of execution. For example, recombination needs more time than mutation
in both problems only in the node HeN4. The reason might be the
creation of new objects (memory allocation), which in Java and in
limited memory (and swapping) requires more time than the iteration of
elements previously created (for example, in the mutation). Adapting
the sub-population size makes the slower node of HeHa behave in similar
way than the other nodes (same time in each stage). Moreover, the size
of the individuals affects to some parts of the EA; for example, in 
OneMax the mutation requires more time than the replacement. However,
it must be taken into account that the duration of each part of the
algorithm is not related to the time to attain the optimum, but rather to
how the diversity and search guidance is maintained in the whole system.  

\begin{figure}[htb]
\centering
\epsfig{file=15.eps, width = 14cm}
\caption{Average running time in each stage of the algorithm for the MMDP problem.}
\label{fig:MMDPbars}
\end{figure}

\begin{figure}[htb]
\centering
\epsfig{file=16.eps, width = 14cm}
\caption{Average running time in each stage of the algorithm for the ONEMAX problem.}
\label{fig:ONEMAXbars}
\end{figure}

\begin{table}[htb]
\centering




\caption{Times of the stages of the algorithm for the MMDP problem (in ms).}
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|} \hline
\multicolumn{6}{|c|}{Heterogeneous Cluster} \\ \hline
Node    & Selection     & Recombination     & Mutation      & Replacement       & Migration         \\ \hline
HoSi HeN1    & 0.077 $\pm$  0.170 &  0.788  $\pm$ 0.779  & 1.004  $\pm$ 0.187 &  1.648  $\pm$ 20.185 & 82.458  $\pm$ 143.266 \\ \hline
HoSi HeN2    & 0.088 $\pm$  0.190 &  0.907 $\pm$  0.932  & 1.145  $\pm$ 0.425 &  1.579  $\pm$ 17.907 & 76.725  $\pm$ 126.360\\ \hline
HoSi HeN3    & 0.105 $\pm$  0.163 &  1.207 $\pm$  0.927  & 1.374  $\pm$ 0.301 &  2.108  $\pm$ 21.848 & 108.605 $\pm$ 142.633\\ \hline
HoSi HeN4    & 1.165 $\pm$  1.526 &  30.445$\pm$  59.553 & 12.221 $\pm$ 7.412 &  10.978 $\pm$ 57.135 & 84.936  $\pm$ 0.000\\ \hline \hline
HeSi HeN1    & 0.067 $\pm$  0.065  & 0.973  $\pm$ 0.403 &  1.411 $\pm$  0.166 &  0.790  $\pm$ 6.266 &  28.081 $\pm$ 42.169 \\ \hline
HeSi HeN2    & 0.062 $\pm$  0.075  & 0.973  $\pm$ 0.470 &  1.433 $\pm$  0.265 &  0.811  $\pm$ 7.056 &  29.667 $\pm$ 48.702 \\ \hline
HeSi HeN3    & 0.066 $\pm$  0.108  & 1.104  $\pm$ 0.346 &  1.435 $\pm$  0.296 &  0.937  $\pm$ 7.072 &  40.964 $\pm$ 40.027 \\ \hline
HeSi HeN4    & 0.109 $\pm$  0.257  & 1.895  $\pm$ 5.611 &  0.913 $\pm$  0.834 &  2.085  $\pm$ 5.626 &  43.880 $\pm$ 7.535 \\ \hline 


\multicolumn{6}{|c|}{Homogeneous Cluster} \\ \hline                                 
Node    & Selection     & Recombination     & Mutation      & Replacement       & Migration \\ \hline
HoSi HoN1    & 0.163 $\pm$  0.223 &  1.884 $\pm$  2.386  & 1.591  $\pm$ 0.479 &  2.254  $\pm$ 5.513  & 40.256  $\pm$ 8.726\\ \hline
HoSi HoN2    & 0.151 $\pm$  0.212 &  1.952 $\pm$  2.876  & 1.597  $\pm$ 0.574 &  2.178  $\pm$ 4.922  & 37.110  $\pm$ 6.999\\ \hline
HoSi HoN3    & 0.154 $\pm$  0.206 &  1.990 $\pm$  3.010  & 1.591  $\pm$ 0.577 &  2.215  $\pm$ 4.743  & 36.413  $\pm$ 5.266\\ \hline
HoSi HoN4    & 0.146 $\pm$  0.196 &  1.913 $\pm$  2.697  & 1.651  $\pm$ 1.167 &  2.194  $\pm$ 5.124  & 38.429  $\pm$ 6.192\\ \hline \hline
HeSi HoN1    & 0.214 $\pm$  0.288  & 2.800  $\pm$ 3.793 &  2.359 $\pm$  0.691 &  2.516  $\pm$ 4.706 &  36.972 $\pm$ 4.214 \\ \hline
HeSi HoN2    & 0.190 $\pm$  0.252  & 2.672  $\pm$ 3.902 &  2.277 $\pm$  0.649 &  2.261  $\pm$ 4.546 &  41.171 $\pm$ 9.672 \\ \hline
HeSi HoN3    & 0.148 $\pm$  0.208  & 2.030  $\pm$ 3.161 &  1.623 $\pm$  0.500 &  2.164  $\pm$ 4.512 &  35.551 $\pm$  6.132 \\ \hline
HeSi HoN4    & 0.045 $\pm$  0.052  & 0.345  $\pm$ 1.121 &  0.217 $\pm$  0.142 &  1.531  $\pm$ 4.856 &  38.106 $\pm$ 9.251 \\ \hline
\end{tabular}
}
\label{tab:mmdptimes}
\end{table}








\begin{table}[htb]
\centering
\caption{Times of the stages of the algorithm for the OneMax problem (in ms).}
\resizebox{14cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|} \hline
\multicolumn{6}{|c|}{Heterogeneous Cluster} \\ \hline
Node    & Selection     & Recombination     & Mutation      & Replacement       & Migration         \\ \hline
HoSi HeN1  &  0.048 $\pm$  0.043  & 18.713 $\pm$ 13.454 & 31.984 $\pm$ 2.104  & 18.375 $\pm$ 197.676 & 1172.986  $\pm$  1108.388 \\ \hline
HoSi HeN2  &  0.052 $\pm$  0.051  & 22.266 $\pm$  22.716 & 33.553 $\pm$ 4.931 &  17.176 $\pm$ 180.580 & 1085.508  $\pm$  995.382 \\ \hline
HoSi HeN3  &  0.091 $\pm$  1.005  & 42.634 $\pm$ 21.621  & 47.674 $\pm$ 0.546 &  26.094 $\pm$ 252.667 & 1708.402 $\pm$   1207.925 \\ \hline
HoSi HeN4  &  0.851  $\pm$ 0.435  & 1491.568 $\pm$ 1185.723 & 344.872$\pm$ 6.634 &  5.655  $\pm$ 16.175 & 154.019 $\pm$0.000 \\ \hline \hline
HeSi HeN1 &   0.072 $\pm$  0.063 &  32.917 $\pm$ 26.792 & 49.103 $\pm$ 2.655  & 3.023 $\pm$  27.647 & 163.479 $\pm$157.172 \\ \hline
HeSi HeN2 &   0.080 $\pm$  0.092 &  43.001 $\pm$ 51.680 & 52.288 $\pm$ 13.210 & 2.527 $\pm$  21.861 & 131.063 $\pm$124.404 \\ \hline
HeSi HeN3 &   0.057 $\pm$  0.052 &  33.951 $\pm$ 15.063 & 41.375 $\pm$ 1.707  & 3.284 $\pm$  30.170 & 186.467 $\pm$163.906 \\ \hline
HeSi HeN4 &   0.075 $\pm$  0.107 &  42.443 $\pm$ 88.536 & 16.236 $\pm$ 12.028 & 4.194 $\pm$  33.119 & 131.135 $\pm$144.359 \\ \hline 
\multicolumn{6}{|c|}{Homogeneous Cluster} \\ \hline                                 
Node    & Selection     & Recombination     & Mutation      & Replacement       & Migration \\ \hline
HoSi HoN1  &  0.091 $\pm$  0.078  & 29.969 $\pm$ 21.459 & 47.445 $\pm$ 2.194 &  2.073 $\pm$  6.970 &  38.782 $\pm$ 40.369 \\ \hline
HoSi HoN2  &  0.093 $\pm$  0.082  & 30.119 $\pm$ 22.029 & 47.247 $\pm$ 2.146 &  2.108 $\pm$  7.440 &  44.303 $\pm$ 42.759 \\ \hline
HoSi HoN3  &  0.089 $\pm$  0.080  & 30.951 $\pm$ 21.904 & 47.103 $\pm$ 2.031 &  2.138 $\pm$  8.006 &  46.107 $\pm$ 47.351 \\ \hline
HoSi HoN4  &  0.098 $\pm$  0.075  & 29.468 $\pm$ 20.876 & 47.086 $\pm$ 1.856 &  2.043 $\pm$  7.491 &  41.458 $\pm$ 44.970 \\ \hline \hline
HeSi HoN1 &   0.144 $\pm$  0.151 &  56.124 $\pm$ 48.229 & 72.811 $\pm$ 5.177  & 2.424 $\pm$  9.056  & 48.165  $\pm$57.798 \\ \hline
HeSi HoN2 &   0.141 $\pm$  0.152 &  51.226 $\pm$ 41.016 & 70.047 $\pm$ 4.152  & 2.427 $\pm$  10.890 & 57.152  $\pm$74.177 \\ \hline
HeSi HoN3 &   0.086 $\pm$  0.088 &  26.932 $\pm$ 20.460 & 42.963 $\pm$ 3.935  & 2.239 $\pm$  8.658  & 51.014  $\pm$49.648 \\ \hline
HeSi HoN4 &   0.007 $\pm$  0.008 &  1.215  $\pm$ 1.133  & 2.470  $\pm$ 0.098  & 1.553 $\pm$  10.078 & 50.498 $\pm$ 63.983 \\ \hline
\end{tabular}
}
\label{tab:onemaxtimes}
\end{table}

\section{Conclusions and future work}
% no cuentes otra vez lo de los computing trends, hombre. Empieza
% diciendo "in this paper we have introduced this o studied that" - JJ FERGU: Cambiada la introducción
%New computing trends, such as Cloud Computing or Service Oriented
%Architecture are providing a massively amount of heterogeneous
%computational devices. 
%In this paper we have performed a study about adapting the
%population size of a distributed Evolutionary Algorithm to the computational
%power of different nodes in a heterogeneous cluster (a cluster with different hardware).


<<<<<<< HEAD
In this paper we describe a study on the adaptation of the population
size of a distributed EA to the computational power of the different
nodes of an heterogeneous cluster. \textcolor{red}{Two adaptation
  schemes (offline and online) have been tested.} % No. Tratamos de
                                % probar un objetivo, dilo aquí!!!!!

Results show that adapting (online or offline) the population size to the computational power of each node in the heterogeneous cluster yields significantly
better results in time than keeping the same parameter in all
nodes. This advantage is due to the combination of the heterogeneous
parameters with the heterogeneity of the machines. % o sea, tener
                                % maquinas heterogéneas y parámetros
                                % heterogéneos es mejor porque usamos
                                % parámetros heterogéneos en máquinas
                                % heterogéneas. Di en qué puede
                                % influir eso en la mejora de los
                                % resultados y discute por qué podría
                                % ser así y propón experimentos para
                                % probar que efectivamente se trata de
                                % eso. - JJ
On the contrary,
the same (heterogeneous) parameter setting in all islands of the
homogeneous cluster could not improve the results than considering the
same parameter value in all nodes. % ¿Y qué más da?  ¿Por qué es esto
                                % relevante? Di que, por tanto, la
                                % mejora no se debe al cambio de
                                % parámetros sólo, sino a la
                                % combinación entre cambio de
                                % parámetros y adaptación al nodo - JJ 
=======
In this paper we describe a study on the adaptation of the sub-population sizes of a distributed EA to the computational power of the different nodes of an heterogeneous cluster. Two adaptation schemes (offline and online) that use information of the computational load of the algorithm have been tested.

Results show that adapting (online or offline) the sub-population size to the computational power of each node in the heterogeneous cluster yields significantly
better results in time than keeping the same parameter in all nodes. This advantage is due to the combination of the heterogeneous parameters with the heterogeneity of the machines. On the contrary, the same (heterogeneous) parameter setting in all islands of the homogeneous cluster could not improve the results than considering the same parameter value in all nodes.
>>>>>>> 820357f3e027f2313f2ba89182bc486b395ab386

%To obtain a fair parameter configuration,
%this parameter has been obtained proportionally to the attained
%average generations of each node in executions with the same number of
%individuals. Results show that adapting the population size to the computational power decreases
%the execution time significantly in heterogeneous clusters, while
%changing this parameter in homogeneous clusters does not always
%performs better. 

Furthermore, changing the sub-population size affects to stages
of the algorithm that are independent of this parameter, such as
the migration. The sub-population size adaptation is also affected by the problem to solve.

In this work, as a possible offline parameter setting, we have calculated the computational power of each node proportionally 
to the average number of generations of the homogeneous parameter set. Moreover, a possible way to adapt 
online the sub-population sizes has been performed comparing the current generation with
 the neighbour generation. These results are a promising starting for adapting EAs to the
performance of each execution node, using more adequate benchmarks or in a dynamic way. 
% Falta una discusión sobre si las mejoras se deben exclusivamente al
% número de evaluaciones o hay algún otro factor ¿menos overhead?
% ¿nodos más rápidos? - JJ FERGU: no, de hecho el número de evaluaciones no siempre disminuye, lo digo arriba.

In the future it would be interesting to check the scalability of this
approach, using more computational nodes and larger problem
instances. In addition, other parameters such as migration rate or
crossover probability could be adapted to the execution
nodes. Other appropriate benchmarks to analyse the algorithm will be also used to lead to automatic
parameter adaptation in runtime (online), with different nodes entering or
exiting in the topology, or adapting the parameters to the current load of the
system. 

\section*{Acknowledgements}
This work has been supported in part by FPU research grant AP2009-2942 and projects EvOrq (TIC-3903), CANUBE (CEI2013-P-14) and ANYSELF (TIN2011-28627-C04-02).
%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}         ==>>  [#]
%%   \cite[chap. 2]{key} ==>> [#, chap. 2]
%%

%% References with bibTeX database:

%\bibliographystyle{elsarticle-num}
%\bibliography{AMIVITAL-ESA}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use elsarticle-num.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}




%\bibliographystyle{plain}
%\bibliography{heterogeneous}
\section*{References}

%\bibliographystyle{plain}
%\bibliography{heterogeneous}


\begin{thebibliography}{10}

\bibitem{EVALUATIONPARALLEL}
E.~Alba and G.~Luque.
\newblock Evaluation of parallel metaheuristics.
\newblock In Springer, editor, {\em Parallel Problem Solving from Nature
  (PPSN)}, volume 4193 of {\em LNCS}, pages 9--14, 2006.

\bibitem{HETEROGENEOUSHARD}
Enrique Alba, Antonio~J. Nebro, and Jos\'e~M. Troya.
\newblock Heterogeneous computing and parallel genetic algorithms.
\newblock {\em Journal of Parallel and Distributed Computing}, 62(9):1362 --
  1385, 2002.

\bibitem{OPENSCIENCEGRID}
Mine Altunay, Paul Avery, Kent Blackburn, Brian Bockelman, Michael Ernst, Dan
  Fraser, Robert Quick, Robert Gardner, Sebastien Goasguen, Tanya Levshina,
  Miron Livny, John McGee, Doug Olson, Ruth Pordes, Maxim Potekhin, Abhishek
  Rana, Alain Roy, Chander Sehgal, Igor Sfiligoi, Frank Wuerthwein, and {Open
  Sci Grid Executive Board}.
\newblock {A Science Driven Production Cyberinfrastructure-the Open Science
  Grid}.
\newblock {\em {Journal of GRID Computing}}, {9}({2, Sp. Iss. SI}):{201--218},
  {JUN} 2011.

\bibitem{MULTIKULTI}
Lourdes Araujo and Juan Juli{\'a}n~Merelo Guerv{\'o}s.
\newblock Diversity through multiculturality: Assessing migrant choice policies
  in an island model.
\newblock {\em IEEE Trans. Evolutionary Computation}, 15(4):456--469, 2011.

\bibitem{LoadBalancingBohn02}
Christopher~A. Bohn and Gary~B. Lamont.
\newblock Load balancing for heterogeneous clusters of {PCs}.
\newblock {\em Future Generation Computer Systems}, 18(3):389 -- 400, 2002.

\bibitem{CLOUD}
Rajkumar Buyya, Chee~Shin Yeo, Srikumar Venugopal, James Broberg, and Ivona
  Brandic.
\newblock Cloud computing and emerging it platforms: Vision, hype, and reality
  for delivering computing as the 5th utility.
\newblock {\em Future Generation Computer Systems}, 25:599--616, June 2009.

\bibitem{HYDROCM}
Juli\'an Dom\'inguez and Enrique Alba.
\newblock {HydroCM}: A hybrid parallel search model for heterogeneous
  platforms.
\newblock In El-Ghazali Talbi, editor, {\em Hybrid Metaheuristics}, volume 434
  of {\em Studies in Computational Intelligence}, pages 219--235. Springer
  Berlin Heidelberg, 2013.

\bibitem{LinpackDongarra03}
Jack~J. Dongarra, Piotr Luszczek, and Antoine Petitet.
\newblock The {LINPACK} benchmark: Past, present, and future.
\newblock {\em Concurrency and Computation: Practice and Experience}, 15:2003,
  2003.

\bibitem{PARAMETERTUNING}
A.~E. Eiben and Selmar~K. Smit.
\newblock Parameter tuning for configuring and analyzing evolutionary
  algorithms.
\newblock {\em Swarm and Evolutionary Computation}, 1(1):19--31, 2011.

\bibitem{GeneticAlgorithmsEiben03}
A.E. Eiben and J.E. Smith.
\newblock Genetic algorithms.
\newblock In {\em Introduction to Evolutionary Computing}, Natural Computing
  Series, pages 37--70. Springer Berlin Heidelberg, 2003.

\bibitem{LinpackEndo10}
T.~Endo, A.~Nukada, S.~Matsuoka, and N.~Maruyama.
\newblock {LINPACK} evaluation on a supercomputer with heterogeneous
  accelerators.
\newblock In {\em Parallel Distributed Processing (IPDPS), 2010 IEEE
  International Symposium on}, pages 1--8, 2010.

\bibitem{SelfRegulatedSizeFernandes06}
Carlos Fernandes and Agostinho Rosa.
\newblock Self-regulated population size in evolutionary algorithms.
\newblock In {\em Parallel Problem Solving from Nature - PPSN IX}, volume 4193
  of {\em Lecture Notes in Computer Science}, pages 920--929. Springer Berlin
  Heidelberg, 2006.

\bibitem{GLOBUS}
I~Foster.
\newblock {Globus Toolkit version 4: Software for service-oriented systems}.
\newblock In {Jin, H and Reed, D and Jiang, W}, editor, {\em {Network and
  Parallel Computing Proceedings}}, volume {3779} of {\em {Lecture Notes in
  Computer Science}}, pages {2--13}, 2005.

\bibitem{PARALLELIMPLEMENTATION}
J.F. Garamendi and J.L. Bosque.
\newblock Parallel implementation of evolutionary strategies on heterogeneous
  clusters with load balancing.
\newblock In {\em Parallel and Distributed Processing Symposium, 2006. IPDPS
  2006. 20th International}, page 8 pp., april 2006.

\bibitem{SOASOCO}
P.~Garc{\'i}a-S{\'a}nchez, J.~Gonz{\'a}lez, P.A. Castillo, M.G. Arenas, and
  J.J. Merelo-Guerv{\'o}s.
\newblock Service oriented evolutionary algorithms.
\newblock {\em Soft Computing}, 17(6):1059--1075, 2013.

\bibitem{goldberg92massive}
David~E. Goldberg, Kalyanmoy Deb, and Jeffrey Horn.
\newblock Massive multimodality, deception, and genetic algorithms.
\newblock In {R. M\"{a}nner} and B.~Manderick, editors, {\em Parallel Problem
  Solving from Nature, 2}, pages 37--48, Amsterdam, 1992. Elsevier Science
  Publishers, B. V.

\bibitem{HETEROGENEOUSPARAMETERS}
Yiyuan Gong and Alex Fukunaga.
\newblock Distributed island-model genetic algorithms using heterogeneous
  parameter settings.
\newblock In {\em Proceedings of the IEEE Congress on Evolutionary Computation,
  CEC 2011, New Orleans, LA, USA, 5-8 June, 2011}, pages 820--827. IEEE, 2011.

\bibitem{HETEROGENEOUSTOPOLOGY}
Yiyuan Gong, Morikazu Nakamura, and Shiro Tamaki.
\newblock Parallel genetic algorithms on line topology of heterogeneous
  computing resources.
\newblock In {\em Proceedings of the 2005 conference on Genetic and
  evolutionary computation}, GECCO '05, pages 1447--1454, New York, NY, USA,
  2005. ACM.

\bibitem{SizingHarik99}
George Harik, Erick Cant\'{u}-Paz, David~E. Goldberg, and Brad~L. Miller.
\newblock The gambler's ruin problem, genetic algorithms, and the sizing of
  populations.
\newblock {\em Evolutionary Computation}, 7(3):231--253, September 1999.

\bibitem{ShrinkageLaredo09}
Juan Lu\'{\i}s~J. Laredo, Carlos Fernandes, Juan~Juli\'{a}n Merelo, and
  Christian Gagn{\'e}.
\newblock Improving genetic algorithms performance via deterministic population
  shrinkage.
\newblock In {\em Proceedings of the 11th Annual conference on Genetic and
  evolutionary computation}, GECCO '09, pages 819--826, New York, NY, USA,
  2009. ACM.

\bibitem{AdaptiveLobo07}
Fernando~G. Lobo and Cl\'{a}udio~F. Lima.
\newblock Adaptive population sizing schemes in genetic algorithms.
\newblock In Fernando~G. Lobo, Cl\'{a}udio~F. Lima, and Zbigniew Michalewicz,
  editors, {\em Parameter Setting in Evolutionary Algorithms}, volume~54 of
  {\em Studies in Computational Intelligence}, pages 185--204. Springer Berlin
  Heidelberg, 2007.

\bibitem{AsynchronousMerelo08}
J.J. Merelo-Guervos, P.A. Castillo, J.~L.~J. Laredo, A.~Mora~Garcia, and
  A.~Prieto.
\newblock Asynchronous distributed genetic algorithms with javascript and
  {JSON}.
\newblock In {\em Evolutionary Computation, 2008. CEC 2008. (IEEE World
  Congress on Computational Intelligence). IEEE Congress on}, pages 1372--1379,
  2008.

\bibitem{PARALLELGRIDHETEROGENEOUS}
Sergio Nesmachnow, H{\'e}ctor Cancela, and Enrique Alba.
\newblock A parallel micro evolutionary algorithm for heterogeneous computing
  and grid scheduling.
\newblock {\em Applied Soft Computing}, 12(2):626 -- 639, 2012.

\bibitem{PLATO}
Andres~J. Ramirez, David~B. Knoester, Betty H.~C. Cheng, and Philip~K.
  McKinley.
\newblock Plato: a genetic algorithm approach to run-time reconfiguration in
  autonomic computing systems.
\newblock {\em Cluster Computing}, 14(3):229--244, 2011.

\bibitem{HETEROGENEOUSMIGRATION}
Carolina Salto and Enrique Alba.
\newblock Designing heterogeneous distributed gas by efficiently self-adapting
  the migration period.
\newblock {\em Applied Intelligence}, 36:800--808, 2012.

\bibitem{ONEMAX}
J.D. Schaffer and L.J. Eshelman.
\newblock {On Crossover as an Evolutionary Viable Strategy}.
\newblock In R.K. Belew and L.B. Booker, editors, {\em {Proceedings of the 4th
  International Conference on Genetic Algorithms}}, pages 61--68. Morgan
  Kaufmann, 1991.

\bibitem{AutomaticallyConfiguringStyles12}
James Styles, HolgerH. Hoos, and Martin M√ºller.
\newblock Automatically configuring algorithms for scaling performance.
\newblock In Youssef Hamadi and Marc Schoenauer, editors, {\em Learning and
  Intelligent Optimization}, Lecture Notes in Computer Science, pages 205--219.
  Springer Berlin Heidelberg, 2012.

\end{thebibliography}

\end{document}

%%
%% End of file `elsarticle-template-num.tex'.
