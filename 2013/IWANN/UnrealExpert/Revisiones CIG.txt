----------------------- REVIEW 1 ---------------------
PAPER: 52
TITLE: Designing and Evolving an Unreal Tournament 2004 Expert Bot
AUTHORS: Antonio Mora, Ricardo Caballero, Francisco Aisa, Pablo García Sánchez, Jj Merelo and Pedro Castillo

OVERALL RATING: -1 (weak reject)
REVIEWER'S CONFIDENCE: 3 (high)

== Contribution

Briefly summarize this paper and its contributions to CIG community. State the novel ideas or advances over previous work. How significant are the contributions?

This paper describes the implementation of high-performance NPC for Unreal Tournament using knowledge acquisition from expert players as well as GA techniques to evolve the bot. The former part of the approach has resulted in notable performance improvements but the latter resulted in considerable effort and apparently meagre enhancements. A considerable amount of ad hoc developments, which culminates in the flowchart descriptions included in the paper, raises a few issues on reproducibility and transferability of results. 


== References

Are the references and their discussion adequate? Please list any missing references that should be added.

Overall, yes. The authors could have included more references on the Soar Quakebot, in particular with regard to knowledge acquisition from human expert players, which is an approach they share with the Soar Quakebot. Previous references from the authors raise the issue of how much novelty there is in the present paper.


== Strength of argument
Does the paper make a sufficiently convincing argument, based on the data or evidence gathered? Does it extend knowledge into new areas, build on prior work, and state its limitations and drawbacks?

The authors are very honest about the actual performance achieved and limitations of the approach, which is why my rating remains moderately negative only. Results from the EBot are interesting, albeit not really novel if one considers the Soar Quakebot. The main limitations could be trying to apply ML techniques to a form of behaviour which remains predominantly procedural and my advice for future work would be to move away from this philosophy to explore more declarative representations. 


== Organization

Is this paper well organized and well written? Is it easy to
understand? Are the grammar and spelling correct? If improvements are needed, state them here.

The paper is overall readable and well illustrated, although some insight into spatial aspects of the the deathmatches (sample trajectories within a level + game action records) would be helpful. Too much space is dedicated to introducing gameplay principles for Unreal Tournament, which may be known to the reader or not differ significantly from other FPS.


== Detailed explanation

Provide a detailed explanation of the reasoning behind your
rating. This section will strongly influence the Program Committee and Program Chairs in making the final decision and help the authors improve their paper. Include comments that will help the authors understand your judgement of the paper.

The paper's main contribution is in outlining the difficulties of applying GA in this context. It still contains positive results but these are not really novel. It is unclear how to convey the message properly. Are the limitations seen a consequence of the difficulties in applying GA to the context, or is there a fundamental flaw in the design of the approach because of the procedural model of behaviour? I cannot respond to this myself and have only put it forward as an hypothesis.


----------------------- REVIEW 2 ---------------------
PAPER: 52
TITLE: Designing and Evolving an Unreal Tournament 2004 Expert Bot
AUTHORS: Antonio Mora, Ricardo Caballero, Francisco Aisa, Pablo García Sánchez, Jj Merelo and Pedro Castillo

OVERALL RATING: 0 (borderline paper)
REVIEWER'S CONFIDENCE: 2 (medium)

== Contribution

The paper presentes and evaluates the implementation of an unreal bot based on the domain knowledge of an expert unreal player and the application of evolution for parameters tuning.


== References

The references seem adequate.


== Strength of argument
The main issue with the paper lies in its argument. It is unclear what does the paper intend to prove and how does this discovery impact the current state of the art.
The initial implementation of the e-bot appears more as a successful engineering task which does not provide any new insight about the issue; moreover, the simple application of evolution for bots parameter tuning has been already proved to be effective by the authors in previous publications.
There is a statement in the conclusions ("The aim is to deal with the noisy nature of the fitness function, since it depends on the results of the battles...") which points to an actual research issue in evolutionary computation in games; however there is little or no analysis of the matter in the article.


== Organization

The paper appears well organised and written.
There are some minor issues with unjustified statements such as: "FPSs are multiplayer fighting games placed in a limited arena" or "UnrealTM [1], launched for PCs by Epic Games in 1998, had a great success since it incorporates the best multiplayer mode to date".
The first statement is simply wrong as the genre contains for more types of games; while the second claim is absolutely arbitrary and not backed by any reference.


== Detailed explanation

I would suggest to define better the aim of the paper in scientific terms and, therefore, design an evaluation that is able to support the so stated hypothesis.


----------------------- REVIEW 3 ---------------------
PAPER: 52
TITLE: Designing and Evolving an Unreal Tournament 2004 Expert Bot
AUTHORS: Antonio Mora, Ricardo Caballero, Francisco Aisa, Pablo García Sánchez, Jj Merelo and Pedro Castillo

OVERALL RATING: -2 (reject)
REVIEWER'S CONFIDENCE: 3 (high)

Designing and Evolving an Unreal Tournament 2004 Expert Bot

== Contribution

Briefly summarize this paper and its contributions to CIG community. State the
novel ideas or advances over previous work. How significant are the
contributions?

In this paper, the authors describe the design and implementation of an UT
2004 bot based on human expert play. The bot uses a database to memorize
locations of interest in the map as they are discovered. Additionally,
parameters used to control behavior in the expert bot are then subject to
evolutionary computation, improving the bot's playing ability.

There are potentially three contributions here: using an expert to develop and
implement the initial rule set, using the two-level states for primary and
secondary tasks, and learning parameters that control the behaviors (using an
evolutionary computation approach). Each of these contributions is
interesting, though not entirely novel.

(The database used to store the locations of items as discovered is perhaps
useful, but not of major interest here. It's not clear how this is an
improvement over just programming in the locations of the items.)


== References

Are the references and their discussion adequate? Please list any missing references that should be added.

There is way too much towards the front of the paper about things like the
history of first-person shooters and the Unreal series. These do not
contribute to this paper, and should be removed. (They are likely of interest
to the community, and it would be interesting to write a separate paper about
this.) 

Similarly, there is too much info here about the process of creating bots for
UT2K4. All this detail is not the point of this paper.

Conversely, a set of references to other CI efforts for FPS bots is presented
as a laundry list of references, without even explaining each individual
contribution. This is incomplete in its presentation and also an incomplete
literature review. Furthermore, given that the learning is just parameters
that affect the core behavior engine, an overview of other examples of
learning parameters to control bot behaviors for videogames would be appropriate.


== Strength of argument
Does the paper make a sufficiently convincing argument, based on the data or
evidence gathered? Does it extend knowledge into new areas, build on prior
work, and state its limitations and drawbacks?

The experiments reported here are modest, though still interesting.


== Organization

Is this paper well organized and well written? Is it easy to
understand? Are the grammar and spelling correct? If improvements are needed,
state them here.

This paper is a lot of work to read. The right level of information is not
presented in most cases. Superfluous things are reported at great length,
while important details are omitted. Additionally, there are many errors in
the use of English.


== Detailed explanation

Provide a detailed explanation of the reasoning behind your
rating. This section will strongly influence the Program Committee and Program
Chairs in making the final decision and help the authors improve their
paper. Include comments that will help the authors understand your judgement
of the paper.

I spent a fair amount of time with this paper because I think the project is
quite interesting and I wanted to understand what was going on. But
ultimately, I feel the presentation is off enough to warrant rejection. I
don't feel that it's likely that the authors can clean this up enough for a
coherent presentation. This is good work, though, and I strongly support
revision and resubmission to CIG or another related venue.

Point one is that there are perhaps three papers worth of ideas in this paper,
and that might be the primary source of the difficulties. One of these papers
would be a survey of FPS and bots, as mentioned previously. A second would be
the use of the expert human as both a subject matter expert and as a coding
collaborator, and creating the intial non-learning system. The third paper
would focus on the GA work.

In multiple places, this paper does not meet the standard of reporting in
sufficient detail that someone else could reproduce the results. None of us
are perfect in this regard, but there's quite a bit missing here. For example,
the implementation of the secondary tasks is not described in anywhere near
enough detail to be understood, much less implemented by another researcher.

The secondary states can't be understood in part because as described, there
is no reason for them to be mutually exclusive in many cases, and in other
cases, they are necessarily mutually exclusive (e.g., offensive vs. defensive
profile). Regardless, while it's clear that there is just one secondary state
at a time, it's not clear how the secondary state is decided. It's also not
clear, and again, this is just another example, what the representation for
the GA rules is. Are the "genes" refered to bits? (it appears so; why not say
"bits"? Why are three genes needed for distance, when there are three mutually
exclusive values? The 126 genes vs. 26 for weapons has not been explained.
etc. It is also frustrating that even what's going on with the core expert
bot is not clear.

How was the fitness function designed? It seems to be very ad hoc.

The GA implementation is quite quirky, and again, not defended or
explained. Note that crossing over an evaluated parent with a new random
parent ends up being like a high mutation rate, and is not what's usually
considered to be a crossover. (There are lots of quirks; that's just one.)

I recommend to the authors that they make their peace with the fact that
learning takes time, and that they are doing themselves a disservice by trying
to evaluate a GA string without enough playing time. This creates a fitness
function that is noisier than necessary. E.g., if you allowed yourself 10
repetitions of each run, the noise would be mitigated. 10 times may be too
much, but three is probably not sufficient, and one certainly isn't.


A couple of minor issues:
"Frag" should be defined/clarified at first use, so that people in the games
community, but who do not play FPS, are informed.

The text in Figures 1 and 2 are way too small.

The colors in Figure 3 aren't doing anything and are therefore just confusing.
