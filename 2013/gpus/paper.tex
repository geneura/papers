\documentclass[prodmode,acmtecs]{acmsmall}

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\usepackage{eurosym}         % euro symbol
\usepackage[utf8]{inputenc}  % text encoding
\usepackage[dvips]{epsfig}
\usepackage{graphicx,times,amsmath,url}
\usepackage{url}

\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
\acmVolume{x}
\acmNumber{x}
\acmArticle{xx}
\acmYear{2012}
\acmMonth{12}

% Document starts
\begin{document}

% Page heads
\markboth{G. Romero et al.}{GPU Parallel Computation in Bioinspired Algorithms. A review}

% Title portion
\title{GPU Parallel Computation in Bioinspired Algorithms. A review}

%\author{G. Romero, M.G. Arenas, P.A. Castillo, A.M. Mora, P. Garcia and J.J. Merelo \affil{University of Granada}}

\author{G. ROMERO \affil{University of Granada}
M.G. ARENAS \affil{University of Granada}
P.A. CASTILLO \affil{University of Granada}
A.M. MORA \affil{University of Granada}
P. GARCIA-SANCHEZ \affil{University of Granada}
J.J. MERELO \affil{University of Granada}}

%Department of Architecture and Computer Technology. CITIC (University of Granada), \email{{gustavo,mgarenas,pedro,amorag,pgarcia,jmerelo}@atc.ugr.es }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Computer algorithms can solve large problems. Run times can be high if their execution require a high amount of computational power. Parallelization is an extended approach for improving time and quality of the solutions. Advances in the gaming industry led to the production of low-cost and high performance graphics processing units (GPUs) which offer more computational resources than traditional central processing units (CPUs). GPUs have arrive to PCs and mobile devices as these units are now included in every one of them. This work presents a review of hardware and software advances in the area of bioinspired methods.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\category{I.3.1}{Hardware Architecture}{Graphics processors, Parallel processing}

\terms{GPUs, Parallel Computation, Bioinspired Algorithms, Review}

\keywords{GPUs, Parallel Computation, Bioinspired Algorithms, Review}

\acmformat{G. Romero, M.G. Arenas, P.A. Castillo, A.M. Mora, P. García-Sánchez and J.J. Merelo. 2012. GPU Parallel Computation in Bioinspired Algorithms. A review.}

\begin{bottomstuff}
Author's addresses: Department of Architecture and Computer Technology. CITIC (University of Granada).
\end{bottomstuff}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

General-purpose computing on graphics processing units (GPGPU) is the technique of using a Graphics Processing Unit (GPU), which typically handles computation only for computer graphics, to perform computation in applications traditionally handled by the central processing unit (CPU). Recently there has been a growing interest in GPU computation. The fact that this kind of processors has the ability to perform restricted parallel processing has elicited considerable interest among researchers with applications that require intensive parallel computation.

\begin{figure}[h]
\centerline{\epsfig{file=./openclfigure2small.eps,width=10cm}}
\caption{GPUs can be seen as SIMD multi-core processors. Internally the GPU contains a number of small processors that are used to perform calculations. Depending on the GPU, the number of threads that can be executed in parallel is in the order of hundreds to thousands.}
\label{figure:openclfigure2small}
\end{figure}

GPUs are specialized stream processors, initially useful for rendering graphics applications. Typically, a GPU is able to perform graphics manipulations at a much higher speed than a general purpose CPU, since the graphics processor is specifically designed to handle certain primitive operations which occur frequently in graphics applications. Internally, the GPU contains a number of small processors that are used to perform calculations. 
Depending on the power of a GPU, the number of threads that can be executed in parallel on such devices is currently in the order of hundreds and it is expected to multiply in a few years. Nowadays, developers can write (easily) their own high-level programs on GPU. Due to the wide availability, programmability, and high-performance of these consumer-level GPUs, they are cost-effective for, not just game playing, but also scientific computing.
Now, GPUs are exposed to the programmer as a set of general-purpose shared-memory SIMD (Single Instruction Multiple Data) multi-core processors (see Figure \ref{figure:openclfigure2small}). This makes these architectures well suited to run large computational problems, such as those from bioinformatics area. 

Then, the goal of this article is to review the use of GPUs to solve bioinformatics problems, explaining the general approach to using a GPU and given an overview of currently available software systems.

To this end, the rest of this paper is structured as follows: Section \ref{sec:parall_and_GPUs} presents GPUs as highly parallel devices architectures.
Section \ref{sec:programming} gives a background on the different higher level programming languages used to profit GPUs. Finally, Section \ref{sec:bioinfor_apps} reviews different bioinspired approaches using GPUs, followed by a brief conclusion (Section \ref{sec:conclusions}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Throughput, parallelism and GPUs}
\label{sec:parall_and_GPUs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Moore's Law describes a long-term trend in the history of computing hardware: the number of transistors that can be placed inexpensively on an integrated circuit has doubled approximately every two years. The trend has continued for more than half a century and is not expected to stop (theoretically until not too many years above 2015). On 2005 Gordon Moore stated in an interview that his law cannot be sustained indefinitely because transistors would eventually reach the limits of miniaturization at atomic levels. Maybe it is time for Koomey's Law \cite{10.1109/MAHC.2010.28} to replace Moore's Law. Koomey says that energy efficiency is doubled every 18 months. For fixed computing load, the amount of battery you need will fall by a factor of two every year and a half.

Parallel computation has recently become necessary to take full advantage of the gains allowed by Moore's law. For years, processor makers consistently delivered increases in clock rates and instruction-level parallelism, so that single-threaded code is executed faster on newer processors with no modification. Now, to manage CPU power dissipation, processor makers favor multi-core chip designs, and software has to be written in a multi-threaded or multi-process manner to take full advantage of the hardware.

Graphics processors have rapidly matured over the last years, leaving behind their roots as fixed function accelerators, and growing into almost general purpose computational devices for highly parallel workloads. Some of the earliest academic work about GPUs as computational devices date back to University of Washington in 2002 \cite{Thompson:2002:UMG:774861.774894} and Stanford in 2004 \cite{Buck:2004:BGS:1015706.1015800}. 

GPUs are similar to multi-core CPUs but with two main differences (see Figure \ref{figure:captura2paperCEC}). CPUs are made for speedup and GPUs for throughput. CPUs try to improve the execution of a single instruction stream while GPUs take the opposite route obtaining benefits from massively threaded streams of instructions and/or data (SIMD). The second difference is how threads are scheduled. The operating system schedule threads over different cores of a CPU in a pre-emptive fashion. GPUs have dedicated hardware for the cooperative scheduling of threads.

\begin{figure}[h]
\centerline{\epsfig{file=captura2paperCEC.eps,width=10cm}}
\caption{CPU-GPU blocks arrangement: The GPU architecture devotes more transistors to data processing.}
\label{figure:captura2paperCEC}
\end{figure}

Physically GPUs are huge in comparison with CPUs, see Table \ref{fig:cpugpu}. Latest microprocessors from the two main vendors, AMD \cite{amd} and Intel \cite{intel}, have about 1 billion transistors. Latest GPUs from AMD and NVIDIA \cite{nvidia} are about 3 billion transistors. CPUs draw 130W at most, a limit established by the cost of commodity heat sink and fan. GPUs have increase power consumption and currently are in the neighborhood of 300W. This can be possible with the use of exotic cooling solutions. CPUs are built with the finest technology, read best lithography, while GPUs are made with budget in mind in more common and older processes.

No standard terminology can be found in the world of graphics hardware.
Every company have a very different set of words to refer to the same underlying objects and principles. Many authors try to alleviate this lack of standard terms calling 'shader core' to an Intel Execution Unit (EU), an AMD Single Instruction Multiple Data (SIMD) or an NVIDIA Streaming Multiprocessors (SM). Any of these refer to a single processor core inside of the GPU that can fetch, decode, issue and execute several instructions. The shader core is composed of several 'execution units' or EU that can execute an individual vector operation equivalent to an Advanced Vector Extensions (AVX) or Streaming SIMD Extensions (SSE) instruction. AMD call this kind of EU streaming processor (SP), while NVIDIA uses the term CUDA core. Table \ref{fig:cpugpu} compares several existing CPUs, GPUs and Accelerated Processing Units (APUs), taking into account these terms and their meaning.

\begin{table}[h]
\tbl{CPU, GPU and APU comparison of the best professional and commodity desktop hardware available nowadays. A slash is used in APUs to separate CPU/GPU parts.\label{fig:cpugpu}}
{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
manufacturer           & transistor & die      & shader & clock      & memory    & GFLOPS     & TDP & price   \\
\&                     & count      & size     & cores  & rate       & bandwidth & (single    &     &         \\
model                  & (million)  & ($mm^2$) & (ALUs) & (GHZ)      & (GB/s)    & precision) & (W) & (\euro) \\
\hline
\hline
AMD Phenom II X6 1100T & 758        & 258      & 6      & 2.6-3.7    & 15.6     & 57.39 & 125 & 175 \\
\hline
Intel Core i7 990X     & 1170       & 240      & 6      & 3.46-3.73  & 24.5     & 107.58 & 130 & 950 \\
\hline
\hline
AMD A8-3850            & 758        & 258      & 4/400  & 2.9/0.6    & 29.8     & 355    & 100  & 135 \\
\hline
Intel Core i7 2600K    & 995        & 216      & 4/48   & 3.4/0.85 & 24.5  & 129.6 & 95  & 317 \\
\hline
\hline
AMD Radeon HD 6970     & 2640       & 389      & 1536   & 0.88       & 176    & 2703 & 250 & 350 \\
\hline
NVIDIA GeForce GTX 580 & 3000       & 520      & 512    & 1.544      & 192.4  & 1581.1 & 244 & 450 \\
\hline
\hline
AMD FirePro S9000      &         4310  &         352  &         1792  & 0.900 &         264  &         3230    & 225 & 2100 \\
\hline
AMD FirePro S10000     & $2\times4310$ & $2\times352$ & $2\times1792$ & 0.825 & $2\times240$ & $2\times2956.8$ & 375 & 3000 \\
\hline
NVIDIA Tesla G20X      & 7100          & 550          & 2688          & 0.732 & 250          & 3950            & 235 & $>2700$ \\
\hline
\hline
Intel Xeon Phi 5100P   & $\approx5000$ & $\approx600$ & 60            & 1053  & 320          & 2020            & 225 &  2200 \\
\hline

\end{tabular}
}
\end{table}

Nowadays NVIDIA has the best single chip GPU card with superior performance in some workloads and AMD the best card with two GPU chips. Both makers takes different compromises in the design of theirs GPUs. AMD has more execution units but its memory hierarchy is weaker. This way software bounded by memory bandwidth or with strong ordering inter-dependencies prefers NVIDIA hardware. On the other side, loads capped by pure ALU execution power use to be faster on AMD hardware.

Recently, CPU with an integrated GPU have appeared. AMD and Intel have just started selling this kind of combined processor and graphics card. The term Accelerated Processing Unit has been coined for this kind of chips. The architectural names are Llano for AMD and Sandy Bridge for Intel. Many reviews state that AMD's CPU cores are slower than Intel's ones but their GPU is faster. 
Which combination is better is not an easy question to answer. It must be backed by specific benchmarks, or better, the real application that we want it to run.

Over time the use of GPUs has passed from odd to common in our systems. Actually, several time consuming processes has been parallelized inside our operating systems such as web page rendering. However, the speedup that graphics hardware can bring to us is not free, as each application to be accelerated must be rewritten (parallelized). 

Figure \ref{fig:gk110}a shows the internals of latest NVIDIA Kepler GK110 architecture. There can be a maximum of 15 SMX units and every one of this has 192 single precision CUDA cores. The six squares on the sides are memory interfaces for a total bus width of 384 bits. It is capable of a maximum bandwidth of 250GB/s connected to GDDR5 memory chips. Figure \ref{fig:gk110}b shows a disclosed SMX unit with its 192 CUDA cores. The maximum theoretical throughput in simple precision is 3950GFLOPS.

\begin{figure}[h]
\begin{tabular}{cc}
\epsfig{file=./nvidia-gk110.eps,height=5.20cm} & \epsfig{file=./nvidia-smx.eps,height=5.20cm} \\
(a) Block diagram. & (b) SMX unit. \\
\end{tabular}
\caption{NVIDIA Kepler GK110 architecture.}
\label{fig:gk110}
\end{figure}

In the Figure \ref{fig:tahiti}a the newest AMD GPU can be seen. It has an architecture called Graphics Core Next (GCN). Every chip, there are 2, is composed by 28 Compute Units (CU) as AMD call it. Every CU is composed by 4 SIMD vector processors. Every SIMD processor can operate on 16 number simultaneously. This way 1792 instructions can be finalized every clock. Every Tahiti chip has six 64-bit dual channel memory controllers (gray rectangles at the bottom of Figure \ref{fig:tahiti}b) connected to 2 GDDR5 memory channels for a total bandwidth of 240GB/s. The maximum theoretical throughput in simple precision for this design is 2956GFLOPS.

\begin{figure}[h]
\begin{tabular}{cc}
\epsfig{file=./amd-tahiti.eps,width=6.5cm} & \epsfig{file=./amd-gcn-cu-2.eps,width=6.5cm} \\
(a) Block diagram. & (b) Compute Unit, a SIMD processor. \\
\end{tabular}
\caption{AMD Graphics Core Next architecture.}
\label{fig:tahiti}
\end{figure}

As CPU makers did some years ago, passing from single core to symmetric multiprocessing system (SMP), and more recently to multi-cores, GPU makers follow the same trend. We can connect more than one graphic card to our computer to improve its GPU capacities or buy a card with 2 graphic chips inside. GPUs are so much powerful than CPUs that even a small cluster of a few GPUs can be faster than classic, and much more expensive, big cluster of processors. First cluster of this kind appear in the scientific literature in 2004 \cite{10.1109/SC.2004.26} with big success. Nowadays, more and more people build small GPU clusters with a couple of mighty graphic cards just to game. Connecting 2, 3 or 4 graphics card is called CrossFire by AMD and Scalable Link Interface (SLI) by NVIDIA.

The information shown in this and next sections is available from the websites of the respective manufacturer: AMD \cite{amd}, Intel \cite{intel} and NVIDIA \cite{nvidia}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{GPUs Programming}
\label{sec:programming}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Programming Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The way GPUs can be exploited is deeply rooted on its hardware. There exists several APIs: every company has a proprietary one tied to their respective products. This way AMD started with Close to Metal and NVIDIA with CUDA. Over time another standard appear, OpenCL \cite{opencl}.

With respect to the programming tools available for developers, most the Application Program Interfaces (APIs) are based on C-like languages, but having some restrictions to improve the parallel execution, such as no recursion or limited pointers. Some of them use the open source compiler LLVM \cite{LLVM} from University of Illinois.

From 2003 the two main GPU developers, ATI an NVIDIA, started selling hardware solutions that need to be programmed with proprietary APIs. Despite previous work, the first widely supported GPUs were DX10 generation GeForce 8 series from NVIDIA, using the more mature CUDA API. On the other hand, the Radeon HD2xxx series from ATI, were programmed with the Close To Metal API. 

Paying attention to operating system vendors, there were efforts in the same direction. Some people at Apple betted on the potential of GPUs and started developing an open API, latter known as OpenCL. In the same time, Microsoft created the DirectCompute API for Windows.

OpenCL aimed to became the OpenGL of heterogeneous computing for parallel applications. It is a cross-platform API with a broad and inclusive approach to parallelism, both in software and in hardware. While explicitly targeting GPUs, it also considers multi-core CPUs and FPGAs. The applications are portable across different hardware platforms, varying performance while keeping functionality and correctness. The first software implementations date back to 2009.

Most companies support OpenCL across their products. Apart from AMD and NVIDIA we can use it on graphic hardware from S3 and VIA. Also IBM has a version of OpenCL for PowerPC and CELL processors. Intel started to offer support from the APU architecture Ivy Bridge and GPUs. Embedded world is also interested in OpenCL. Imagination Technologies offer support for the SGX545 graphics core. As does Samsung with their ARM based microprocessors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Execution Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

OpenCL, DirectCompute and CUDA are APIs designed for heterogeneous computing with both a host CPU and an optional GPU device. The applications have serial portions, that are executed on the host CPU, and parallel portions, known as \textit{kernels}. The parallel kernels may execute on an OpenCL compatible device (CPU or GPU), but synchronization is enforced between kernels and serial code. OpenCL is distinctly intended to handle both task and data parallel workloads, while CUDA and DirectCompute are primarily focused on data parallelism. 

A kernel applies a single stream of instructions to vast quantities of data that are organized as a 1-3 dimensional array (see Figures \ref{figure:captura1paperCEC} and \ref{figure:grid}). Each piece of data is known as a work-item in OpenCL terminology, and kernels may have hundreds or thousands of work-items. The kernel itself is organized into many work-groups that are relatively limited in size; for example a kernel could have 32K work-items, but 64 work-groups of 512 items each. 

\begin{figure}[h]
\centerline{\epsfig{file=./hierarchy.eps,width=7cm}}
\caption{Hierarchy of computing structure in a GPU.}
\label{figure:captura1paperCEC}
\end{figure}

Unlike traditional computation, arbitrary communication within a kernel is strongly limited. However, communication and synchronization is generally allowed locally within a work-group. So work-groups serve two purposes: first, they break up a kernel into manageable chunks, and second, they define a limited scope for communication. 

\begin{figure}[h]
\centerline{\epsfig{file=grid.eps,width=7cm}}
\caption{Execution model: Each piece of data is a work-item (thread); a kernel has thousands of work-items and is organized into many work-groups (thread blocks); each work-group process many work-items.}
\label{figure:grid}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Memory Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The memory model defines how data is stored and communicated within a device and between the device and the CPU. The memory model can be seen in Figure \ref{figure:memory}. DirectCompute, CUDA and OpenCL share the same four memory types (with different terminology):

\begin{figure}
\centerline{\epsfig{file=memory.eps,width=9cm}}
\caption{Memory model defines how the data is stored and communicated between CPU and GPU. Global memory is RW for both CPU and work-items; constant memory is RW for CPU and RO for work-items; private memory is RW for a single work-item; local memory is RW for a work-group.}
\label{figure:memory}
\end{figure}

\begin{itemize}
\item Global memory: it is available for both read and write access to any work-item and the host CPU.
\item Constant memory: is a read-only region for work-items on the GPU device, but the host CPU has full read and write access. Since the region is read-only, it is freely accessible to any work-item.
\item Private memory: is accessible to a single work-item for reads and writes and inaccessible for the CPU host. The vast majority of computation is done using private memory, thus in many ways it is the most critical term of performance.
\item Local memory: is accessible to a single work-group for reads and writes and is inaccessible for the CPU host. It is intended for shared variables and communication between work-items and is shared between a limited number of work-items.
\end{itemize} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bioinpired Methods on GPUs}
\label{sec:bioinfor_apps}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section reviews different bioinspired approaches using GPUs found in bibliography, focusing mainly on Evolutionary Computation (EC) and Artificial Neural Networks (ANN).

Alba et al. \cite{Alba2005book} reviewed and surveyed parallel metaheuristics on EC. They identified the majority of paradigms to be hosting parallel/distributed Evolutionary Algorithms (EAs), according to Flynn's taxonomy, to fall under the Multiple Instruction Multiple Data (MIMD) category. This argument is fairly valid as in the last two decades the most dominant platform hosting parallel/distributed EAs was clusters, also fine-grained EAs on massive parallel processors (MPPs) are wildly used.

The parallel EAs community has a long legacy with MIMD architectures compared to a very little contribution for SIMD system. This comes in part due to the dominance of MIMD architectures as compared to SIMD ones.

Alba classifies the main parallel metaheuristic models as follow:
\begin{itemize}
 \item Parallel Genetic Algorithms (Cant\'u Paz \cite{Cantu-Paz98asurvey}) %(G. Luque, et al.).
\item Parallel Genetic Programming (F. Fernández, et al.\cite{springerlink:10.1023_A:1021873026259}).
\item Parallel Evolution Strategies (G. Rudolph \cite{Rudolph92parallelapproaches}).
\item Parallel Ant Colony Algorithms (S. Janson, et al. \cite{ParallelAntColony}).
\item Parallel Estimation of Distribution Algorithms (J. Madera, et al. \cite{springerlink:10.1007_3_540_32494_1_7}).
\item Parallel Scatter Search (F. Garcia, et al. \cite{GarciaLopez2003575}).
\item Parallel Variable Neighborhood Search (F. Garc\'ia-l\'opez, et al.\cite{Garcia_lopez_theparallel}).
\item Parallel Simulated Annealing (\cite{Genetic_parallelsimulated}) .
\item Parallel Tabu Search (T. Crainic, et al.\cite{Crainic97towardsa}).
\item Parallel Greedy Randomized Adaptive Search Procedures (M. Resende and C. Ribeiro \cite{Resendeparallelgreedy}).
\item Parallel Hybrid Metaheuristics (C. Cotta, et al. \cite{Cotta05e:parallel}). 
\item Parallel MultiObjective Optimization (A. Nebro, et al. \cite{Nebro07mocell:a}).
\item Parallel Heterogeneous Metaheuristics (F. Luna, et al. \cite{ANL04}). 
\end{itemize}

Nevertheless, when the research community use GPGPU, several authors have developed algorithms using three parallel approaches: master-slave model, fine-grained model \cite{jian_ming_li_efficient_2007}, coarse-grained model \cite{Maitre:2009:CGP:1569901_1570089} \cite{pospichalParallelGeneticAlgorithOnCUDA2010} or hybrid models  \cite{DBLP:conf/gecco/PospichalMOSJ11} that use two or more of the previous parallel approaches in an hierarchical way. All the EC approaches on GPUs are parallel, thus a classification depending on the parallel model used is presented in this section. We will focus on master-slave \cite{ZhangImplementationMasterSlave}, fine-grained (cellular EAs), coarse-grained (Island Model or Deme Model) approaches; and an hierarchical model \cite{Zhang2009}.

As far as the ANN approaches, although the computation for ANN is inherently parallel, many algorithms require some steps that are difficult to parallelize on the GPU.

Most of these methods can be used with almost zero effort by using existing frameworks. The most complete and comprehensive review is from Parejo \cite{springerlink:10.1007/s00500-011-0754-8}. It is a comparative study of metaheuristic optimization frameworks. As criteria for comparison, a set of 271 features grouped in 30 characteristics and 6 areas has been selected. These features include the different metaheuristic techniques covered, mechanisms for solution encoding, constraint handling, neighborhood specification, hybridization, parallel and distributed computation, software engineering best practices, documentation and user interface.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Master-slave Approaches}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Master-slave Evolutionary Algorithms are usually used for problems involving computationally expensive fitness functions, where the master node runs the entire algorithm while the slaves execute the fitness evaluations. Hence, master-slave implementations are more efficient as the evaluations usually are the more expensive portion of the total run time.

In the field of GPUs, the work of Wong et al. \cite{man-leung-wong-parallel-2005} proposed an EP algorithm for solving five simple test functions, called Fast Evolutionary Programming (FEP). In this master-slave approach, some actions are executed in the CPU, as the main loop of the algorithm and crossover operator, while evaluation and mutation are run in the GPU (no need of external information). The competition and selection of the individuals are performed on the CPU, while mutation, reproduction and evaluation are performed on the GPU. In this case, the reproduction step implies interaction among, at least, two individuals. A maximum speedup of x5.02 is obtained when the population size increases. This is the most common organization in GPU implementations, since no interaction among individuals is required during the evaluation, so this step can be fully parallelized.

A GP method proposed by Harding and Banzhaf \cite{4215552} uses the GPU only for performing the evaluation, while the rest of the steps of the algorithm are run on the CPU. The authors tested real-coded expressions up to 10000 nodes, boolean expressions up to 1500 nodes, and some real world problem where they evaluate expressions up to 10000 nodes. In some cases, the results yielded speedup of thousand times.

Zhang et al. \cite{Zhang2009} adapt different EAs to a GPU using CUDA. The authors implemented an hierarchical parallel genetic algorithm using a deme model at the high level, and a master-slave schema at the low level. In this implementation, the CPU initializes the populations and distributes them to thread blocks in shared memory. Then, GPU threads within each block run a GA independently, selection, crossover, mutation and evaluation, and migrates individuals to other thread blocks in its neighborhood. In this case, no speedup results were reported.

There are other papers related with the previous approach, like Tsutsui et al. paper \cite{Tsutsui:2011:GECCO}. This chapter uses a master-slave approach with an Ant Colony Optimization (ACO) algorithm \cite{Dorigo:1999:ACO:329055_329062} with Tabu Search \cite{Glover:1997:TS:549765}. Tsutsui uses an Intel Core i7 965 (3.2 GHz) processor and a single NVIDIA GeForce GTX480 GPU. They compare CPU and GPU implementations with and the results showed that GPU computation with MATA, an efficient method for thread assignment cost which they call Move-Cost Adjusted Thread Assignment, showed a promising speedup compared to computation with CPU.

In \cite{Cano:2012:SUE:2150467.2150468} a massively parallel evaluation model is proposed. The implementation, based on CUDA, speeds up the fitness calculation phase and greatly reduce the computation time. Results on the GPU are up to 820 times better than the sequential case. The problem tested is of classification using Genetic Programming and data sets with sizes ranging from 150 to 1025010 patterns. The authors, Cano et al., proposes to evaluate the individuals using one or two GPUs and they test two models, the NVIDIA GeForce 285 GTX and the NVIDIA GeForce 480 GTX. These paper uses the traditional approach of using the GPU only during the evaluation process.

In \cite{Contreras:2012:UGA:2150467.2150469} the authors use a CPU-GPU architecture for stock market trading. The proposed architecture offers the benefices of GPU distribution for stock market researchers, rather for computer architecture experts. The authors used Jacket, a framework that provides GPU versions, in CUDA, of most of Matlab functions. Steps and guidelines to migrate from CPU code to GPU code are explained. The for loops in selection, evaluation, crossover and mutation are translated to Jacket's gfor. Three different CPU configurations are used for the experiments: Pentium 4, Pentium SU41000 and Intel Core i7-860. The former is also combined with an nVidia 460GTX for GPU experiments. Different population size are also tested. The time is reduced to 65\% in comparison with the CPU version. Other conclusions are obtained: higher speedups are obtained increasing the number of individuals, rather increasing the number of evaluations; and tournament selection reduces time over roulette-wheel selection. %FERGU: Añadido lo de "Other conclusions..." y "The propopsed architecture offers.." ""

PUGACE\cite{5586286} is a framework in CUDA for cellular GAs (cGAs), but only for linear neighborhood structures. The architecture of this framework is presented and experiments with the QAP problem are performed. Fitness evaluation uses a thread for each chromosome. Speedups for several problem instances varies from 15 to 18. The execution platform was a Pentium Dual Core at 2.5Ghz and an nVidia GeForce 98000 GTX+. %FERGU: Aquí no pongo nada porque está dicho que es para cellulars y no hay mucho más que decir...

\cite{Pedemonte:2011:BOG:2001858.2002031} studies the difference of using different representations for binary problems in GPUs in GAs: boolean data type versus packing multiple bits in a non boolean data type. Results shows that
packing in 32 bits data types achieve speedup values of up to 100 compared with boolean data types, specially when the size of the instances increases. The execution platform was a PC with Quad Core Intel Xeon E5530 processor at 2,4 Ghz and a Tesla C1060 with 240 CUDA cores.

\cite{5586403} shows a methodology for mapping the search space onto the GPU memory hierarchy in three levels: the distribution of the search process among the CPU and GPU (1), the mapping of the neighborhood in the GPU threads (2) and the effective usage of the texture memory in the context of hybrid EAs. Experiments to solve the QAP problem with CUDA are presented, where the evolutionary process is performed in the CPU and the generation of the Local Search neighborhood are performed on parallel in the GPU, achieving an acceleration of 14,6 times faster. Hardware used is Core 2 Duo 2,67 Ghz and Nvidia GTX 280.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fine-grained Approaches}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Traditionally, fine-grained Evolutionary Algorithms or Cellular Evolutionary Algorithms (cEAs) have not received as much attention as other types of EAs. This is mainly due to the necessity of special hardware, i.e. a relatively large supply of processors in the underlying architecture. On the contrary, the legacy of parallel/distributed architectures has shown dominance of loosely coupled systems which are not adequate for fine-grained EAs. The reason behind that is the high cost of building massively parallel architectures which normally attracted fewer researchers to work on one grained EAs. A review of the trends in parallel architecture foresee a radical change in position of fine-grained EAs among other parallel EAs. This is due to three reasons:

\begin{itemize}
 \item Growing trend of massive number of processors on chip or card. 
\item The very high inter-processors speed which is a major factor affecting the efficiency of fine-grained EAs. 
\item The huge drop of cost of these architectures which will attract a wide base of researchers and developers.
\end{itemize}

For this kind of algorithms, each individual is the parent at the beginning of the algorithm wile the second parent is selected by applying some selection function for its neighboring. As a result cEAs provide automatic niching effect, avoiding an early convergence. 


In this scheme, Wong et al. \cite{man-leung-wong-parallel-2006,wong-implementation-2009} proposed a parallel hybrid GA (HGA) where the whole evolutionary process is run on the GPU, and only the random number generation is done in CPU. Each GA individual is set to each GPU, and each one selects probabilistically an individual in its neighborhood to mate with. Just one offspring individual is generated, and replaces the old one in that GPU. The authors compare HGA with a standard GA run in a CPU and the FEP \cite{man-leung-wong-parallel-2005} algorithm. Using a new pseudo-deterministic selection method, the amount of random numbers transferred from the CPU is reduced. HGA reaches speedup of 5.30 when compared against the sequential version.

%Yu \cite{yu-parallel-2005} recommends the use of GA with 2D structured population also called cGA for implementation of fine-grained GAs over the SIMD processors of a GPU. The 2D structure of cGA maps well to the GPU architecture.

Yu et al. \cite{yu-parallel-2005} implemented the first real cellular EA using GPU, for solving the Colville minimization problem. They place the population in a toroidal 2D grid and use the classical Von Newmann neighborhood structure with five cells. They store chromosomes and their fitness values in texture memory on the graphic card, and both, fitness evaluation and genetic operations, are implemented entirely with fragment programs executed in parallel on GPU. Real-coded individuals of a population are represented as a set of 2D texture maps. $BLX-\alpha$ crossover and non-uniform mutation are run as tiny programs on every pixel at each step in a SIMD-like fashion, solving some function optimization problems and reaching a speedup of x15 with a population of 512x512 individuals. They store a set of random numbers at the beginning of the evolution process to solve the random number generation problem when using GPU processors. 

Luo et al. \cite{zhongwen-luo-cellular-2006} implemented a cellular algorithm on GPU to solve three different SAT problems 
% Antonio - Poner qué significan las siglas SAT
using a greedy local search (GSAT) \cite{Selman93domain-independentextensions} and a cellular GA (cGA). 
They saved local minimums using a random walk strategy, jumping to other search space location. 
%Mutation operator in the traditional GA is some kind similar to random walk, which guarantees the exploration of the search towards different location and might be more promising regions. 
The cellular GA adopts a 2D toroidal grid, using the Moore neighborhood, stored on texture GPU memory. This implementation generates the random numbers in the GPU (using a generated seed on the CPU at the beginning of the process). 
%They carry out the experiments using two GSAT implementations, CGSAT (with crossover and without mutation) and PGSAT (without crossover and with mutation) running them on CPU and GPU with different population sizes. 
%A great time reduction is obtained using the GPU parallelization approach (from 95ms to 18 ms for CGSAT and from 464ms to 77 ms for PGSAT).  
The GPU version reduces in about 6 times the running time.

Li et al. \cite{jian-ming-li-efficient-2007} proposed a cellular algorithm on GPU for solving some common approximation functions. The authors reported experiments using big populations (up to 10000 individuals) reaching speedups of x73.6 for some implementations. 

In \cite{Li:2009:PIA:1726585.1726930} the authors propose a fine-grained parallel immune algorithm (FGIA) based on GPU acceleration, which maps parallel IA algorithm to GPU using CUDA. The results show that the proposed method (even increasing the population size) reduces running time.% and provides ordinary users with a feasible FGIA solution.

Alba et al. \cite{springerlink:10.1007978-3-642-12538-619} use CUDA and store individuals and their fitness values in the GPU global memory. Both, fitness evaluation and genetic operators, are run on GPU (no CPU is used). 
They use a pseudo random number generator provided by the SDK of CUDA named Merseinne Twister. Their experiments include some general discrete and continuous optimization problems, and they compare physical efficiency and numerical efficacy with respect to CPU implementation. 

In \cite{PSO-GPU_Mussi} L. Mussi propose a Particle Swarm Optimization (PSO) approach, where a fine-grained model was implemented by omitting the synchronization step, the main constraint for profiting the parallelization power of GPUs in classical metaheuristics. CUDA threads were considered as processing elements for every particle independently. The experiments compared this implementation with a synchronous one, on a GeForce GTX260AMP and a GeForce GTS450 solving different hard synthetic functions problems. The speed-ups factors in the comparison with the sequential approach are up to 200 using the GTS card for the 100 dimensions Rastrigin problem, considering the asynchronous approach, and around 30 for the synchronous implementation, to cite one instance. In average, the first method doubled the later with respect to this factor.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Coarse-grained Approaches (island model)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Coarse grained algorithms are the most common among parallel EAs. Generally, coarse-grained algorithms require less tightly coupled parallel architectures, as compared to fine-grained. Coarse grained EAs divide the main population into sub-populations (also known as Islands) where the sub-populations evolve concurrently. This basic feature of coarse-grained EAs hits a physical limit of GPUs. 

In order to run a coarse-grained EA using a GPU, several kernels simultaneously are run where each kernel handles a sub-population is not possible. This limitation of GPU would mean the conventional mechanics of coarse-grained EAs would need to be changed if GPU would be used.

With regard to the last topology (fine-grained EAs), one of the first island models on GPU approaches was published on the GPU competition of GECCO 2009 \cite{gecco2009CompetitionPospichal}. It presents some technical details of an island model entirely hard-coded on GPU, with a ring-like topology. Nevertheless, the evolutionary operators implemented on GPU are only specific to the GECCO competition, and the validity of the experiments just works on a small number of problems.

Tsutsui et al. \cite{1570355} propose run a coarse-grained GA on GPU to solve the QAP problem using CUDA. This is one of the hardest optimization problems in permutation domains. 
Their model generates the initial population on CPU and copied it to the GPU VRAM; then, each subpopulation in a GPU (NVIDIA GeForce GTX285) is evolved. At some generations, individuals in subpopulations are shuffled via the GPU VRAM. 
Results showed a speedup from x3 to x12 (using eight QAP instances), compared to the Intel i7 965 processor. 

The model by Luong et al. \cite{LUONG:2010:INRIA-00520464:1} is based on a re-design of the island model.
Three different schemes are proposed: The first one implements a coarse-grained EA using a master-slave model to run the evaluation step on GPU. The second one distributes the EA population on GPUs, while the third proposal extends the second one using fast on-chip memory. 
Second and third approaches reduce the CPU/GPU memory latency, although their parameters (number of islands, migration topology, frequency and number of migrants) must be adapted to the GPU features. 
Sequential and parallel implementations are compared, obtaining a speedup of x1757 using the third approach.

Pospichal et al. \cite{pospichalParallelGeneticAlgorithOnCUDA2010,9253} propose a parallel GA with island model running on GPU. The authors map threads to individuals, thus, threads-individuals can be synchronized easily in order to maintain data consistency, and on-chip hardware scheduler can swiftly swap existing islands between multiprocessors to hide memory latency. Fast, shared memory within the multiprocessor is used to maintain populations.
Since the population size is limited to 16KB per island on most GPUs, if the population is larger, slower main memory has to be used. The migration process is based on an asynchronous unidirectional ring that requires an inter-island communication (slower main memory has to be used). The authors report speedups up to 7000 times higher on GPU compared to CPU sequential version of the algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hybrid Approaches}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The hybrid model simply utilizes two or more of the master-slave, coarse-grained and fine-grained in a hierarchical method. At the higher level, an island model algorithm runs while at the lower level the demes, another name for sub-populations, themselves are running in parallel (fine-grained, master-slave or even another island model with high migration rates). Hybrid models are not the most common in legacy EAs due to:
\begin{itemize}
 \item The need for additional new parameters to account for a more complex topology structure. 
 \item The need for hierarchical parallel architectures to host hybrid algorithms, while such hierarchical parallel architectures are not so common. 
\item The high complexity of programming such models. 
\end{itemize}

Nevertheless, for the GPUs, hybrid EAs are a perfect candidate to exploit the hierarchical memory and flexible block sizing in GPUs by well structured populations. 

The design and implementation of a hybrid EA plus local search to solve MAX-SAT over GPUs was thoroughly discussed in \cite{Munawar:2009:HGA:1666141_1666143}. Manuwar et al. uses a hierarchical algorithm of 2D structured sub-populations arranged as islands in a 2D grid. Therefore, each individual has 4 neighboring individuals (north, south, east and west) and each sub-population has 4 neighboring sub-populations (north, south, east and west). Instead of using a conventional algorithm for migration between the sub-populations, they introduced a new technique that they call diffusion. Diffusion is more suitable for implementation of cGAs based pGA over a GPU. In the proposed implementation, the host processor (CPU) acts as a controller while an NVIDIA Tesla C1060 GPU provides the required computational resources. Configuration, memory allocation and initialization are performed over the host processor. After the initialization stage, data is transferred to the device and the code enters a loop. The loop keeps on repeating until the maximum number of generation criteria is satisfied. Results were collected over a system with NVIDIA Tesla C1060 GPU mounted on a motherboard with Intel Core i7 920@2.67GHz as the host CPU. C1060 have 4GB of device memory, 30 streaming multiprocessors, and the total number of processing cores is 240. The maximum amount of shared memory per block is 16KB and clock rate is 1.30GHz. They compare the results of the algorithms over NVIDIA with optimized for local search, mutation, recombination, selection and diffusion (migration) with different implementations using serial implementation, OpenMP implementation over Intel and over Ultra Spark architectures. The found that the maximum speedup is for larger problems, and it is up to 25x if compared the serial implementation over Intel Core 2 Duo 3.3GHz with the NVIDIA implementation. 

In \cite{fjdiego-vrp} F.J. Diego et al proposed a parallel strategy for solving the Capacitated Vehicle Routing Problem (CVRP) by means of an ACO algorithm. They combined the CPU processing, random number generation and centralized pheromone information dealing, with the GPU parallel capabilities: trails initialization, build solutions, choose the best and pheromone evaporation. The experiments were conducted on a GeForce GTX460 in a Pentium Dual Core 2.7GHz with 2GB RAM, using CUDA. The results showed a speed-up of 12 in the best case.

In \cite{ACO-on-GPU_Develacq} A. Develacq test different parallel strategies for the ACO metaheuristic on a NVIDIA Fermi C2050 GPU, on a four-core Xeon E5640 @2.6 GHz with 24GB RAM. The Max-Min Ant System is considered for solving the Traveling Salesman Problem (TSP) for problem sizes to up to 2100 cities. Authors proposed multi-ant (one colony) and multi-colony approaches implemented with CUDA. They distributed the solutions on the GPU processing elements: an ant per thread, and an ant per block respectively. The common structures, pheromone matrix, distances and candidate lists, are stored in shared memory, meanwhile the CPU is used for a preliminary set of random numbers generation and centralized control tasks. For the multi-colony algorithms, a set of ants is assigned to every processing element. This time, almost the whole algorithm is run in the GPU. They obtained speed-ups of up to 19 in the multi-ant, and up to 8 in the multi-colony approaches, having similar solution quality than the original sequential approach.

Authors of \cite{Cecilia201342} proposed three different techniques for improving the classical ACO algorithms parallelization on GPUs: a data parallelism scheme for tour construction on GPU, a GPU-based pheromone updating stage, and a roulette wheel implementation on GPU, called I-Roulette. They introduced the queen ants, associated with CUDA blocks, and the worker ants, associated to CUDA threads. The experiments were performed on a four-core Intel Xeon E5620 running at 2.4 GHz, with 16GB RAM, and a NVIDIA Tesla C2050 Fermi. The proposed techniques lead to obtain a speed-up factor of up to 20 in comparison with the sequential implementation.

In \cite{luong-ppsn2012} a guideline to exploit heterogeneous computing resources, GPU and CPUs, for effective hybrid metaheuristics is proposed. The goal of this approach is not the evaluation of the fitness of some individuals, but the parallelization of other metaheuristics that are not inherently parallel, like local search algorithms. The parallelization of these algorithms can evaluate several different solutions or test on the neighborhood at the same time, \cite{luong:inria-00520461}. Experiments running several instances of Fast Ant System \cite{Taillard:1998:FFA:870509} and selecting the best neighbor at each iteration of the search and the Quadratic Assignment Problem \cite{Zhu:2010:0020-7543:1035} are shown. They use an Intel Core i7 930 with NVIDIA GTX 480 or the Intel Xeon E5520 with Tesla C1060. The GPU implementation is 15 times faster than the one running on the CPU.

In \cite{5586530} three cellular GA versions are compared: a CPU, a mono-GPU and a multi-GPU version. A 2D grid population is used and divided in two GPUs in the multi version. Selection, recombination, mutation and evaluation are performed in parallel. In the multi GPU case, a thread for each GPU is controlled by a CPU thread and borderline individuals of the sub-populations are exchanged. The
speedup ranges from 8 to 711. However, there is not significantly difference between mono and multi GPU versions, probably due the overhead of the CPU. Hardware used is Intel Quad processor 2.67GHz and Nvidia GeForce GTX 285.

% Maribel, creo esta secci´on nueva para meter lo que no quepa dentro del saco de las otras tres secciones.
\subsection{Hybrids Models}

Pospichal et al. in 2011 has addressed several papers related with GPU devices. One of the last is \cite{DBLP:conf/gecco/PospichalMOSJ11} where they proposes to use a GPU device for Grammatical Evolution evolving complete programs in an arbitrary language using variable length binary strings. For this problem, the individual is a program, so it must to be compiled and sent to the GPU for evaluation each generation. But the most time-consuming part of this approach is to sent the individuals to the GPU and no the evaluation itself, so, Pospichal et al. propose to evolve the whole Grammatical Evolution algorithm on the GPU with a special mapping function. They mapped one individual per block of threads and one thread is used for manipulate each individual. They compare the execution time of an Intel Core i7 with NVIDIA GTX 480 using CUDA (CPU implementation with C) an a Java library called GEVA \cite{O'Neill:2008:GGE:1527063.1527066} which is interpreted language (CPU implementation using GEVA).  The authors include results for both implementations (C and GEVA) and one GPU implementation and they compare them with and without overhead for CPU-GPU communication. We include only the results with overhead, because the parallel version need the CPU-GPU communication so the authors might to take care about it. The results of GPU implementation are in average 5.3 speedup than CPU with C implementation which is an expected result. The speedup grows until 102.8 when GPU implementation and CPU implementation with GEVA are analysed but the authors do not include any variance results, so we do not recommend the last result as the most important of the paper. Thus, this paper proposes two levels of parallelism: 1) indi-
viduals are evaluated in parallel (by threads int he same block) and 2) data within individuals (genes, crossover points, mutations, fitness points, etc.) are maintained by parallel access as well (by block of threads).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Artificial Neural Networks implementations on GPUs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Artificial neural networks attempt to capture the adaptability of biological neurons in a mathematical model for information processing. ANNs are very powerful tools that are highly parallel but also computationally expensive and match well with the GPU computing architecture.  As a workhorse of the computational intelligence field, there exists a high demand for  this acceleration.  As a highly analytic structure, neural networks can be reduced to a series of matrix operations, and thus are easily parallelized, as the GPU is highly optimized to perform these kinds of operations \cite{Meuth2006}.  

Several authors provide tips for ensuring efficient implementation  of algorithms on GPU’s \cite{Oh2004,Luo2005,Martinez2007,Martinez2010}.

Zhongwen uses the GPU to first extract a set of characteristics from image data, then applies a pre-trained MLP to these characteristics for classification \cite{Luo2005}.

Bernhard \cite{BernhardCITA17} proposes a different approach, implementing spiking neural networks for image segmentation.

In general, significant performance gains can be elicited from implementing neural network algorithms on graphics processing units. However, these implementations are difficult to obtain.

Finally, several developers provide libraries and tools to help practitioners to develop ANN applications on GPUs \cite{url2}.

%\begin{figure}[b]
%\sidecaption
%\includegraphics[scale=.65]{figure}
%\caption{If the width of the figure is less than 7.8 cm use the \texttt{sidecapion} command to flush the caption on the left side of the page. If the figure is positioned at the top of the page, align the sidecaption with the top of the figure -- to achieve this you simply need to use the optional argument \texttt{[t]} with the \texttt{sidecaption} command}
%\label{fig:1}
%\end{figure}

%\begin{table}
%\caption{Please write your table caption here}
%\label{tab:1}
%\begin{tabular}{p{2cm}p{2.4cm}p{2cm}p{4.9cm}}
%\hline\noalign{\smallskip}
%Classes & Subclass & Length & Action Mechanism  \\
%\noalign{\smallskip}\svhline\noalign{\smallskip}
%Translation & mRNA$^a$  & 22 (19--25) & Translation repression, mRNA cleavage\\
%Translation & mRNA cleavage & 21 & mRNA cleavage\\
%Translation & mRNA  & 21--22 & mRNA cleavage\\
%Translation & mRNA  & 24--26 & Histone and DNA Modification\\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%\end{tabular}
%$^a$ Table foot note (with superscript)
%\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Bioinspired algorithms have been used in traditional computers and clusters in 
the past decades, but with the actual technological enhancement of computer graphics, a new way to execute this kind of algorithms in GPUs has been open. 

This paper shows a survey of the usage of GPUs to execute bioinspired
algorithms to solve optimization problems. An introduction to different GPU architectures, with the languages and frameworks used in the industry has been presented.

Also, a number of works available in the literature have been explained, with a description of a taxonomy to classify them. Most of the studied algorithms use the GPU for parallel fitness evaluation, but other examples for parallel distribution are being implemented, such as Ant Colony Optimization.

In the most of the cases, extremely higher speedups are attained compared with traditional CPU sequential versions. Several authors agree on the bottlenecks due to synchronization in the CPU parts of the algorithms, so new directions 
about minimize the communication with the CPUs are emerging.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{acks}
This work has been supported in part by 
the UGR PR-PP2011-5 project, 
%the Junta de Andaluc\'{\i}a {SIPEsCa} (G-GI3000/IDIF) project,
the P08-TIC-03903 project awarded by the Andalusian Regional Government, 
the FPU Grant 2009-2942 
and 
the TIN2011-28627-C04-02 project.

%The authors are grateful to anonymous referees for their constructive comments and advice about the first version of our chapter.
%The authors are very grateful to the anonymous referees whose comments and suggestions have contributed to improve this chapter.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Appendix}
%\addcontentsline{toc}{section}{Appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{acmsmall} 
\bibliography{antonio,gpus,gustavo,maribel,pablo,todo}

\bigskip

% History dates
\received{Dec-2012}{-}{-}

% Electronic Appendix
%\elecappendix

\end{document}

