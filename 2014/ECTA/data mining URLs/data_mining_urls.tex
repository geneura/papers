\documentclass[a4paper,twoside]{article}

\usepackage[latin1]{inputenc}
\usepackage{epsfig}
\usepackage{graphicx}        % standard LaTeX graphics tool
%\usepackage{subfigure}
\usepackage{url}
\usepackage{subfig}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{caption}
\usepackage{SCITEPRESS}


%\subfigtopskip=0pt
%\subfigcapskip=0pt
%\subfigbottomskip=0pt

\begin{document}

\title{Applying Classification Methods to Manage\\ Secure URL Accesses in the Company}
% applied to what? En el título tiene que quedar bien claro de qué
% problema se trata. Puesto así suena a "aplico data mining a ver qué
% pasa" - JJ
% Antonio - Ya, era el título dummy. He puesto uno menos dummy, pero no me acaba de convencer... :D

\author{\authorname{Author1\sup{1}, Author2\sup{1} and Author3\sup{2}}
\affiliation{\sup{1}Maracena Institute of Technology (MIT), Anonymous university, Anonymous Street num. X, AnonymousLand}
\affiliation{\sup{2}Department of Computing, Authors University (AU), Authors Street num. AAA, Metropolis, Moon}
\email{\{auth1,auth2\}@mit.edu, auth3@au.edu}
}

\keywords{Data Mining, Security Policies, URL request, Machine Learning, Classification}

\abstract{This paper presents a deep analysis of a dataset based in
  URL sessions performed by the employees of a company. The aim is to
  find the best classification algorithm in order to make the decision
  of allowing or denying future URL requests, considering a set of
  corporate security policies. % no cuentes lo que has hecho, sino qué
                               % problema quieres resolver. Por qué
                               % pienasas que tu método resuelve el
                               % problema y prueba que efectivametne
                               % lo hace. Escritura de papers 101 - JJ
The complete preprocessing phase is also described, including a labelling stage based in the defined set of security rules.
Two data balancing techniques are tested (under- and oversampling) over several different training/test partitions.
The results show that...} %Todo esto me suena más a que estás
                          %presentando un método de preprocesamiento
                          %de URLs. - JJ

\onecolumn \maketitle \normalsize \vfill

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\uppercase{Introduction}}
\label{sec:introduction}

\noindent Corporate security is one of the key areas inside an
enterprise, and thus, one of the main research topics
nowadays. %Típica frase vacía de contenido. main ¿en qué contexto?
          %¿quién lo dice? Aquí deberías de definir qué problema es el
          %que quieres resolver y el contexto en el que se
          %encuentra. Por ejemplo, "Corporate security involves all kinds
          %of challanges, including this and that and that. Every
          %problem is approached in a different way, but since mobile
          %devices are used for browsing, it is... - JJ
The new scenario defined by the Bring Your Own Device (BYOD) tendency %defínela
in the companies, in which the 
users or employees  themselves have become one of the main threats for
the security assurance % assurance? qué significa eso? - JJ
 inside them \cite{Opp_Security11}, has meant % the new scenario
                                % ... has meant - JJ
the rising of new security issues , which should be dealt in a different way, normally by means of \textit{Corporate Security Policies}.
These are a set of rules aimed to protect the company's assets
defining sets of permissions to be considered for every different
action to be performed inside the security system. % párrafo confuso
                                % que yo borraría del tirón. Empezaría
                                % explicando qué son las CSP, en qué
                                % contexto se usan y por qué, y
                                % seguiría a partir de ahí - JJ

One of the main breaches with respect to 
% primer regarding - JJ
% Antonio - quitado
the security in an enterprise is the access to non-confident (or non-certified) web sites ***REF***. Moreover, several webpages should be also controlled for productivity or suitability reasons.
So, there are usually defined sets of allowed or denied pages/sites to
be potentially accessed by the enterprise users/employees. 
% There are defined sets? En serio? - JJ
% Antonio - no lo sé a ciencia cierta, pero a nosotros nos pasaron una serie de reglas especificando qué sitios se podían y no se podían acceder...¿?

In this work we try to define a tool for automatically making % for
                                % automatically made? Quién diablos ha
                                % escrito esto? JJ 
% Antonio - creo que ahora está bien
an allowance or denial decision for some webpages/sites that are not included in the aforementioned list.
Thus, the problem is transformed into a classification one, in which we started from a set of unlabelled patterns modelling the connection properties from a huge amount of URL accesses (the so-called sessions). Then we assign a label to many of them, considering a set of security rules defined by the Chief Security Officer in the company.
We process this dataset by means of different classification methods, and applying some data balancing techniques, in order to choose the best algorithm for this problem.

The results are quite good, getting classification accuracies around 95-97\% in the test phase, even when using the non-balanced datasets.

The paper is structured as follows. Next section describes related work regarding the application of data mining and machine learning techniques to security issues inside the company. Section \ref{sec:problemDescription} presents the problem to solve and the dataset to work with. The followed methodology is described in Section \ref{sec:methodology}, concerning the data preprocessing and a first round of experiments comparing different classification methods. Once the best of them are selected, a set of experiments have been conducted, and the results are described and discussed in Section \ref{sec:results}. Finally, the conclusions and future lines of research are presented in Section \ref{sec:conclusions}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  STATE OF THE ART  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\uppercase{State of the Art}}
\label{sec:stateofart}
%

\noindent Although Data Mining (DM) and Machine Learning (ML) techniques have been used since long ago in many scientific fields, and given that research in computer security was growing since the eighties \cite{computer_security_80}, it was not until the nineties that these techniques began to be applied to security issues \cite{Clifton1996}. On the one hand, DM helped developing new solutions to computer forensics \cite{DeVel2001}, being the researchers able to extract information from large files with events gathered from infected computers. Another important advance took place after the 9/11 events, because \textit{clustering techniques} and \textit{social network analysis} started to be performed in order to detect pontential crime networks \cite{Hsinchun2003}.

On the other hand, and more closely to our work, there exist some user-centric solutions to problems like user authentication in a personal device, who Greenstadt and Beal \cite{cognitive_security_08} proposed to address using collected user biometrics along with machine learning techniques. Then, when a security policy is going to be applied, P.G. Kelley et al. \cite{user-controllable_learning_08} found important to include the user in the machine learning process for refining the policy model. They called it \textit{user-controllable policy learning}. Another approach to the refinement of user's privacy policies has been described by Danezis in \cite{inferring_policies_socialnetworks_09}, for he uses ML techniques over the user's settings in a social network.

In the same line, Lim et al. have two works \cite{sec_policy_evolution_gp_08,pol_evol_gp_3_approaches_08} where a certain set of computer security policies evolve by means of Genetic Programming (GP), taking again into account the user's feedback. Furthermore, Suarez-Tangil et al. \cite{rule_generation_gp_09} take the same approach as Lim et al., but also bringing event correlation in. These two latter author's works are interesting for ours, though they are not focused in company Information Security Policies (ISPs)  - for instance, our case with the allowed or denied http requests -.

Finally, a system named MUSES (from Multiplatform Usable Endpoint Security System) \cite{MUSES_SAC_14} is being developed under the European Seventh Framework Programme (FP7). This system will include event treatment on the user actions inside a company, DM techniques for applying the set of policies from the company ISP to the actions, allowing or denying them, and ML techniques for improving the set of rules derived from these policies, according to user's feedback and behaviour after the system decisions.  

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROBLEM DESCRIPTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{\uppercase{Problem and Data Description}} 
\label{sec:problemDescription}

\noindent The problem to solve is related with the application of corporate security policies in order to deal with potential URL accesses inside an enterprise. To this end a dataset of URL sessions (requests/accesses) is analysed. These data are labelled with the corresponding permission or not for that access following the aforementioned rules. The problem is then transformed into a classification one, in which every new potential URL request/access will be classified, and thus, a grant or deny action will be assigned to that pattern.

The analysed data come from a typical \texttt{access.log} of Squid \cite{squid:site}. This open source tool works as a proxy, but with the advantage of storing a cache of recent transactions so future requests may be answered without asking the origin server again \cite{DuaneWessels2004}. 
Every pattern, namely a URL session, have associated 10 variables, which are described in Table \ref{tabdata}, indicating if the variable is numeric or nominal/categorical.

\begin{table*}[htpb]
\centering
 \caption{\label{tabdata} Independent Variables corresponding to a URL session (a connection to a URL for some time). The URLs are parsed as detailed in Subsection \ref{subsec:logparsing}.}
{\scriptsize
\begin{tabular}{llll}
\hline\noalign{\smallskip}
Variable name & Description & Type & Rank/Number of Values (if categorical)\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\texttt{http\_reply\_code} & Status of the server response & Categorical & 20 values\\
\texttt{http\_method} & Desired action to be performed & Categorical & 6 values\\
\texttt{duration\_milliseconds} & Session duration & Numerical & integer in [0,357170]\\
\texttt{content\_type} & Media type of the entity-body sent to the recipient & Categorical & 11 values (main content), 85 values (whole content)\\
\texttt{server\_or\_cache\_address} & IP address & Categorical & 2343 values\\
\texttt{time} & connection hour (in the day) & Date & 00:00:00 to 23:59:59\\
\texttt{squid\_hierarchy} & It indicates how the next-hop cache was selected & Categorical & 3 values\\
\texttt{bytes} & Number of transferred bytes during the session & Numerical & integer in [0,85135242]\\
\texttt{client\_address} & IP address & Categorical & 105 values\\
\texttt{URL} & Core domain of the URL, not taking into account the TLD & Categorical & 976 values\\
%\noalign{\smallskip} \hline\noalign{\smallskip}
%Non-financial Variables (used in GP) & Description & Type\\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%$x_0$, $x_1$, $x_2$ & Size & Small/Medium/Large& Categorical\\
\noalign{\smallskip}\hline
\end{tabular}
}
\end{table*}

The dependent variable or class is a label which inherently assigns an action/decision to every request (URL session). This can be: \textit{ALLOW} if the access is permitted according to the corporate security policies, or can be \textit{DENY}, if the connection is not permitted. These patterns are labelled using an `engine' based in a set of security rules, that specify the decision to make. This process is described in Subsection \ref{subsec:ruleparsing}.

These data were gathered along a period of two hours, from 8.30 to 10.30 am (30 minutes after the work started), monitoring the activity of all the employees in a medium-size company (80-100 people). There is an amount of 100000 patterns.

Due to the nature of the data (URL accesses performed by humans), the dataset is highly unbalanced \cite{imbalanced_data_05}. Thus, after a labelling process described in Section \ref{sec:methodology}, there are 38972 pattern belonging to class ALLOW (positive class) and 18530 of class DENY (negative class), so just a 67.78\% of the samples belongs to the majority class. This is a very important problem of the dataset, since a classifier trained considering these proportions will aim to classify all the samples as ALLOW, getting a theoretically quite good classification accuracy equal or greater than 68\%.

In order to deal with this problem there exist several methods in the literature, but all of them are mainly grouped in three techniques \cite{imbalance_techniques_02}:
\begin{itemize}
\item \textit{Undersampling the over-sized classes}: i.e. reduce the considered number of patterns for the classes with the majority.
\item \textit{Oversampling the small classes}: i.e. introduce additional (normally synthetic) patterns in the classes with the minority.
\item \textit{Modifying the cost associated to misclassifying the positive and the negative class} to compensate for the imbalanced ratio of the two classes. For example, if the imbalance ratio is 1:10 in favour of the negative class, the penalty of misclassifying a positive example should be 10 times greater.
\end{itemize}

The first option has been applied in some works, following a random undersampling approach \cite{random_undersampling_08}, but the problem is the loss of valuable information. 

On the other hand, the second option has been so far the most exploited, following different approaches, such as SMOTE (Synthetic Minority Oversampling Technique) \cite{smote_02}, a method proposed by Chawla et al. for creating `artificial' samples for the minority class, in order to balance the amount of them with respect. However this technique is based in numerical computations, which consider different distance measures, in order to generate useful patterns  (i.e. realistic or similar to the existing ones).

The third option implies using a method in which a cost can be associated to the classifier accuracy at every epoch. This was done for instance by Alfaro-Cid et al. in \cite{cost_adjustment_07}, where they used a Genetic Programming approach in which the fitness function was modified in order to consider a penalty when the classifier makes a false negative (an element from the minority class was classified as belonging to the majority class).
However almost all the approaches deal with numerical (real, integer) data. 

In our case, the dataset contains a majority of categorical/nominal data, so we have performed different approaches for data balancing:
\begin{itemize}
\item Undersampling: we will remove random samples of the majority class until the amount in both classes are similar. ***REF?***
\item Oversampling: we will duplicate random samples of the minority class, in order to get a close number of patterns in both classes. This has be done due to the impossibility of creating synthetic data when dealing with categorical values (there is not a proper distance measure between two values in a category). ***REF A ALGO SIMILAR HECHO***
Actually, since the number of samples in the majority class is almost twice the minority one, we have just duplicated all of those belonging to the minority class.
\end{itemize}

These approaches will be evaluated in the experiments section.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   METHODOLOGY  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\uppercase{Methodology}}
\label{sec:methodology}


\noindent Before classification techniques are applied, a data preprocessing step has been performed. First, the raw dataset is labelled according a set of \textit{initial corporate security rules}, i.e. every pattern is assigned to a label indication if the corresponding URL request/access would be ALLOWED or DENIED considering these rules. This step is necessary in order to transform the problem into a classification one. However, in order to apply the rules they must be transformed from their initial format into another one that can be applied in our programs (a hash in Perl\footnote{A \textit{hash} in Perl is an object that represents a \textit{hash table}, which is a set of pairs key-value. Sometimes, the value can be another hash itself.}). This is described in Subsection \ref{subsec:ruleparsing}. 

Subsection \ref{subsec:logparsing} details how the patterns of the navigation data log (URL sessions) are also converted to a Perl hash to perform the matting/labelling process. 

At the end of these two steps, the two hashes are compared in order to obtain which entries of the log should be ALLOW or DENY, know as the \textit{labelling} step. This is similar to perform a decision process in a security system.

Finally, in Subsection \ref{subsec:methods} we explain the selection of the methods to apply in order to classify the data. We just have considered the patterns correctly labelled in the preprocessing phase. Thus, a supervised classification process \cite{classification_67} has been conducted on the balanced datasets.
Weka Data Mining Software\footnote{http://www.cs.waikato.ac.nz/ml/weka/} has been used, in order to select the best set of methods in order to deal with these data. These classifiers will be further tested in Section \ref{sec:results}.

% ------------------------------------------------------------------
%
\subsection{Security rules parsing}
\label{subsec:ruleparsing}

\noindent In this work we have considered Drools \cite{drools:site} as the tool to create and therefore managing rules in a business environment. This so called Business Rule Management System (BRMS) has been developed by the JBoss community under an Apache License and it is written in Java. Though this platform consist of many components, here we focus on Drools Expert and the Drools Rule Language (DRL, \cite{drools:doc}). Then, the defined rules for a certain company are inside of a file with a \texttt{.drl} extension, the file that needs to be parsed to obtain the final set of rules. In Figure \ref{fig:drools_hash}, (a), there is the typical rule syntax in DRL. Two main things should be obtained from the parsing method: both left and right sides of the rule, taking into account that the left side is where the company specifies the conditions required to apply the action indicated in the right side. Also, for describing the conditions, Squid syntax is used (see Section \ref{sec:problemDescription}), having thus the following structure: \texttt{squid:Squid(\textit{conditions})}. Finally, from the right side of the rule, the \textit{ALLOW} or \textit{DENY} label to apply on the data that matches with the conditions, will be extracted. The Perl parser that we have implemented applies two regular expressions, one for each side of the rule, and returns a hash with all the rules with the conditions and actions defined. The `before and after' performing the parsing over the \texttt{.drl} file is in Figure \ref{fig:drools_hash}.

\begin{figure}[htb]
\centering
\subfloat[Drools Rule]{
\begin{tabular}{ p{0.05cm} p{0.05cm} p{2.5cm} }
  \texttt{rule~"name"} & & \\
   & \texttt{attributes} & \\
   & \texttt{when} & \\
   & & \texttt{/* Left Side of the Rule */} \\
   & \texttt{then} & \\
   & & \texttt{/* Right Side of the Rule */} \\
  \texttt{end} & & \\
\end{tabular}
}
~
\subfloat[Hash Rule]{
\begin{tabular}{ p{0.05cm} p{0.05cm} p{2.5cm} }
  \texttt{\%rules~=~(} & & \\
   & \texttt{rule~=>\{} & \\
   & & \texttt{field => xxx} \\
   & & \texttt{relation => xxx} \\
   & & \texttt{value => xxx} \\
   & & \texttt{action => [allow, deny]} \\
   & \texttt{\},} & \\
  \texttt{);} & & \\
\end{tabular}
}
\caption{(a) Structure of a rule in Drools Expert. (b) Resulting rule, after the parsing, in a global hash of rules. \label{fig:drools_hash}}
\end{figure}

% ------------------------------------------------------------------
%
\subsection{URL log data parsing}
\label{subsec:logparsing}

\noindent Usually, the instances of a log file have a number of fields, in order to have a registration of the client who asks for a resource, the time of the day when the request is made, and so on. In this case, we have worked with an \textit{access.log} (see Section \ref{sec:problemDescription}) file, converted into a CSV format file so it could be parsed and transformed in another hash of data. All 10 fields of the Squid Log yield a hash like the one depicted in Figure \ref{fig:data_hash}.

Once the two hashes of data were created, they were compared in such a way that for each rule in the hash of rules, it was determined how many entries in the data log hash are covered by the rule, and so they were applied the label that appears as `action' in the rule.

One of the problems was to extract from a whole URL the part that was more interesting for our purposes. It is important to point out that in a log with thousands of entries, an enormous variety of URLs can be found, since some can belong to advertisements, images, videos, or even some others does not have a domain name but are given directly by an IP address. For this reason, we have taken into account that for a domain name, many subdomains (separated by dots) could be considered, and their hierarchy grows from the right towards the left. The highest level of the domain name space is the Top-Level Domain (TLD) at the right-most part of the domain name, divided itself in country code TLDs and generic TLDs. Then, a domain and a number of subdomains follow the TLD (again, from right to left). This way, the URLs in the used log are such as \textit{http://subdomain...subdomain.domain.TLD/} \textit{other\_subdirectories}. However, for the ARFF\footnote{Format of Weka files} file to be created, only the domain (without the subdomains and the TLD) should be considered, because there are too many different URLs to take into consideration. Hence, applying another regular expression, the data parser implemented in Perl obtains all the core domains of the URLs, which makes 976 domains in total.

\begin{figure}[htb]
\centering
\caption{Perl hash with an example entry. The actual hash used for this work has a total of 100000 entries, with more than a half labelled as \textit{ALLOW} or \textit{DENY} after the comparing process. \label{fig:data_hash}}
\begin{tabular}{ p{0.1cm} p{0.1cm} p{6cm} }
  \texttt{\%logdata~=~(} & & \\
   & \texttt{entry~=>\{} & \\
   & & \texttt{http\_reply\_code => xxx} \\
   & & \texttt{http\_method => xxx} \\
   & & \texttt{duration\_miliseconds => xxx} \\
   & & \texttt{content\_type => xxx} \\
   & & \texttt{server\_or\_cache\_address => xxx} \\
   & & \texttt{time => xxx} \\
   & & \texttt{squid\_hierarchy => xxx} \\
   & & \texttt{bytes => xxx} \\
   & & \texttt{url => xxx} \\
   & & \texttt{client\_address => xxx} \\
   & \texttt{\},} & \\
  \texttt{);} & & \\
\end{tabular}
\end{figure}

% ------------------------------------------------------------------
%
\subsection{Classification Methods}
\label{subsec:methods}

% Ránking:
% Undersampling:             |      80% entrenamiento - 20% test (random)
%                            |
% 1- J48 97'02%              |                      97'38%
% 2- Random Forest 96'87%    |                      97'63%
% 3- REP Tree 96'79%         |                      97'41%
% 4- NNge 96'49%             |                      97'34%
% 5- PART 96'45%             |                      96'85%
%                            |
% Oversampling:              |      90% entrenamiento - 10% test (random)
%                            | 
% 1- J48 98%                 |                      97'56%
% 2- Random Forest 98'84%    |                      97'56%
% 3- REP Tree 97'67%         |                      97'56%
% 4- NNge 98'76%             |                      97'02%
% 5- PART 97'54%             |                      97'21%
%                            |


\noindent As said in Section \ref{sec:problemDescription}, the data used for this work is not only numerical or nominal, thus, only classification algorithms that support both types of data have been considered. Weka has a great number of possible algorithms to work with, so we have conducted a preselection phase trying to choose those which would yield better results in the experiments. More specifically, we have focused in rule-based and decision-tree-based algorithms. 

%It is important to point out that rules can become trees, but rules cannot always be derived from trees (for instance, a tree modelling a mathematical operation).

In this way, a decision-tree algorithm is a group of conditions organised in a top-down recursive manner in a way that a class is assigned following a path of conditions, from the root of the tree to one of its leaves. Generally speaking, the possible classes to choose are mutually exclusive. Furthermore, these algorithms are also called ``divide-and-conquer'' algorithms. On the other hand, there are the ``separate-and-conquer'' algorithms, which work creating rules one at a time, then the instances covered by the created rule are removed and the next rule is generated from the remaining instances.

A reference to each Weka classifier can be found at \cite{Frank2011}. Below are described the top five techniques, obtained from the best results (See Table \ref{tabresults_todos}) of the experiments done in this stage, along with more specific bibliography.

\begin{table}[htpb]
\centering
 \caption{\label{tabresults_todos} Results of all the tested classification methods on balanced data. The best ones are marked in boldface.}
{\small
\begin{tabular}{|l|l|l|}
\cline{2-3}
\multicolumn{1}{l|}{} & Undersampling & Oversampling \\ 
\hline
Conjunctive Rule & 60.14 & 60.02 \\ 
\cline{1-1}
Decision Table & 94.08 & 90.29 \\ 
\cline{1-1}
DTNB & 94.75 & 95.65 \\ 
\cline{1-1}
JRip & 90.08 & 92.47 \\ 
\cline{1-1}
NNge & \textbf{96.49} & \textbf{98.76} \\ 
\cline{1-1}
One R & 93.45 & 93.70 \\ 
\cline{1-1}
PART & \textbf{96.45} & \textbf{97.54} \\ 
\cline{1-1}
Ridor & 87.22 & 89.87 \\ 
\cline{1-1}
Zero R & 51.39 & 51.26 \\ 
\cline{1-1}
AD Tree & 77.73 & 77.68 \\ 
\cline{1-1}
Decision Stump & 60.14 & 60.02 \\ 
\cline{1-1}
J48 & \textbf{97.02} & \textbf{98.00} \\ 
\cline{1-1}
LAD Tree & 79.95 & 79.97 \\ 
\cline{1-1}
Random Forest & \textbf{96.87} & \textbf{98.84} \\ 
\cline{1-1}
Random Tree & 95.14 & 98.35 \\ 
\cline{1-1}
REP Tree & \textbf{96.79} & \textbf{97.67} \\ 
\hline
\end{tabular}
}
\end{table}


\begin{description}
   \item[J48] This classifier generates a pruned or unpruned C4.5 decision tree. Described for the first time in 1993 by \cite{Quinlan1993}, this machine learning method builds a decision tree selecting, for each node, the best attribute for splitting and create the next nodes. An attribute is selected as `the best' by evaluating the difference in entropy (information gain) resulting from choosing that attribute for splitting the data. This way, the tree continues to grow till there are not attributes anymore for further splitting, meaning that the resulting nodes are instances of single classes. 
   \item[Random Forest] This manner of building a decision tree can be seen as a randomization of the previous C4.5 process. It was stated by \cite{Breiman2001} and consist of, instead of choosing `the best' attribute, the algorithm randomly chooses one between a group of attributes from the top ones. The size of this group is customizable in Weka.
   \item[REP Tree] Is another kind of decision tree, it means Reduced Error Pruning Tree. Originally stated by \cite{Quinlan1987}, this method builds a decision tree using information gain, like C4.5, and then prunes it using reduced-error pruning. That means that the training dataset is divided in two parts: one devoted to make the tree grow and another for pruning. For every subtree (not a class/leaf) in the tree, it is replaced by the best possible leaf in the pruning three and then it is tested with the test dataset if the made prune has improved the results. A deep analysis about this technique and its variants can be found in \cite{Elomaa2001}.
   \item[NNge] Nearest-Neighbor machine learning method of generating rules using non-nested generalised exemplars, i.e., the so called `hyperrectangles' for being multidimensional rectangular regions of attribute space \cite{Martin1995}. The NNge algorithm builds a ruleset from the creation of this hyperrectangles. They are non-nested (overlapping is not permitted), which means that the algorithm checks, when a proposed new hyperrectangle created from a new generalisation, if it has conflicts with any region of the attribute space. This is done in order to avoid that an example is covered by more than one rule (two or more).
   \item[PART] It comes from `partial' decision trees, for it builds its rule set from them \cite{Frank1998}. The way of generating a partial decision tree is a combination of the two aforementioned strategies ``divide-and-conquer'' and ``separate-and-conquer'', gaining then flexibility and speed. When a tree begins to grow, the node with lowest information gain is the chosen one for starting to expand. When a subtree is complete (it has reached its leaves), its substitution by a single leaf is considered. At the end the algorithm obtains a partial decision tree instead of a fully explored one, because the leafs with largest coverage become rules and some subtrees are thus discarded.
 \end{description} 

These methods will be deeply tested on the dataset (balanced and unbalanced) in the following section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   RESULTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\uppercase{Experiments and Results}}
\label{sec:results}

\noindent Several experiments have been conducted, once a subset of classification methods have been chosen in previous section.
To this end, some training/test datasets have been created, starting from the set of labelled patterns. It contains 57502 samples, belonging 38972 to class ALLOW, and 18530 to class DENY.

In order to better prove the methods, two different divisions (training-test) have been done, namely 90\%-10\% and 80\%-20\%. Moreover, two additional splits have been considered in every case, using both a random and a sequential approach for selecting samples from the original file. Thus, in the latter, consecutive patterns have been included in the training file up to the desired percentage. The rest have composed the test file. In the first approach, a random selection is performed.

The aim of the sequential division is to compare if the online activity of the employees considering URL sessions could be somehow `predicted', just using data from previous minutes or hours.

With respect to the data, the initial file was unbalanced, as it can be seen in the number of patterns per class. Thus, as stated in Section \ref{sec:problemDescription}, two data balancing methods have been applied to all the files, to get similar numbers in both classes: undersampling (random removal of patterns) and oversampling (duplication of patterns).

Results for unbalanced data are presented in Table \ref{tabresults_nobalan}.
Three different tests have been done for the random pattern distribution approach, so the mean and standard deviation is shown in the corresponding columns.

\begin{table*}[htpb]
\centering
 \caption{\label{tabresults_nobalan} Percentage of correctly classified patterns for non-balanced data}
{\small
\begin{tabular}{|l|l|l|l|l|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\ 
\hline
J48 & 97.56 $\pm$ 0.20 & 88.48 & 97.70 $\pm$ 0.15 & 82.28 \\ 
\cline{1-1}
Random Forest & 97.68 $\pm$ 0.20 & 89.77 & 97.63 $\pm$ 0.13 & 82.59 \\ 
\cline{1-1}
REP Tree & 97.47 $\pm$ 0.11 & 88.34 & 97.57 $\pm$ 0.01 & 83.20 \\ 
\cline{1-1}
NNge & 97.23 $\pm$ 0.10 & 84.41 & 97.38 $\pm$ 0.36 & 80.34 \\ 
\cline{1-1}
PART & 97.06 $\pm$ 0.19 & 89.11 & 97.40 $\pm$ 0.16 & 84.17 \\ 
\hline
\end{tabular}
}
\end{table*}

As it can be seen in the table...

***
Se puede ver que todos los métodos obtienen un rendimiento similar en aciertos, siendo éste bastante alto. Si se mira la desviación estándard se ve que los resultados no son fortuitos, ya que ésta es muy pequeña.

Como era de esperar la división en 90\%-10\% mejora los resultados, aunque no mucho, por lo que habría que hacer una prueba con una división más agresiva (trabajo futuro) para saber las capacidades de los métodos al trabajar con pocos datos de entrenamiento.

Respecto a los datos secuenciales sus resultados son peores, pero aún así bastante buenos (> 85\%).
La bajada en aciertos se debe sin duda a la aparición de patrones totalmente nuevos a partir de determinada hora (puede que algo que se haga de forma programada cada día o determinados días), por lo que no hay patrones similares con los que entrenar y se falla al clasificar en el test.

Los aciertos son incluso menores (hay una bajada de 5-6 puntos) al hacer una división considerando más patrones, contrariamente a lo esperado/logico, por lo que esto refuerza la teoría de patrones que sólo suceden a determinadas horas.

Como técnica destaca levemente \textit{Random Forest}, siendo la que mejores resultados obtiene en casi todos los casos, incluyendo las más complejas particiones secuenciales. Aunque mirando la desviación estándar nos decantamos por \textit{REP Tree}, que obtiene resultados más robustos.
***

***
Los resultados de los datos balanceados se muestran en la Tabla \ref{tabresults_balan}. Nuevamente los resultados de las particiones aleatorias se muestran en media de tres particiones.
***

\begin{table*}[htpb]
\centering
 \caption{\label{tabresults_balan} Percentage of correctly classified patterns for balanced data (under- and oversampling)}
{\small
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\cline{2-9}
\multicolumn{1}{l|}{} & \multicolumn{4}{c|}{80\% Training - 20\% Test} & \multicolumn{4}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-9}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{Undersampling} & \multicolumn{2}{c|}{Oversampling} & \multicolumn{2}{c|}{Undersampling} & \multicolumn{2}{c|}{Oversampling} \\ 
\cline{2-9}
\multicolumn{1}{l|}{} & Rand (mean) & Sequential & Rand (mean) & Sequential & Rand (mean) & Sequential & Rand (mean) & Sequential \\ 
\hline
J48 & 97.05 $\pm$ 0.25 & 84.29 & 97.40 $\pm$ 0.03 & 85.66 & 96.85 $\pm$ 0.35 & 76.44 & 96.37 $\pm$ 0.06 & 74.24 \\ 
\cline{1-1}
Random Forest & 96.61 $\pm$ 0.17 & 88.59 & 97.16 $\pm$ 0.19 & 89.03 & 96.99 $\pm$ 0.13 & 79.98 & 97.25 $\pm$ 0.33 & 81.33 \\ 
\cline{1-1}
REP Tree & 96.52 $\pm$ 0.13 & 85.54 & 97.13 $\pm$ 0.25 & 85.41 & 96.55 $\pm$ 0.10 & 77.65 & 97.14 $\pm$ 0.09 & 76.81 \\ 
\cline{1-1}
NNge & 96.56 $\pm$ 0.42 & 85.28 & 96.90 $\pm$ 0.28 & 83.46 & 96.33 $\pm$ 0.05 & 81.93 & 96.91 $\pm$ 0.06 & 78.73 \\ 
\cline{1-1}
PART & 96.19 $\pm$ 0.14 & 85.16 & 96.82 $\pm$ 0.09 & 84.50 & 96.09 $\pm$ 0.10 & 79.70 & 96.68 $\pm$ 0.11 & 78.16 \\ 
\hline
\end{tabular}
}
\end{table*}

***
Como puede verse en la tabla...\\

A DESARROLLAR:\\

* Undersampling peor que Oversampling - lógico porque se pueden perder patrones que pueden ser relevantes y en el otro se mantienen todos.\\

* Resultados empeoran al usar las técnicas de balanceo. La Justificación sería algo como esto:\\

DON'T WORK PROPERLY:\\
Undersampling - Losses important information (key patterns).\\
Oversampling - Just duplication because there are several nominal values so it doesn't work as well as in other approaches which uses numerical computations for creating the new patterns to include in the minority class.\\
Sequential - Some kind of URL accesses are just performed at a time, which could not be included in the training set.\\

* Resultados secuenciales peor que aleatorios por lo explicado antes. Aquí incluso empeoran más porque se pierde más información que antes\\

En este caso destacan los algoritmos \textit{J48}, muy cercano a \textit{Random Forest} en el procesamiento de ficheros aleatorios, y \textit{REP Tree}, que mejora a todos los demás al trabajar con datos secuenciales.
***\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\uppercase{Conclusions and Future Work}}
\label{sec:conclusions}

\noindent In this paper a set of classification methods have been used in order to perform a decision process inside a company, according to some predefined corporate security policies.
This decision is focused on the allowance or denial of URL accesses/requests.

Thus, starting from a big dataset (100000 patterns) about employees' URL sessions information, and considering a set of URL access permissions, we composed a labelled dataset (57000 patterns).

Several classification methods have been tested over this dataset, after some balancing techniques have been applied. Then, the best five have been deeply proved over several training/test divisions, one of them using sequential patterns.\\

The results show that...\\

So we can conclude that...\\

Future lines of work include the implementation of other classification methods, such as one based in Genetic Programming, in which a modification of the cost associated to misclassifying could be applied as a class balancing method (as the authors did in \cite{cost_adjustment_07}).
However, and considering the good classification results obtained in this work, the first step could be the application of these methods in the real system from which data was gathered, counting with the opinion of expert CSOs, in order to know the real value of the proposal.

*** probar una división en entrenamiento/prueba más grande ***

%ACKNOWLEDGMENTS are optional
\section*{\uppercase{Acknowledgements}}

\noindent %This paper has been funded in part by European and National projects FP7-318508 (MUSES) and TIN2011-28627-C04-02 (ANYSELF) respectively, project P08-TIC-03903 (EV\-ORQ) awarded by the Andalusian Regional Government, and project 83 (CANUBE) awarded by the CEI-BioTIC UGR.

%If any, should be placed before the references section
%without numbering. To do so please use the following command:
%\textit{$\backslash$section*\{ACKNOWLEDGEMENTS\}}


\vfill
\bibliographystyle{apalike}
{\small
\bibliography{data_mining_urls}}


%\section*{\uppercase{Appendix}}
%
%\noindent If any, the appendix should appear directly after the
%references without numbering, and not on a new page. To do so please use the following command:
%\textit{$\backslash$section*\{APPENDIX\}}
%
%\vfill
\end{document}
