POLLACK_BACKGAMMON
------------------
However, no backpropagation,
reinforcement or temporal difference learning methods were employed. Instead we apply simple hillclimbing in a
relative fitness environment. We start with an initial champion of all zero weights and proceed simply by playing
the current champion network against a slightly mutated challenger and changing weights if the challenger wins.
Surprisingly, thisworked rather well. We investigate howthe peculiar dynamics of this domain enabled a previously
discarded weak method to succeed, by preventing suboptimal equilibria in a “meta-game” of self-learning.

Such results reinforce the 
importance of choosing the right opponent to evaluate against
in competitive coevolution.

TOGELIUS
--------


THOMPSON POKER
--------------

Results vary wildly between simulations, with further analysis showing
that the ability to create robust strategies is difficult given
the adversarial dynamic of the game. Despite this, agents are
still capable of adhering to guidelines recommended in expert
literature.


AVERY HUMAN-LIKE
----------------

By creating a computer player that changes its strategy
with influence from the human strategy, we have shown that
the holy grail of gaming – an individually tailored gaming
experience, is indeed possible. We designed the computer player
for the game of TEMPO, a zero sum military planning game.
The player was created through a process that reverse engineers
the human strategy and uses it to coevolve the computer player.