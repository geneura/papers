% oooo   oooo  ooooooo   
%  8888o  88 o888   888o 
%  88 888o88 888     888 
%  88   8888 888o   o888 
% o88o    88   88ooo88   
                       
% oooooooooo  ooooooooooo ooooo  oooo ooooo  oooooooo8      o      oooooooooo  
%  888    888  888    88   888    88   888  888            888      888    888 
%  888oooo88   888ooo8      888  88    888   888oooooo    8  88     888oooo88  
%  888  88o    888    oo     88888     888          888  8oooo88    888  88o   
% o888o  88o8 o888ooo8888     888     o888o o88oooo888 o88o  o888o o888o  88o8 
                                                                             
%      o    ooooo  oooo oooo   oooo 
%     888    888    88   8888o  88  
%    8  88   888    88   88 888o88  
%   8oooo88  888    88   88   8888  
% o88o  o888o 888oo88   o88o    88  


 _   _  ____    _____  ________      _______  _____         _____  
% | \ | |/ __ \  |  __ \|  ____\ \    / /_   _|/ ____|  /\   |  __ \ 
% |  \| | |  | | | |__) | |__   \ \  / /  | | | (___   /  \  | |__) |
% | . ` | |  | | |  _  /|  __|   \ \/ /   | |  \___ \ / /\ \ |  _  / 
% | |\  | |__| | | | \ \| |____   \  /   _| |_ ____) / ____ \| | \ \ 
% |_| \_|\____/  |_|  \_\______|   \/   |_____|_____/_/    \_\_|  \_\
                                                                   
                                                                   
%          _    _ _   _ 
%     /\  | |  | | \ | |
%    /  \ | |  | |  \| |
%   / /\ \| |  | | . ` |
%  / ____ \ |__| | |\  |
% /_/    \_\____/|_| \_|
                                  

%Pos eso, que aún no os pongáis a revisar y esas cosas :)



\documentclass{llncs}
\usepackage{graphics}
\usepackage[dvips]{epsfig}
\usepackage[latin1]{inputenc}
\usepackage{color}
\usepackage{longtable}
%\usepackage{multirow}
\usepackage[dvips]{graphicx} 
%\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{url}
\usepackage{algpseudocode}

\newcommand{\tab}{\hspace{20mm}}

\setlength{\textfloatsep}{8pt plus 2pt minus 2pt}
\setlength{\intextsep}{8pt plus 2pt minus 2pt}

%#\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%#    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%\hyphenation{}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Co-Evolutionary Optimization of Autonomous Agents in a Real-Time Strategy Game}
% autonomous por qué? Por qué no simplemente playing?

%OK, no revisamo0s, pero para eso mejor haz una rama y trabaja con la
%rama. No cambies nombres que despista. - JJ

\titlerunning{Co-Evolutionary Optimization of Bots in an RTS}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   AUTHORS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\author{A. Fernández-Ares \and A.M. Mora \and  J.J. Merelo \and \\ P. García-Sánchez \and P.A. Castillo}
%\authorrunning{A. Fernández-Ares. et al.}
%
%\institute{Departamento de Arquitectura y Tecnolog\'{\i}a de Computadores.\\
%Universidad de Granada (Spain)\\
%\email{antares.es@gmail.com}, \\ \email{\{amorag,jmerelo,pgarcia,pedro\}@geneura.ugr.es}
%}

\maketitle

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract}
This paper presents an approach based in an evolutionary algorithm
(EA), aimed to improve the behavioral parameters which guide the
actions of an autonomous agent (bot) inside a real-time strategy game
(RTS) named Planet Wars. 

Specifically the work describes a co-evolutionary implementation of a
previously presented method, which yielded successful results.
Thus, it's analyzed the effects of considering several individuals
to be evolved (improved) at the same time in the algorithm,
the use of 3 different fitness for measure the goodness of each bot in the evaluation,
and the variance of use an EA with and without previous knowledge for the training.

To this end, 4 on 4 matches have been considered. Two variants are presented:
without previous knowledge (where the 4 bots belong to the population) and with
(2 bots of the population versus 2 previously studied with good results bots).

For the fitness, 3 methods are studied: one based in turns and result position,
and another two based in the survey of the percentage of ships that belong each bot in each turn of the battle.

In this paper we set several goals for the uses of the co-evolution:
reduce the time needed for training in behavior with a huge time of evaluation,
improve best bots for used in 4 on 4 battles,
studies the significant different between the training with and without previous knowledge
and finally studies many fitness for co-evaluations.

\end{abstract}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introduction}
\label{sec:intro}
%

%En un track de juegos, no puedes empezar justificando la
%investigación en juegos. Habla de por qué usas coevolución o, si
%acaso, el interés que puede tener mejorar bots (puede haber gente de
%juegos de mesa o de cartas) - JJ
Autonomous agents (or \textit{bots}) in videogames have become very popular in the last years, because they can increase the challenge and lasting appeal of the game, by competing or cooperating with the human player.
% Maribel: tal y como dice J, podrías decir justo lo mismo, pero de enfoques coevolutivos, puedes referenciar algoritmos coevolutivos recientes en varios ámbitos
Thus, designing a good behavioural engine for them is one of the main
topics of interest in the actual videogame development task.
% ¿Qué significa good en este caso? ¿Asesino? - JJ
They have been widely used in Fisrt Person Shooter games (FPSs) % Falta una referencia a FPSs
from the nineties, but in this paper we will % Maribel- no uses futuro, en español se usa el futuro, pero en inglés el paper se cuenta en presente o en pasado y en cualquier caso, nunca mezclandolo con futuro. Cuando escribes el paper, ya lo has hecho, no es futuro.
work with them on a Real-Time Strategy game (RTS).
RTSs are a sub-genre of strategy-based video games in which the contenders control a set of units and structures distributed in a playing area and combat using them for conquering the scenario or defeating the opponent.
Command and Conquer\texttrademark,
Starcraft\texttrademark, Warcraft\texttrademark~ and Age of
Empires\texttrademark~ are some examples of these type of games.   

%RTS games often employ two levels of AI: the
%first one, makes decisions on the set of units (workers, soldiers, machines,
%vehicles or even buildings); the second level is devoted to every one of
%these small units. These two level of actions, which can be considered
%{\em strategical} and {\em tactical}, make them inherently difficult;
%but they are made even more so due to their real-time nature (usually addressed by constraining the time that can be used to make a decision) and also for the huge search space (plenty of possible behaviours) that is implicit in its action.
The RTS considered % Maribel, ves, aquí vuelves a usar pasado 
in the paper 
% Maribel- Sólo en el paper, en otro sitio no se llama así?
in named \textit{Planet Wars}, and it was used a platform in the Google AI Challenge 2010. % Maribel, a esta frase anterior le falta algo, un in quizás?
In this contest, `real time' is sliced in one second \textit{turns}, with players receiving the chance to play sequentially. However, \textit{actions} happen at the \textit{simulated} same time. 

% Hala, salto. Pasas de describir qué juego a decir qué haces. Métela
% poquito a poco: hicimos esto, funcionó regular porque no teníamos
% más que no sé cuantos oponentes, vamos a hacerlo de esta forma
% porque no sé quién dijo que era muy guay - JJ 
% Maribel- Ahora hablas en presente, menudo ?ío!!
This paper describes a Co-Evolutionary \cite{Coevolution95} approach
for improving % mejorar con respecto a qué? - JJ
the decision engine of a bot that plays that RTS. This engine consists
in a set of rules previously designed and evolved by the authors in
\cite{Genebot_CEC11} % recordad doble ciego, anonimizar - JJ
, using an % Maribel- el an sobra, debería ser a
regular Genetic Algorithm (GA) \cite{GAs_Goldberg89}. We
applied an offline evolution (i.e., not during the match, but prior to
the game battles) of the parameters on which the behavioural rules
depends. 

The evaluation of the quality (fitness) of each set of rules in the
population was made by playing the bot against predefined opponents, being a pseudo-stochastic or \textit{noisy} function, since the results for the same individual evaluation may change from time to time, yielding good or bad values depending on the battle events and on the opponent's actions. We have dealt with this noisy nature \cite{genebot-evo12} % Maribel- revisión doble ciega otra vez aquí 
by means of a reevaluation phase of all the individuals every generation, along with an average calculation of the fitness value of every individual after five combats (in five different an representative maps). %Maribel- Pon el criterio con el que has elegido los mapas, por qué esos y no otros.

The aim is that the co-evolutionary scheme improves the fitness
convergence of the population, since the individuals cooperate in
their evolution. % lo uno no se sigue de lo otro, explicar el
                 % argumento - JJ
                 
% Maribel- Deberías decir algo de por qué el enfoque anterior no era bueno y por qué has decidido hacerlo coevolutivo, es decir que si
% el otro se estancaba, por qué se estancaba y en qué casos y así concluir poco a poco que un enfoque coevolutivo mejoraría esos %problemas previos 
 Thus, we have  considered  matches with four players, with two of the
 contenders the individuals being evolved at a specific generation,
 and the other two opponents with a fixed AI engine, namely the
 competition sparring \textit{GoogleBot} in one of the experiments,
 and our best individual to date, baptised as \textit{Genebot-8}, in
 the other one.  
 % Maribel- Esto creo que debería estar en el apartado de experimentos
 
%Estás explicando la metodología antes de tiempo. Tienes que
%justificar por qué usas coevolución y qué pistas tienes de que los
%resultados serían mejores - JJ

%In the experiments, we will show that the set of rules evolve towards better bots, and finally an efficient player is returned by the GA. 
%In addition, several experiments have been conducted to analyse the issue of the cited \textit{noisy fitness} in this problem.
%The experiments show its presence, but also the good behaviour of the chosen fitness function to deal with it and yield good individuals even in these conditions. 

% ¿esto por qué lo borras - JJ 
%The paper is structured as follows: 
%The next section reviews related approaches to behavioural engine design in similar game-based problems. 
%Section \ref{sec:planet_wars} addresses the problem by describing the game of Planet Wars.  
%Section \ref{sec:genebot} presents the proposed method, termed {GeneBot}, starting from the initial approach, and the GA used to evolve the behaviour. 
%The experiments and results are described and discussed in Section \ref{sec:experiments}. Finally, the conclusions and future lines of research are presented in Section \ref{sec:conclusions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  STATE OF THE ART  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{State of the Art}
\label{sec:stateofart}
%

Video games have become one of the biggest sectors in the
entertainment industry; after the previous phase of searching for the
graphical quality perfection, the players now request opponents
exhibiting intelligent behaviour, or just human-like behaviours
\cite{artifical-stupidity-game-wisdom2-2004}. % irrelevante en el
                                % contexto. Tienes que empezar
                                % enmarcando el juego y las técnicas
                                % usadas: algoritmos genéticos, RTS,
                                % coevolución aplicado a unos y otros
                                % - JJ

%Nowadays, the games AI research has followed a different path, mainly starting with the improvement of FPS Bot's AI with Doom\texttrademark~ or
%Quake\texttrademark~ by the beginning of the 90s; and the most famous environment inside this kind of games, Unreal Tournament \texttrademark~\cite{Agent_Smith_CEC2009,ControllingBot_CEC2010,cooperativebots_CIG2010}.

Most of the researches have been done on relatively simple games such
as Super Mario \cite{Togelius_SuperMario}, Pac-Man
\cite{Pac-MAnt_CIG2010} or Car Racing Games \cite{CarRacing_Pelta09},
being many bots competitions involving them. % Vale, todo el mundo es
                                % un pringao y usa juegos simples, de
                                % estas te meten una hostia tus
                                % compañeros de sesión. Basta con que
                                % digas simplemente que los RTS tienen
                                % un enfoque diferente a estos juegos
                                % - JJ

RTS games show an emergent component \cite{emergence_in_games2008} as a consequence of the cited two level AI, since the units behave in many  % maribel- Many different es redundante, pon sólo many 
(and sometimes unpredictable) ways. This feature can make a RTS game more entertaining for a player and maybe more interesting for a researcher.
There are many research problems with regard to the AI for RTSs, % no uses with regard, pon regarding
including planning in an uncertain world with incomplete information;
learning; opponent modelling and spatial and temporal reasoning
\cite{hongchoCIG2005}. 

However, the reality in the industry is that in most of the RTS games,
the bot is  controlled by a fixed script that has been previously
programmed (following a finite state machines or a decision tree, for
instance). % tampoco me sirve a estas alturas. Hay miles de trabajos
           % de RTS- CI Justifica las técnicas usadas, no que vayas a
           % usar ese tipo de juego - JJ
Once the user has learnt how such a game will react, the game
becomes less interesting to play. In order to improve the users' gaming
experience, some authors such as Falke et al. \cite{falke2003} proposed a learning classifier system that can be used to endow the computer with
dynamically-changing %Maribel- por qué le pones el guión, quitaselo, dice lo mismo y esa palabra no existe como tal
strategies that respond to the user's strategies,
thus greatly extending the games playability. % Vale, seguimos
                                % justificando el uso de CI en juegos
                                % en un congreso de CI en juegos. - JJ
%Moreover, some authors have research the implementation of a human-like AI \cite{weber2011-humanlevelAI}.

In addition, in many RTS games, traditional artificial intelligence techniques fail to play at a human level because of the vast search spaces that they entail \cite{Aha05learningtowin}. In this sense, Ontano et at. \cite{ontanon2007} proposed to extract behavioural knowledge from expert demonstrations in form of individual cases. This knowledge could be reused via a case-based behaviour
generator that proposed advanced behaviours to achieve specific
goals. %Irrelevante para el trabajo - JJ 

%In this line there are several works dealing with the case-base reasoning such as Baumgarten et al. \cite{Baumgarten-combiningAImethods} that combines some AI techniques, or Palma et al. \cite{Palma-behaviortrees} who apply behaviour trees.

%Recently a number of soft-computing techniques and algorithms, such as co-evolutionary algorithms \cite{co-evol-rts2006} or multi-agent based methods \cite{HagelbackJ09}, just to cite a few, have already been applied to handle these problems in the implementation of RTS games. For instance, there are many benefits attempting to build adaptive learning AI systems which may exist at multiple levels of the game hierarchy, and which co-evolve over time. 
%In these cases, co-evolving strategies might be not only opponents but also partners coperating at different levels \cite{Livingstone05}.
%Other authors propose using co-evolution for evolving team tactics
%\cite{avery2010coevolving}. However, the problem is how tactics are
%constrained and parametrised and how the overall score is computed. 

% Las carencias de estrategias tradicionales han llevado a... Un
% artículo es una narrativa, una historia, escríbelo como tal - JJ
Evolutionary algorithms have been widely used in this field, %\cite{Ponsen_EvLearn_RTS,Su-EAs_StrategySel09}, 
but they involve considerable computational cost and thus are not
frequently used in on-line games. In fact, the most successful
proposals for using EAs in games correspond to off-line applications
\cite{offline-evolutionary-learning}, that is, the EA works (for
instance, to improve the operational rules that guide the bot's
actions) while the game is not being played, and the results or
improvements can be used later during  the game. Through offline
evolutionary learning, the quality of bots' intelligence in commercial
games can be improved, and this has been proven to be more effective
than opponent-based scripts.
% ¿Eso es todo?  ¿Nada de RTS en juegos? Mira review de otros trabajos
% anteriores donde nos meten caña por eso, menciona RTSs que los hay
% a puñados. JJ

This way, in the present work, an offline GA is applied to a
parametrised tactic (set of behaviour model rules) inside the Planet
Wars game (a basic RTS), in order to build the decision engine of a
bot for that game, which will be considered later in the online
matches.
% Esto es de otro, ¿no? - JJ

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROBLEM DESCRIPTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem Description}
\label{sec:problemDescription}

We work with a simplified version of the game Galcon, aimed at performing bot's fights which was used as base for the Google AI Challenge 2010 (GAIC)\footnote{http://ai-contest.com}.

\begin{figure}[ht]
\begin{center}
  \epsfig{file=./imagenes/naves.eps,width=6cm}
\end{center}
\caption{Simulated screen shot of an early stage of a run in Planet Wars. White planets belong to the player (blue colour in the game), dark grey belong to the opponent (red in the game), and light grey planets belong to no player. The triangles are fleets, and the numbers (in planets and triangles) represent the starships. The planet size means growth rate of the amount of starships in it (the bigger, the higher).}
\label{figura:PlanetWars1}
\end{figure}

A Planet Wars match takes place on a map (see Figure \ref{figura:PlanetWars1}) that contains several planets (neutral, enemies or owned), each one of them with a number assigned to it that represents the quantity of starships that the planet is currently hosting. 

The aim of the game is to defeat all the starships in the opponent's planets. Although Planet Wars is a RTS game, this implementation has transformed it into a turn-based game, in which each player has a maximum number of turns to accomplish the objective. At the end of the match the winner is the player owning more starships. 

There are two strong constraints which determine the possible methods to apply to design
a bot: a simulated turn takes \textit{just one second}, and the bot is \textit{not allowed to store any kind of information} about its former actions, about the opponent's actions or about the state of the game (i.e., the game's map).

Therefore, the goal in this paper is to design a function that, according to the state of the map in each simulated turn (input) returns a set of actions to perform in order to fight the enemy, conquer its resources, and, ultimately, win the game. In the original game, only two bots are faced but in this paper we studies what happen when we simulated 4 on 4 battles, it's mean, when 4 bots fight in the same map.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  CO-EVOLUTION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Co-Evolutionary Approach}
\label{sec:co-genebot}

We will resort to an generational co-evolutionary algorithm, 

One of the major problems we found in the use of EAs for training bots for this behavior was the huge time needed for the evaluation, because we have to simulated fulls battles, in several maps. In addition, we are forced to use re-evaluation between generations, when dealing with a noise and stochastic problem.

Theoretically, the use of a co-evolution allow reduce the number of simulations needed because it evaluates the population in "groups". That's, if we have a population of 100 individuals, in a classic GA we need make 100 evaluations, once for each individual, for each generation. In co-evolution case, for example, if we are using 2 bots of the population for the co-evolution we only need 50 evaluations. It's likely that 4-simulations will take more time that a 2-simulation, but the question is if the time taken for a "4-simulations" is less that the time taken for two "2-simulation". In that case, the co-evolution will decrease the time needed for the training.

The use of 2 individuals or 4 individuals of the population for the  will depend of the use (or not) of previous knowledge. 

\subsection{Previous Knowledge vs Auto-generated Knowledge}
\label{sec:knowledge}

We have a best training bot, which has proved its worth in 1vs1 battles. The question is: can we use our previous knowledge for improve (faster) better bots in a similar problem? Or maybe, it's best don't use previous knowledge and allow fight versus individuals of the population that theoretically (as the bases of the GAs) will be better and better in each generation? For answer this question, we will make two types of co-evolutions.

\subsubsection{Co-evolution with previous Knowledge}

In this case, we will simulated battles between two individuals of the population versus two of our best bots (in 1vs1). We expected that our co-bots can learn the bases of our best bots, and improve for be better in 4 on 4 battles. It's desirable that this type of co-evolution prize bots that at least can win in a battled of our best bots.

Talcking about the time needed for the execution, we don't expected an huge reduction of the time needed for the training of the bots. It's something that we will try to get in the next co-evolution method.

\subsubsection{Co-evolution with Auto-generated Knowledge}
In this case, we will simulated battles between four individuals of the population. We don't use previous knowledge of our previous works, but we are improving our bots for be better versus bots every time betters (because we expected that our population will better in each generation).

Talcking about the time, we expected that this method reduce the time needed near to the half that the previous method.

\subsection{Fitness}

In our previous works, we evaluated a single bot of the population versus always the same bot (an reference-bot). For fitness function, the bot was evaluated several times (in different maps). The fitness function was defined depending of the result of the battle (if the bot win all his battles or lose in someone) and the numbers of turns needed for end the game. For two bots of the population (A and B) the fitness is defined like show in Fig.\ref{fig:fitness_clasico}

\begin{figure}
\begin{algorithmic}
\State{$A,B\in Population $}
\If {A WINs always}
    \If{B LOSEs some battle}
        \State A is better than B
    \ElsIf{A take less turns than B}
        \State A is better than B
    \Else
        \State B is better than A
    \EndIf
\Else
     \If{B WINs always}
        \State B is better than A
    \ElsIf{A take less turns than B}
        \State B is better than A
    \Else
        \State A is better than B
    \EndIf
\EndIf
\end{algorithmic}
\caption{Fitness used in battles of 2 bots}
\label{fig:fitness_clasico}
\end{figure}

This fitness works well for battles between two bots, and our first fitness proposed is an natural evolution of this fitness for 4 bots battles.

%However it'snt prepared for battles between 4 bots. For example, 2 bots that in their battleds finish in third place, it's best the one that take more turns or the other? The answer to this question, it's that depends of the other bots. Of course, all the fitness 

\subsubsection{Fitness based in Position-turns}

This fitness it's an natural evolution of the previous fitness, applied to 4 bots battles. Again, the evaluations are in several maps. In this case, we studie the position (1th,2th,..) of the bot in the 4-battle and the number of turns. For a bot that wins all the battles (it's 1th in all the battles) we call the sum of the numbers of turns {$ferocity$}, and in previous works we found that a bot that wins in less turns it's best that other that takes more turns (even if the both wins). In other case, the sum of turns is call {$sturdy$} and oposite to the {$ferocity$}, it's desirable a bot that take more turns in be defeated.

\begin{figure}
\begin{algorithmic}
\State{$A,B\in Population $}
\If {A average position $<$ B average position}
    \State A is better than B
\ElsIf{A average position $>$ B average position}
    \State B is better than A
\Else
    \If{A,B is always 1th}
        \If{A take less turns than B}
            \State A is better than B
        \Else
            \State B is better than A
        \EndIf
    \Else
        \If{A take less turns than B}
            \State B is better than A
        \Else
            \State A is better than B
        \EndIf
    \EndIf
\EndIf
\end{algorithmic}
\caption{Fitness used in battles of 4 bots based in turns and results}
\label{fig:fitness_co_clasico}
\end{figure}



The principal problem in the previous fitness, it's that there we are using two independents variables. In this case, we try to found a fitness that resume the goodness of our bot in a single number. %Describir ventajas de que un solo fitness, puede ser operado (sumado).

In this fitness, we are only interesting in the final result: position and turn. We aren't student how the bot reach it. In the nexts fitness, we use another metric to define the goodness of the bots. The percentage of the ships that in each turn belong to each player.

\begin{figure}
\begin{center}
  \epsfig{file=imagenes/nubecita.eps,width=9.2cm}
\end{center}
\caption{Representation of the number if ships of each bot in each turn} 
\label{figura:nubecita}
\end{figure}

\subsubsection{Fitness based in Slope}

For this fitness, we use the leats squares regression analysis for resume the cloud of points to a simple rect. The rect is represented as {$y = \alpha \times x + \beta $} where, {$\alpha$} and {$\beta$} are calculated as show the Fig. \ref{equation:LeatsSqueares}:

\begin{figure}
    \begin{equation}
        \alpha = \frac{\sum_{i=1}^{n}(X_{i} - \bar{X_{i}})(Y_{i} - \bar{Y_{i}})}{\sum_{i=1}^{n}(X_{i} - \bar{X_{i}})^{2}}
    \end{equation}
    \begin{equation}
        \beta = \bar{Y}-\alpha\bar{X}
    \end{equation}
    \caption{Leats squares regression}
    \label{equation:LeatsSqueares}
\end{figure}

\begin{figure}
\begin{center}
  \epsfig{file=imagenes/nubecita_pendiente.eps,width=9.2cm}
\end{center}
\caption{Representation of the number if ships of each bot in each turn} 
\label{figura:nubecita}
\end{figure}

For our fitness, we take the slope of the rect: {$\alpha$}. We can see if the bot win, {$\alpha>0$} and if loses, {$\alpha<0$} . However,  

How we are using several evaluations in different maps, 


\subsection{Fitness bases in Integral-Area}

\label{sec:fitness}


For the co-evolution of the bots 

%Maribel- No me he enterado todavía qué se co-evoluciona, como interactuan los individuos de las especies? cada cuanto interactúan? es coevolución competitiva o cooperativa?
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   EXPERIMENTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Experiments and Results}
\label{sec:experiments}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Conclusions and Future Work}
\label{sec:conclusions}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ACKNOWLEDGEMENTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgements}

This paper has been funded in part by projects P08-TIC-03903 (Andalusian Regional Government), TIN2011-28627-C04-02 (Spanish Ministry of Science and Innovation), and project 83 (CANUBE) awarded by the CEI-BioTIC UGR.

\bibliographystyle{splncs}
\bibliography{genebot}


\end{document}
