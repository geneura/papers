\documentclass{sig-alternate}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}        % standard LaTeX graphics tool
%\usepackage{subfigure}
\usepackage{url}
\usepackage{subfig}


\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{GECCO'14,} {July 12-16, 2014, Vancouver, BC, Canada.}
    \CopyrightYear{2014}
    \crdata{TBA}
    \clubpenalty=10000
    \widowpenalty = 10000

\title{Data Mining Applied to Secure URL Accesses in the Company}

\numberofauthors{2}
 \author{
 \alignauthor
 Author1\\
        \affaddr{Maracena Institute of Technology (MIT)}\\
        \affaddr{Anonymous Street num. X}\\
        \affaddr{AnonymousLand}\\
        \email{aut1@mit.com}
 \alignauthor
 Author2\\
 \affaddr{Authors University (AU)}\\
 \affaddr{Authors Street num. AAA}\\
 \affaddr{Metropolis}\\
 \email{aut2@au.com}
 }

%\numberofauthors{2}
% \author{
% \alignauthor
% A.M. Mora, P. De las Cuevas, J.J. Merelo\\
%        \affaddr{University of Granada}\\
%        \affaddr{Department of Computer Architecture and Technology, ETSIIT/CITIC}\\
%        \affaddr{Granada, Spain}\\
%        \email{amorag,paloma,jmerelo@geneura.ugr.es}
% \alignauthor
% S. Zamarripa, Anna I. Esparcia-Alcázar\\
%        \affaddr{S2 Grupo}\\
%        \affaddr{Valencia, Spain}\\
%        \email{szamarripa,aesparcia@s2grupo.es}
% }


\maketitle

\begin{abstract}
This paper presents an analysis of a dataset based in URL requests performed by the employees of a company. The aim is to get a classification according to their features which can assign a new request a decision, based in the enterprise security policies...
\end{abstract}

% *** Esto hay que buscarlo y fijarlo para este artículo ***
% A category with the (minimum) three required fields
%\category{I.2.1}{Computing Methodologies}{ARTIFICIAL INTELLIGENCE}[Applications and Expert Systems]\\
%A category including the fourth, optional field follows...
%\category{G.1.6}{Mathematics of Computing}{Numerical Analysis}[Optimization]
%
%D.2.8 [Software Engineering]: Metrics complexity mea-
%sures, performance measures
%\terms{Algorithms}


\keywords{Data Mining, Security Policies, URL request, Classification}


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introduction}
\label{sec:intro}

seguridad en la empresa, políticas y reglas, data mining en seguridad (breve)

%The paper is structured as follows. Next section describes the problem enclosed in the Planet Wars game.
%Section \ref{sec:stateofart} reviews related work regarding the scope (videogames) and the implementation (EAs with no fitness computation or different selection mechanisms).
%Section \ref{sec:survival_bots} presents the proposed method, termed {Survival Bot}, and its different implementations. 
%The experiments and results are described and discussed in Section \ref{sec:experiments}. Finally, the conclusions and future lines of research are presented in Section \ref{sec:conclusions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  STATE OF THE ART  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{State of the Art}
\label{sec:stateofart}
%

data mining/clasificación aplicado a problemas de seguridad

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROBLEM DESCRIPTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Problem and Data Description} 
\label{sec:problemDescription}


%***problema de las URLs, reglas a aplicar, datos de los que disponemos (descripción de los mismos y análisis preliminar)***

The analysed data comes from a typical \textit{access.log} of Squid \cite{squid_site}. This open source tool works as a proxy, but with the advantage of storing a cache of recent transactions so future requests may be answered without asking the origin server again \cite{DuaneWessels2004}. Thus, the studied log file has 10 fields that are described in the Table \ref{tabdata}. 

\begin{table*}[htpb]
\centering
 \caption{\label{tabdata} Independent Variables corresponding to an URL session (a connection to an URL for some time). ***The URLs are parsed extracting the XXXX Whatever XXXX, in order to avoid an infinite number of values (potentially there could be infinite different urls, considering that some of them can contain PHP variables and values, for instance.***)}
{\tiny
\begin{tabular}{llll}
\hline\noalign{\smallskip}
Variable name & Description & Type & Rank/Number of Values (if categorical)\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\texttt{http\_reply\_code} & Status of the server response & Categorical & 20 values\\
\texttt{http\_method} & Desired action to be performed & Categorical & 6 values\\
\texttt{duration\_milliseconds} & Session duration & Numerical & integer in [0, 357170]\\
\texttt{content\_type} & Media type of the entity-body sent to the recipient & Categorical & 11 values (main content), 85 values (whole content)\\
\texttt{server\_or\_cache\_address} & IP address & Categorical & 2343 values\\
\texttt{time} & connection hour (in the day) & Date & 00:00:00 to 23:59:59\\
\texttt{squid\_hierarchy} & It indicates how the next-hop cache was selected & Categorical & 3 values\\
\texttt{bytes} & Number of transferred bytes during the session *** & Categorical & 3 values\\
\texttt{client\_address} & IP address & Categorical & 105 values\\
\texttt{URL} & Core domain of the URL, not taking into account the TLD & Categorical & 976 values\\
%\noalign{\smallskip} \hline\noalign{\smallskip}
%Non-financial Variables (used in GP) & Description & Type\\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%$x_0$, $x_1$, $x_2$ & Size & Small/Medium/Large& Categorical\\
\noalign{\smallskip}\hline
\end{tabular}
}
\end{table*}

*** The dependent variable or class is a label which inherently assigns an action/decision to every connection (URL session). This can be: \textit{ALLOW} if the access is permitted according to the corporate security policies, or can be \textit{DENY}, if the connection is not permitted. ***\\

********************************************
\\

Due to the nature of the data (URL accesses), the dataset is highly unbalanced \cite{imbalanced_data_05}. Thus, there are 38972 pattern belonging to class ALLOW (positive class) and 18530 of class DENY (negative class), so just a 67'78\% of the samples belongs to ALLOW class. This is a very important problem of the dataset, since a classifier trained considering these proportions will aim to classify all the samples as DENY, getting a theoretically quite good classification accuracy of YY\%.

In order to deal with this problem there exist several methods in the literature, but all of them are mainly grouped in three techniques \cite{imbalance_techniques_02}:
\begin{itemize}
\item \textit{Undersampling the over-sized classes}: i.e. reduce the considered number of patterns for the classes with the majority.
\item \textit{Oversampling the small classes}: i.e. introduce additional (normally synthetic) patterns in the classes with the minority.
\item \textit{Modifying the cost associated to misclassifying the positive and the negative class} to compensate for the imbalanced ratio of the two classes. For example, if the imbalance ratio is 1:10 in favour of the negative class, the penalty of misclassifying a positive example should be 10 times greater.
\end{itemize}

The first option has been applied in some works, following a random undersampling approach \cite{random_undersampling_08}, but the problem is the loss of valuable information. 

On the other hand, the second option has been so far the most exploited, following different approaches, such as SMOTE (Synthetic Minority Oversampling Technique) \cite{smote_02}, a method proposed by Chawla et al. for creating `artificial' samples for the minority class, in order to balance the amount of them with respect. However this technique is based in numerical computations, which consider different distance measures, in order to generate useful patterns  (i.e. realistic or similar to the existing ones).

The third option implies using a method in which a cost can be associated to the classifier accuracy at every epoch. This was done for instance by Alfaro-Cid et al. in \cite{cost_adjustment_07}, where they used a Genetic Programming approach in which the fitness function was modified in order to consider a penalty when the classifier makes a false negative (an element from the minority class was classified as belonging to the majority class).

However almost all the approaches deal with numerical (real, integer) data. In our case, the dataset contains a majority of categorical/nominal data, so we have performed different approaches for data balancing:
\begin{itemize}
\item Undersampling: we will remove random samples of the majority class. ***REF?***
\item Oversampling: we will duplicate random samples of the minority class, in order to get a closer number of patterns in both classes. This has be done due to the impossibility of creating synthetic data when dealing with categorical values (there is not a proper distance measure between two values in a category). ***REF A ALGO SIMILAR HECHO***
\item We have not used a cost-based method... *** MIRAR SI ES VERDAD O JUSTIFICARLO MEJOR ***
\end{itemize}

These approaches will be evaluated in the experiments section.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   METHODOLOGY  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Methodology}
\label{sec:methodology}

descripción de la solución/metodología a aplicar. 

% ------------------------------------------------------------------
%
\subsection{Security rules parsing}
\label{subsec:ruleparsing}

In this work we have considered Drools \cite{drools_site} as the tool to create and therefore managing rules in a business environment. This so called Business Rule Management System (BRMS) has been developed by the JBoss community under an Apache License and it is written in Java. Though this platform consist of many components, here we focus on Drools Expert and the Drools Rule Language (DRL, \cite{drools_doc}). Then, the defined rules for a certain company are inside of a file with a \textt{.drl} extension, the file that needs to be parsed to obtain the final set of rules. In Figure \ref{fig:drools_hash}, (a), there is the typical rule syntax in DRL. Two main things should be obtained from the parsing method: both left and right sides of the rule, taking into account that the left side is where the company specifies the conditions required to apply the action indicated in the right side. Also, for describing the conditions, Squid syntax is used (see Section \ref{sec:problemDescription}), having thus the following structure: \texttt{squid:Squid(\textit{conditions})}. Finally, from the right side of the rule, the \textit{ALLOW} or \textit{DENY} label to apply on the data that matches with the conditions, will be extracted. The Perl parser that we have implemented applies two regular expressions, one for each side of the rule, and returns a hash with all the rules with the conditions and actions defined. The `before and after' performing the parsing over the \texttt{.drl} file is in Figure \ref{fig:drools_hash}.

\begin{figure}[htb]
\centering
\caption{(a) Structure of a rule in Drools Expert. (b) Resulting rule, after the parsing, in a global hash of rules. \label{fig:drools_hash}}
\subfloat[Drools Rule]{
\begin{tabular}{ p{0.05cm} p{0.05cm} p{2.5cm} }
  \texttt{rule~"name"} & & \\
   & \texttt{attributes} & \\
   & \texttt{when} & \\
   & & \texttt{/* Left Side of the Rule */} \\
   & \texttt{then} & \\
   & & \texttt{/* Right Side of the Rule */} \\
  \texttt{end} & & \\
\end{tabular}
}
~
\subfloat[Hash Rule]{
\begin{tabular}{ p{0.05cm} p{0.05cm} p{3cm} }
  \texttt{\%rules~=~(} & & \\
   & \texttt{rule~=>\{} & \\
   & & \texttt{field => xxx} \\
   & & \texttt{relation => xxx} \\
   & & \texttt{value => xxx} \\
   & & \texttt{action => [allow, deny]} \\
   & \texttt{\},} & \\
  \texttt{);} & & \\
\end{tabular}
}
\end{figure}

% ------------------------------------------------------------------
%
\subsection{URL data parsing}
\label{subsec:urlparsing}

The other solved problem was to extract from a whole URL the part that was more interesting for our purposes. It is important to point out that in a log with thousands of entries, an enormous variety of URLs can be found, since some can belong to advertisements, images, videos, or even some others does not have a domain name but are given directly by an IP address. For this reason, we have taken into account that for a domain name, many subdomains (separated by dots) could be considered, and their hierarchy grows from the right towards the left. The highest level of the domain name space is the Top-Level Domain (TLD) at the right-most part of the domain name, divided itself in country code TLDs and generic TLDs. Then, a domain and a number of subdomains follows the TLD (again, from right to left). This way, the URLs in the used log are such as \textit{http://subdomain...subdomain.domain.TLD/other_subdirectories}. However, we think that for labelling purposes, only the domain (without the subdomains and the TLD) should be considered, because there are too many different URLs to take into consideration. Hence, applying another regular expression, the data parser implemented in Perl obtains all the core domains of the URLs, which makes 976 domains in total.

% ------------------------------------------------------------------
%
\subsection{Data Mining Methods}
\label{subsec:methods}

% Ránking:
% 
% Basados en reglas:
% 
% 1- NNge 96'49% / 98'76%
% 2- PART 96'45% / 97'54%
% 3- DTNB 94'75% / 95'65%
% 4- Decision Table 94'08% / 90'30%
% 5- One R 93'45% / 93'7%
% 6- JRip 90'08% / 92'47%
% 7- Ridor 87'22 % / 89'87%
% 8- Conjunctive Rule 60'14% / 60'02%
% 9- Zero R 51'39% / 51'26%
% 
% Basados en árbol:
% 
% 1- J48 97'02% / 98%
% 2- Random Forest 96'87% / 98'84%
% 3- REP Tree 96'79% / 97'67%
% 4- Random Tree 95'14% / 98'35%
% 5- LAD Tree 79'95% / 79'97%
% 6- AD Tree 77'73% / 77'68%
% 7- Decision Stump 60'14% / 60'02%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   RESULTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Obtained Results}
\label{sec:results}

describir y analizar lo que se ha obtenido en la clasificación (acierto, tasa de error, etc)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Conclusions and Future Work}
\label{sec:conclusions}

qué se puede concluir del trabajo y qué haremos a continuación


%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
%This paper has been funded in part by European and National projects FP7-318508 (MUSES) and TIN2011-28627-C04-02 (ANYSELF) respectively, project P08-TIC-03903 (EV\-ORQ) awarded by the Andalusian Regional Government, and project 83 (CANUBE) awarded by the CEI-BioTIC UGR.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{datamining_urls} 

\end{document}
