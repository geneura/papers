\documentclass[conference]{IEEEtran}
% If the IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it: e.g.,
% \documentclass[conference]{../sty/IEEEtran}

\usepackage[latin1]{inputenc}
\usepackage{graphicx,color,longtable,multirow,times,amsmath,url}
\usepackage[dvips]{epsfig}
\usepackage{textcomp}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fancyvrb}
\usepackage{subfigure}
\usepackage{listings}
% correct bad hyphenation here
\hyphenation{}

\IEEEoverridecommandlockouts    % to create the author's affliation portion
                % using \thanks

\textwidth 178mm    % <------ These are the adjustments we made 10/18/2005
\textheight 239mm   % You may or may not need to adjust these numbes again
\oddsidemargin -7mm
\evensidemargin -7mm
\topmargin -6mm
\columnsep 5mm

\setlength{\textfloatsep}{8pt plus 2pt minus 2pt}
\setlength{\intextsep}{8pt plus 2pt minus 2pt}

\begin{document}

%TODO LIST
% Acronimos GP y GA

%DUDAS PARA ANTARES
% En cada evaluaci�n, cuantas batallas hay?
% Qu� mapas se usan en cada evaluaci�n?

% paper title: Must keep \ \\ \LARGE\bf in it to leave enough margin.
\title{\ \\ \LARGE\bf Struggling to Survive: Evolving RTS Bots via
  joust selection}

%Usar palabras negativas como "without" en t�tulos no me acaba de
%gustar, porque no dice qu� hace, sino qu� no se hace. "using real
%tournaments" pod�a ser una alternativa. Y mejor si los llam�rais
%"joust" o justas, "using joust tournaments" y llamar al sistema
%"joust evaluation", por ejemplo.

% Antonio - Aclararnos bien con Explicit o Implicit. :D

% Tampoco me gusta "Fighting to survive": "Fight for life" o
% "Struggling to survive" est� mejor dicho - JJ FERGU: Struggling,
% pues, que lo he visto en pelis

% �que no me gusta el without, co�e! Que lo quit�is y pong�is una
% frase positiva, no una negativa. He puesto joust selection, hala - JJ

\author{A.J. Fern\'andez-Ares, P. Garc\'ia-S\'anchez, A.M. Mora, P. A. Castillo and J.J. Merelo \thanks{Department of Computer Architecture and Computer Technology, University of Granada, Spain, {\tt \{antares,pablogarcia\}@ugr.es, \{amorag,pedro,jmerelo\}@geneura.ugr.es}}}
% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership
% use only for invited papers
%\specialpapernotice{(Invited Paper)}

% make the title area
\maketitle

\begin{abstract}
This paper proposes an evolutionary algorithm for evolving game bots
that eschews an explicit fitness function for an actual match between
individuals, which we will call {\em joust}, % pod�a quedar hasta bien
                                % - JJ
implemented as a selection mechanism. Instead of measuring
fitness by making the bots
perform certain tasks or fight against baseline bots, they fight with
each other and  the winner will  survive, passing  to the next
generation. Explicit evaluation is thus omitted and substituted by
explicit comparisons between bots. This algorithm has been designed as
an optimization to generate the behavioural engine of
bots for the RTS game Planet Wars using Genetic Programming. This algorithm has two
objectives: first, to  deal better with the noisy nature of the fitness
function (the evaluation for the same individual may vary from one
time to another, due to the stochastic component of the combats); and
second, obtaining more general bots than those evolved considering a
specific opponent, which are optimized to fight against it, and so,
they are specialized bots. %In addition, avoiding the evaluation step
%ideally will reduce the algorithm time consumption.
% Todas estas cosas tienes que probarlas.
%Different approaches are proposed and compared, namely %steady-state and generational implementations, and a pool-based approach in which a combat arena with potential individuals is considered. Each of them applies 1 vs
%1 bots combats.
Results shows that the generated bots deal better with the noise, obtaining competitive bots with respect other bots available in the literature.

%They implement different exploration versus
%exploitation tradeoffs in order to decide the best balance between
%these factors.
% �Por qu� consideras todas estas cosas? �Por qu� es
% importante la exploraci�n frente a la explotaci�n en
% este tipo de trabajos?
% Antonio - porque pienso que es importante en todos los algoritmos de optimizaci�n decidir el justo equilibrio entre exploraci�n y explotaci�n. Pero no queda muy adecuado aqu�, as�q ue lo quito.
\end{abstract}


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introduction}
\label{sec:intro}
%
Evolutionary Algorithms (EAs) have been widely applied in a number of problems, including videogames area \cite{Ponsen_EvLearn_RTS,co-evol-rts2006,Su-EAs_StrategySel09,cooperativebots_CIG2010,Cook_Platforming2012}. This metaheuristic performs very well in most of cases, but the presence
 of the so-called $noise$ in the evaluation process can make them not
 working properly \cite{Genebot_JCST}.

% citas sobre el tema, incluyendo los tuyso
                    % propios - JJ
This problem is quite common in the videogames scope, due to the
pseudo-stochasticity present in some factors, such as the game rules
or status, the opponents' behaviour or the random initial conditions,
which obviously have an influence on the score obtained by the agents,
which is usually the base (sometimes the only one) for giving an agent  a
fitness in an EA.
This problem also arises when the opponents follow non-deterministic
Artificial Intelligence (AI) behavioural models, i.e. when they are
Non-Playing Characters (NPC) or $bots$, since their behaviour
considers stochastic factors which can influence the result of the
game, and can vary from time to time. % citas para todo. FERGU TODO

Planet Wars, the game used in the Google AI Challenge
2010\footnote{http://planetwars.aichallenge.org/}, is not an exception
when an EA is applied to improve bots for playing it
\cite{Genebot-IWANN2011,Genebot_CEC11,Genebot_CIG2012},  and presents
this problem in the fitness calculation phase \cite{Genebot_JCST}. Usually several matches are carried out for the same
individual (maybe in different maps or against different opponents)
and then its fitness value is computed as an average or sum of all the obtained results. This way, a more accurate (less noisy) measure of the individual's quality is obtained, but it is still not a completely reliable solution because it depends on some values (number of victories or number of created ships, for example) or on the rival's performance, which could be a previously created bot.

% decid por qu� no es perfecta FERGU: depende de par�metros y del oponente
% Antonio - he cambiado lo de 'perfect' por 'reliable', ya que no habr� soluci�n perfecta, sino m�s o menos fiable/representativa

Even if we could obtain a statistically significant fitness
evaluation, the way this fitness is obtained might include an additional
bias due to opponent selection. This issue concerns the overfitting
of the population with respect the selected rival/s, i.e. the
individuals learn to play against it/them, and could behave poorly
against another type of enemy \cite{Genebot_JCST,wilcoxon:ga}.

% cita. Que se vea que no te lo has inventado. - JJ TODO
% Antonio - S�, mejor con una cita FERGU: a�adido

The present paper proposes an co-evolutionary EA \cite{EAs_Back96} for improving bot's AI in Planet Wars by means of an implicit fitness evaluation,
% Antonio - sera 'implicit', no? EXPLICIT = NUMERICO, pa no olvidarnos ;)
based in the \textit{survival of the individuals}.
To this end the selection process is transformed into a \textit{tournament} (or \textit{joust}, to distinguish it from the classical tournament in EAs) in which just the winners will survive and become parents of the new offspring. This way, the fitness computation is omitted and thus, the influence of noise is reduced.  Moreover, it does not require the usual ad-hoc parameters, such as the number of battles or the score values, neither a previously existent bot to compare.
This model is closer to the \textit{real natural selection process} which happens in nature \cite{darwin1859}, where just the fittest individuals survive.
% Antonio - no s� si explicar esto mejor. Buscar una cita???

A Genetic Programming (GP) \cite{GP_Koza92} approach has been implemented, due to the additional flexibility factor that this method offers with respect to a Genetic Algorithm (GA), i.e. GP is able to create new sets of behavioral rules meanwhile GA is devoted to optimize the parameters of previously designed rules. Moreover, this technique has yielded excellent results in previous works related with agent generation in videogames \cite{GarciaGP14,EsparciaGP2013}.
% Antonio - Poner cita a los trabajos anteriores de GP en Planet Wars (nuestros o de otros) FERGU: est�n puestos m�s abajo

% Antonio - Dejar esto para luego (para las conclusiones o para los comentarios de los resulatdos)
% It is not completely avoided due to the pseudo-stochasticity of the bots' behaviours.

Since the algorithm runs over Planet Wars, the tournament is modeled as a battle in the game. %However, in order to deal with the still present noise (pseudo-stochasticity of the bots' behaviors) every battle consists in a set of matches.
%TODO Borrar esto si al final se hace contra uno o varios (fergu) BORRADO.
Thus, the survivor of every battle (the one who wins)
passes to the next generation and also become a parent for the next
offspring. The loser is removed.

% Una cosa es que no hables en general de la selecci�n
% natural y todo eso y otro que no la menciones.
% Antonio - ya lo he puesto yo antes. ;)

In addition,  considering all the individuals in the population as opponents, and not using a specific one, makes the training (evolution) more
general, and thus, the obtained individuals would, potentially, be able to face a wider amount of possible rivals.
%Vale, a buenas horas...

%Two different Genetic Algorithms (GAs) have been implemented and
%studied in this work: the common steady-state \cite{Genitor_whitley},
%and generational models \cite{GAs_Goldberg89}. 1 vs 1 and 1 vs 3
%battles have been considered, getting four approaches with different
%levels of diversity, i.e. different exploitation/exploration
%factors. % Pero �por qu�?
%TODO descomentar si se usan y justificar

% *** ���Decir que es un tipo de co-evoluci�n??? �No lo son todos los algoritmos que hemos hecho y no lo hemos dicho? ***

% *** Hablar de los experimentos y de las comparaciones que se har�n ***
In this paper, an algorithm that creates bots using GP, without explicit (that is, numerical) fitness evaluation is presented. Several experiments to measure the convergence and the noise impact, with comparisons with other bots available in the literature, will be conduced to solve the next research questions:
\begin{itemize}
\item Is the implicit fitness evaluation proposed a feasible way to evaluate individuals?
\item Is this approach less sensible to noise than others?
\item How does affects not using a previously defined opponent?
\item How good is the behaviour of the generated bots?
\end{itemize}
% Antonio -  Mejorar estos puntos y poner otros si hace falta

% Antonio - [TODO] mejorar este texto de abajo:
All these questions have been answered in the experiments, in which the evolution progress (during the run) is analysed, along with the associated noise to the best obtained bots. Then, the these bots are tested against some of the state-of-the-art to check their competitiveness.

The paper is structured as follows. The background section reviews related work regarding the scope (videogames) and the implementation (EAs with no fitness computation or different selection mechanisms). It also describes the problem enclosed in the Planet Wars game.
Section \ref{sec:survival_bots} presents the proposed method, termed {Survival Bot}.
The experiments analyzing the evolution, noise and obtained bots are described and discussed in Section \ref{sec:experiments}. Finally, the conclusions and future lines of research are presented in Section \ref{sec:conclusions}.
%ANTONIO RECUERDA QUE ES INGLES AMERICANO, NO PONER ANALYSE NI COSAS BRITISH!!!!!
% Antonio - Arreglar esto con la distribuci�n final. Candidato a ser borrado por falta de espacio. :D


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  STATE OF THE ART  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Background}

% Ten�is que introducir esto de alguna forma. �Cu�l es el contexto?
% �No pon�is nada de evaluaci�n impl�cita del fitness? �No ser�a el
% contexto m�s bien temas de ruido en la evaluaci�n? Se puede hacer
% una intro general a CI en RTS, pero es un contexto demasiado amplio
% y ese no es el tema del trabajo - JJ
\subsection{Computational intelligence in RTS games}
\label{subsec:soa}
% �A santo de qu� esto?

% No os olvid�is de citar lo del Wilcoxon, que est� publicado en
% figshare y est� en geneura.bib - JJ
Evolutionary Computation (EC) has been applied in a wide variety of issues inside the videogames scope. One of the most profiting areas inside them is the parameter optimisation of behavioural engines \cite{Mora-Evo2010,cooperativebots_CIG2010,Genebot-IWANN2011}.
% Antonio - poner citas de otra gente que evolucione par�metros de bots
%
% or content generation \cite{LaraMaps14}, among others.
% Antonio - quito esto que no viene a cuento aqu�. ;)
%
This is normally a step in the generation of an AI engine to control the NPCs that play the game. The first approach is to create a set of rules by a human expert, and then optimize the set of parameters which determine how this bot behaves. This kind of improvement has been previously performed in \cite{Genebot_CEC11,genebot-evo12,Genebot_CIG2012,CarSetup} by means of off-line (before the game) Evolutionary Algorithms.
Furthermore, the use of GP \cite{GarciaGP14,EsparciaGP2013} dispenses the human expert to define the set of rules, as these rules, along with their numerical parameters, are created and evolved automatically during the run.  Thus, it is a more flexible approach for defining the behavioural engine of the bots, which can find rules that a human expert cannot imagine at all. For this reason GP has been used in this paper to generate the engines.

%, and comparing if this approach can compete with bots whose set of rules have been defined by an human expert, and optimized using GAs.
% Antonio - esto no s� si es verdad FERGU: s� es verdad, pero al final no sale tan bueno este bot contra ellos...

One of the shortcomings of previous works is that they depend on a baseline bot, taken as rival during the evolution, or an ad-hoc fitness function that involves some kind of parameterization (for example, number of matches or scoring the actions). To avoid this, co-Evolutionary Algorithms (CEAs)
% Antonio - Cita??? FERGU: puesta
% Antonio - no, la cita que decia de los co-evolutivos. Ponla porfa XD
% Antonio - [TODO] De hecho habria que darle mucho m�s bombo a los co-evolutivos en el art�culo. Hablar un poco de ellos desde la intro, ya que esto es 'a shape of Co-evolutionary approach'. No lo pongo en la para no descuajaringar nada, pero estar�a bien. M�xime cuando en los experimentos s� que te refieres al m�todo como Co-GA. OJO tambi�n con eso. ;)
have been previously used in this scope, as it is a natural choice to use in problems where the behaviour of one agent is related to the behaviour of others \cite{Coevolving13Samothrakis}.

The co-evolutionary scheme was initially used in puzzle and board games such as Backgammon \cite{Pollack_Backgammon98}, or Go \cite{Runarsson_Go2005}.
The first work proposed a very simple hill-climbing algorithm to evolve a population of neural networks, playing among them as rivals, in a competitive co-evolutionary approach. The latter paper presented a co-evolutionary learning approach which performed well once the EA was correctly tuned, moreover, this method yields better players to solve small Go boards since every individual is evaluated against a diverse population of rivals.
In the same line, there are some other works in the card games area, such as \cite{Thompson_Poker2008}, aimed to create Poker agents, considering a co-evolution process in which the players are part of the learning process. This meant a difficult process to get robust strategies, due to the variation in opponents, but the results shown to fit with some recommended strategies according to experts.
The aim of the present work is to conduct a study implementing a similar co-evolutionary approach, being competitive in the selection of individuals as parents for the next offspring,
% Antonio - no hay fitness calculation!!! Cuidao con esto. ;D
but cooperative since all the opponents are also part of the same learning process (same population).
% Antonio - pon cita. Mira el art�culo de antares en el EVO* en el que met� un p�rrafo chulo de Co-evoluci�n con citas y dem�s. Pon lo que veas en la intro o aqu�, pero hay que poner m�s de este tema. ;)

In recent years, this type of EAs has been also applied to videogames, enclosed in the Computational Intelligence (CI) branch of AI.
For instance Togelius et al. \cite{Togelius_Cars2007} studied the co-evolution effects of some populations in car racing controllers, comparing the performance of a single population against various, implementing both generational and a steady-state approaches. Avery and Michalewicz introduced in \cite{Avery_Human2008} a co-evolutionary algorithm for the game TEMPO, which used humans as rivals for the individuals in the evolutionary process.
Cook et al. \cite{Cook_Platforming2012} presented a cooperative co-evolutionary approach for the automated design of levels in simple platform games. And recently Cardona et al. \cite{Cardona_MSPacman2013} studied the performance of a competitive algorithm for the simultaneous evolution of controllers to both Ms. PacMan and the Ghost Team which has to chase her.

Co-evolution has also been used in the RTS scope. Livingstone \cite{Livinstone_RTS2005} compared several AI-modelling approaches for RTS games, and proposed a framework to create new models by means of co-evolutionary methods. He considered two levels of learning in a hierarchical AI model (inside an own-created RTS), evolving at the same time different partners in different strategic levels, so it was a cooperative approach. It is different to the one proposed here, since in the present work the co-evolution occurs at the same level for all the individuals.
The work by Smith et al. \cite{Smith_RTS_SpatialTactics2010} presents an analysis on how a co-evolutionary algorithm can be used for improving students' playing tactics in RTS games. Other authors proposed using co-evolution for evolving team tactics \cite{Avery_RTS_Team2010}. However, the problem is how tactics are constrained and parametrised and how the overall score is computed.
Nogueira et al. \cite{Nogueira_HoF2013} considered in a recent
publication the use of a Hall of Fame as a set of rivals (in the
evaluation function) inside a co-evolutionary algorithm to create
autonomous agents for the RTS game {\em RobotWars}. An updated version of this algorithm was also applied to Planet Wars game \cite{NogueiraCoevolutionary14}. This approach is based in a self-learning algorithm similar to the one we are proposing, but focused in a subset of individuals (the elite) which might have a negative effect in the generalisation factor or the bots' knowledge. Moreover, they use an ad-hoc fitness function with specific parameters, taking into account several battles and extra score measurement. Also, using the evolution to a fixed set of players could not lead to strong players \cite{Coevolving13Samothrakis}.

The approach presented in this work implements a survival-based co-evolutionary scheme, which omits an explicit fitness computation. Instead, the agents or bots (individuals) compete against the rest in the so-called \textit{joust tournaments}. Thus, just the survivors will remain in the population and will reproduce to generate the next offspring.
This tries to minimize the influence of a \textit{noisy fitness function} \cite{Genebot_JCST} in the evolution of the individuals; i.e. a good fitness value could be assigned to a bad player by chance, and the other way round. Moreover the proposed scheme has two advantages with respect to previous works: not adding ad-hoc parameters (such as the number of victories), and not using a specific bot as rival during evolution, which would lead to a specialisation of the individuals to fight against it.


% Ya estamos con el este hizo este, este hizo el otro y el otor hizo
% el otro. y regarding lo otro, pues lo dem�s. Tienes que contar la
% historia del estado del arte (incluir tambi�n los papers de Carlos
% Cotta) y llegar al punto de ahora en el que vas a avanzar. FERGU: le he metido el problema que tienen los trabajos anteriores, y c�mo lo resolvemos nosotros arriba y al final.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROBLEM DESCRIPTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\subsection{Problem Description} % esto deber�as integrarlo a la
                              % introducci�n, para que tras la
                              % introducci�n fuera el SoA, que es lo
                              % cl�sico. FERGU: Movido a subsecci�n Background, ya est� despu�s del SOA
\label{sec:problemDescription}

The game used here as testbed, Planet Wars, is a simplified version of the game
{\em Galcon}\footnote{http://www.galcon.com/},
% Antonio - poner un enlace mejor que este que he encontrado. ;)FERGU: hombre, este es el oficial, no? xD
which models turn-based space battles between two to four
contenders. This game is interesting in research in the RTS scope because it models a minimal RTS game: only one type of resources (planets), one type of unit (spaceships) and only one type of attack. Therefore, it has been used previously in different fields of computational intelligence in games, such as competitive bot creation \cite{ziolko2012automatic,NogueiraCoevolutionary14} or map generation \cite{LaraMaps14,LaraCabrera2014aesthetic}.

 % �No hay ning�n
                                % art�culo cient�fico? Hay un mont�n
                                % de Keldon y Carlos Cotta sobre
                                % generaci�n de mapas - JJ FERGU: metidos.

\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/planet_wars_battle.eps,width=6cm}
\end{center}
\caption{Simulated screenshot of an early stage of a 1 vs 1 match in Planet Wars. White planets belong to the player (blue colour in the game), dark grey belong to the opponent (red in the game), and light grey planets belong to no player. The triangles are fleets, and the numbers (in planets and triangles) represent the ships. The planet size models the growth rate of the amount of ships in it (the bigger, the higher).}
\label{figura:PlanetWars1}
\end{figure}

A Planet Wars match takes place on a map (see Figure \ref{figura:PlanetWars1}) that contains several planets (neutral, enemies or owned), each one of them with a number assigned to it that represents the quantity of ships that the planet is currently hosting.

The aim of the game is to defeat all the ships in the opponent's planets. Although Planet Wars is a RTS game, this implementation includes the concept of {\em turn} (1 second slot to decide the actions), and each player has a maximum number of turns to accomplish the objective. At the end of the match, the winner is the player that remains alive, or that who owns more ships if more than one survives.

The problem in this paper is to create a bot's AI in order to win the game, i.e. able to defeat every possible opponent in a 1 vs 1 match.% (four independent bots fighting in the same map).
 The bot must react according to the state of the map in each simulated turn (input), returning a set of actions to perform in order to fight the enemy, conquering its resources, and, ultimately, wining the game.

There are two strong constraints which determine the possible methods to apply to design a bot: a simulated turn takes just one second (that is, the maximum time to decide next action is one second), and the bot is not allowed to store any kind of information about its former actions, about the opponent's actions or about the state of the game (i.e., the game's map).

%We start from a designed bot's AI \cite{Genebot_CEC11}, named Genebot. It was defined from scratch (by an expert player), so it consists in a predefined set of behavioural rules. These rules depend on a set of parameters, which model thresholds, probabilities and weights, and which in turn, define how the bot will behave.

Thus, the aim in this paper is to study the generation and improvement of that set of behavioural rules and parameters by means of some novel (in this scope) evolutionary approaches, based in the survival to evolve. They are described in the following section.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   SURVIVAL BOT  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Survival Bots}
\label{sec:survival_bots}


% Joeves, usad comentarios!!!!

%***\\
%Contar la idea general:
%	- sustituir la evaluaci�n y el fitness
%	- selecci�n en base a supervivencia (y quiz� a antig�edad)
%	- eliminaci�n del ruido
%	- pormenores:
%		   . 5 combates en 5 escenarios representativos
%			. rival dentro de la misma poblaci�n (auto-aprendizaje???)
%***\\

%*** tipos de codificaci�n, operadores ($BLX-\alpha$) ***

%A shape of co-evolution. They compete but the whole population (offspring) is improved.

%***All these approaches are novel, at least in the application to the present problem. The third one is completely new.***

% ------------------------------------------------------------------
%

This section describes the algorithm proposed in this work to generate competent bots (called {\em SurvivalBots}). A Genetic Programming \cite{GP_Koza92} algorithm to generate the agent's behavior is combined with different selection and replacement policies, using an implicit fitness computation in a co-evolutionary way.

% ----------------------------------------------------------------------------

\subsection{Bot generation using GP}
\label{subsec:generationgp}

To generate the bot's behavior the so-called {\em GPBot} algorithm (presented in \cite{GarciaGP14}) has been used as a reference. However, the proposed method follows a different philosophy, based in the survival of the individuals, highly inspired by the crude natural evolutionary process. To this end new selection and replacement mechanisms have been adopted in the SurvivalBot approach.

In our approach, as in GPBot, a GP algorithm is used to evolve a set of rules which, in turn, models a Decision Tree.
% Antonio - Pedro ped�a una cita de Decision Tree. Yo no la veo imprescindible, si la encontramos bien y si no, no hace falta.
During the evolution, every individual in the population (a tree) must be evaluated. To do so, the tree is set as the behavioural engine of an agent, which is then placed in a map against a rival in a Planet Wars match. %Depending on the obtained results, the agent (i.e. the individual) gets a fitness score, that will be considered in the evolutionary process as a measure of its value.

Thus, during the match the tree will be used (by the bot) in order to select the best strategy at every moment, i.e. for every planet a target will be selected along with the number of ships to send from one another.

These Decision Trees are binary trees of expressions composed by
two different \textit{types of nodes}:

\begin{itemize}
\item {\em Decision}: a logical expression composed by a variable, a less than operator ($<$), and a number between 0 and 1. It is the equivalent to a ``primitive'' in the field of GP.
\item {\em Action}: a leave of the tree (therefore, a ``terminal''). Each decision is the name of a function, and a ratio between 0 and 1. The function indicates to which target planet the bot must send a percentage of the available amount of ships in the planet (from 0 to 1). As the bot applies the tree one time per planet it uses each time the information of the current planet.

%when the bot is deciding what to do regarding a specific planet (when it is analysing the status of the planet and the possible actions).
% Antonio - El �rbol lo ejecuta un bot para decidir qu� hacer considerando un planeta, no lo ejecuta un planeta, �no?. Lo he intentado escribir as�. FERGU: lo he reescrito yo tambi�n para que sea m�s entendible
% Antonio - mira a ver si esto es verdad, Fergu. �se decide el porcentaje tambi�n aqu�, �no? FERGU: reescrito todo
\end{itemize}

The \textit{decisions} are based in the values of different \textit{variables} which are computed considering some parameters in the game. They were defined by a human expert in \cite{GarciaGP14}, and are shown next (with the acronyms, preceded by a $?$ character, that will be used in the rest of the paper):

\begin{itemize}
% Antonio - �estas interrogaciones se dejan?
\item {\em myShipsEnemyRatio} [?mSE]: Ratio between the player's ships and enemy's ships.
\item {\em myShipsLandedFlyingRatio} [?mSLF]: Ratio between the player's landed and flying ships.
\item {\em myPlanetsEnemyRatio} [?mPE]: Ratio between the number of player's planets and the enemy's ones.
\item {\em myPlanetsTotalRatio} [?mPT]: Ratio between the number of player's planet and total planets (neutrals and enemy included).
\item {\em actualMyShipsRatio} [?aMS]: Ratio between the number of ships in the specific planet that evaluates the tree and player's total ships.
\item {\em actualLandedFlyingRatio} [?maLF]: Ratio between the number of ships landed and flying from the specific planet that evaluates the tree and player's total ships.
\item {\em Random} [?R]: This decision was not included in the original GPBot. It has been included in the list to add a stochastic component to the agent, with the aim of performing the noise study presented in this work (in Section \ref{sec:experiments}). It is, essentially, a probability added to select one branch or the other.
\end{itemize}

Finally, the possible \textit{actions} (and acronyms) are:

\begin{itemize}
\item {\em Attack Nearest (Neutral|Enemy|NotMy) Planet} [ANN, ANE, ANNm]: The objective is the nearest planet.
\item {\em Attack Weakest (Neutral|Enemy|NotMy) Planet} [AWkN, AWkE, AWkNm]: The objective is the planet with less ships.
\item {\em Attack Wealthiest (Neutral|Enemy|NotMy) Planet} [AWN, AWE, AWNm]: The objective is the planet with the highest lower rate.
\item {\em Attack Beneficial (Neutral|Enemy|NotMy) Planet} [ABN, ABE, ABNm]: The objective is the  more profitable planet, that is, the one with the highest value for growth rate divided by the amount of ships.
\item {\em Attack Quickest (Neutral|Enemy|NotMy) Planet} [AQN, AQE, AQNm]: The objective is the easiest planet to be conquered: the lowest product between the distance from the planet being evaluated by the tree and the number of ships in the objective planet.
\item {\em Attack (Neutral|Enemy|NotMy) Base} [ANB, AEB, ANmB]: The objective is the planet with more ships (that is, the base).
\item {\em Attack Random Planet} [AR].
\item {\em Reinforce Nearest Planet} [RN]: Reinforce the nearest player's planet to the planet that is being evaluated by the tree.
\item {\em Reinforce Base} [RB]: Reinforce the player's planet with the highest amount of ships.
\item {\em Reinforce Wealthiest Planet} [RW]: Reinforce the player's planet with highest growth rate.
\item {\em Reinforce Weakest Planet} [RWk]: Reinforce the player's planet with less ships.
\item {\em Do nothing} [DN].

\end{itemize}

The bot's general behaviour is described in Algorithm \ref{algoturn}.
% esta frase queda como "descolgada"... no entiendo a qu� viene nombrar el Alg. 1 aqu� (al menos de esta forma).   [pedro]
% Antonio -  aclarado cambiando el orden


\begin{algorithm}[htb]
\begin{algorithmic}

\STATE {\em /* At the beginning of the execution the agent receives the tree */}
\STATE tree $\leftarrow$ readTree()
\WHILE{game not finished}
  \STATE {\em /* starts the turn */}
% Antonio - e.g. significa 'por ejemplo'. �No querr�s decir i.e. que significa 'es decir'?
  \STATE calculateGlobalPlanets() {\em /* e.g. Base or Enemy Base */}
  \STATE calculateGlobalRatios()  {\em /* e.g. myPlanetsEnemyRatio */}
  \FOR{Each p in PlayerPlanets}
    \STATE calculateLocalPlanets(p) {\em /*e.g. NearestNeutralPlanet to p*}
    \STATE calculateLocalRatios(p)  {\em /* e.g actualMyShipsRatio */}
    \STATE executeTree(p,tree)  {\em /* Choose and Send a percentage of ships to destination*/}
% Antonio - he a�adido 'choose'
   \ENDFOR
\ENDWHILE

\end{algorithmic}
\caption{Pseudocode of a GPBot. The same tree is used during all the agent's execution}
\label{algoturn}
\end{algorithm}


In addition, an example of a possible decision tree is presented in Figure \ref{fig:javatree}. This example tree has five nodes, including two decisions and three actions, and a depth of three levels.

\begin{figure}[htb]
% usar el estilo Algorithmic, esto queda fe�simo - JJ FERGU: esto es c�digo java, el algoritmic lo uso abajo

\begin{lstlisting}[frame=single,language=Java,tabsize=4]
if(myShipsLandedFlyingRatio < 0.696)
	if(actualMyShipsRatio < 0.421)
		attackWeakestNeutralPlanet(0.481);
	else
		attackNearestEnemyPlanet(0.913);
else
	attackNearestEnemyPlanet(0.891);
\end{lstlisting}

%\begin{Verbatim}[frame=single,fontsize=\small]
%
%if(myShipsLandedFlyingRatio < 0.696)
%   if(actualMyShipsRatio < 0.421)
%      attackWeakestNeutralPlanet(0.481);
%   else
%      attackNearestEnemyPlanet(0.913);
%else
%   attackNearestEnemyPlanet(0.891);
%
%\end{Verbatim}
\caption{Example of the code generated for a decision tree of an individual.}
\label{fig:javatree}
% Antonio - m�s que un decision tree es un rule-based system, no?
% Antonio - Si queremos hablar de decision trees, se podr�a dibujar uno modelando esas reglas. ;) FERGU: arreglao
\end{figure}


% ----------------------------------------------------------------------------


\subsection{Joust-based Selection}
The algorithm presented in \cite{GarciaGP14} % �El algoritmo de la anterior
                              % generaci�n? �Eso qu� es? - JJ FERGU: cierto, arreglado
is combined with an \textit{implicit fitness evaluation} for selection and
replacement. This `evaluation' is, in essence, a match between individuals, called {\em joust} (to distinguish it from the classical tournament in EAs), in a battle map of the game.
%which explains the title we have given to this paper.  % Esta frase casi que la quitar�a  [pedro] FERGU: ala, fuera
Thus, the selection of the two mating parents is performed each one in a battle. The winner of the match is selected to mate
% en lugar de crossover, pon "mate" [pedro] FERGU:  fale
and the loser will be definitely removed from the population (as it will be explained below).

%Two different types of battles will be compared: {\em 1vs1} and {\em All vs All of 4 bots},
% Antonio - esto lo pon�a yo mal tambi�n, no son combates 4 contra 4, sino 1vs3. ;D FERGU: arreglao.
%as they are the only possible match types in Planet Wars.
%In both configurations the winner will be selected to mating and the loser (i.e. the first to be eliminated) will be removed from the population. Therefore, in the case of All vs All, the individuals ending in 2nd and 3rd position neither are selected for mating nor removed.

%This implicit selection and replacement have been chosen because...  % �estoy en ascuas!  ��por qu�?!  ;)  [pedro]

This selection mechanism tries to emphasize the survival of the fittest individuals, since just the best bots will be chosen as parents, and thus, will remain one more generation. Actually, in this algorithm, the concept of generation is changed by `iteration', since it is not a classical evolutionary process, as will be deeply explained in the next section.

The use of such a selection/survival process tries to reduce the noise added by the fitness evaluation in the evolutionary process \cite{Genebot_JCST}. So, the individuals which are not able to win in a match are strongly penalised, and thus, removed from the population of the next iteration.

% --------------------------------------------------------------------------

\subsection{Replacement of losers}
\label{subsec:replacement}

%The bots that have lost, and the offspring generated in the previously explained battles are removed/added to the population following one of the two next policies: steady-state or generational mechanism.

%The replacement policy chosen is the steady-state, an implementation based in the classical Steady-State EA approach \cite{Genitor_whitley}.
%A steady-state algorithm has been chosen as replacement policy. In this case, the classical Steady-State EA approach \cite{Genitor_whitley} has been implemented.   % reescribo la frase; la dejo comentada a continuacion por si hay que restaurarla  ;)  [pedro]
%The replacement policy chosen is the steady-state, an implementation based in the classical Steady-State EA approach \cite{Genitor_whitley}.

% Antonio - la reescribo de otra forma. :D
The classical Steady-State EA approach \cite{Genitor_whitley} has been implemented as replacement policy. In it, the majority of the population remains the same in the following generation, and just a small subset of individuals are substituted (usually just the worst). This method aims to increase the exploitation factor in the EA, in order to increase the convergence, which is an interesting factor in a noisy search space as the scope of videogames is.

%, since just the worst individuals in the population are substituted by fitter ones.
%This is interesting for this problem because......   % otra vez en ascuas!   :D

% And this is interesting for this problem because... - JJ

Thus, the proposed approach follows this idea and just performs two battles (or jousts) per generation, the aforementioned selection policy. The contenders are randomly selected from all the individuals in the population (ensuring that the same individuals are not chosen for both battles).
% Antonio - revisad que esto es as� (se asegura que no se eligen los mismos individuos como padres).
The two winners of the battles will be the parents for the \textit{crossover operation}, which generates two new individuals (offspring), which will be also mutated.
% Antonio - Decir que tipo de cruce se ha implementado
% Antonio - decir que operador de mutacion se ha usado y justificarlo FERGU: explicado
In this paper, sub-tree crossover and 1-node mutation operators have been used, as they obtain good results in generation of bots using GP \cite{EsparciaGP2013}.
These individuals are inserted in the population after being created, substituting the bots that lose the jousts.

This approach presents a higher random component than the
original, % sera random, no stochastic. Stochastic se es o no se es,
          % es como estar embarazada - JJ FERGU: cambiado
due to the lack of a fitness value which can value every individual with a simple number. The random selection of all the individuals also increases the chance of reducing the presence of noisy bots, i.e. those which are not good enough to remain in the population.
% Antonio - no se incrementa la diversidad por seleccionar padres aleatorios, �no? Se incrementar�a si se eligiesen malos individuos como padres. ;D FERGU: esto lo has escrito tú! XD
This will be a key factor in the resolution of this problem, as will be proved in the experiments.


%In the second policy to be tested, the {\em generational} replacement\cite{GAs_Goldberg89}, half of the population is substituted every generation, having a big diversification and thus exploration factor.
%As in the previous configuration, the individuals are paired randomly,
%conducting battles. The winners (half of the population)
%will be the parents of the next offspring. % �Y por qu� se elimina el
                                % ruido? �No puede ser que gane uno
                                % por casualidad? - JJ
%Then, after applying the crossover and mutation, the rest of the population is generated (two descendants per couple). The parents are grouped with all the offspring and the bots which have lost are deleted at the same time, increasing this way the diversity/exploration of the algorithm.

Algorithm \ref{alg:completealgorithm} shows the combination of the GP approach, together with the implicit fitness evaluation, and the selection and replacement mechanisms.


\begin{algorithm}[htb]
\begin{algorithmic}

\STATE population $\leftarrow$ initializePopulation()
\WHILE{stop criterion not found}

  \STATE offspring,losers,selected $\leftarrow$ \{\}
%  \IF{type=steadystate}
%    \STATE N $\leftarrow$ 1
%  \ELSIF{}
%    \STATE N $\leftarrow$ population.size/4
%  \ENDIF

%  \FOR{1 to N}
    \STATE {\em /* Two random contenders for the joust */}%in 1vs1 and 4 contenders in All vs All*/}
% Antonio - lo del 4vs4 FERGU: comentado
    \STATE contenders $\leftarrow$ selectContenders(population-selected)
    \STATE {\em /* The contenders fight and the winner and loser are obtained */} %(in case of All vs All, the first to be defeated is the loser)*/}
    \STATE winner1,loser1 $\leftarrow$ battle(contenders)
    \STATE {\em /* Previously selected bots not participate again in the tournament */}
    \STATE selected $\leftarrow$ selected + winner1 + loser1
    \STATE {\em /* Contenders of the second joust */}
    \STATE contenders $\leftarrow$ selectContenders(population-selected)
    \STATE {\em /* The contenders fight and the winner and loser are obtained */}
    \STATE winner2,loser2 $\leftarrow$ battle(contenders)
% Antonio - poner m�s comentarios. Yo de hecho los suelo poner antes de cada instrucci�n (y m�s si son tan improtantes como estas)
% Antonio - Si no se quiere hacer dos pseudoc�digos para 1vs1 y para 1vs3, se pueden poner cosas comentadas sobre quienes son winner1 y loser1 en cada caso, poner algunas l�neas que s�lo se ejecutar�an en un caso o en otro, etc FERGU: puestos comentarios
    \STATE selected $\leftarrow$ selected + winner2 + loser2
    \STATE {\em /* The losers will be removed from the population */}
    \STATE losers $\leftarrow$ losers + loser1 + loser2
    \STATE {\em /* Evolutionary process */}
    \STATE son1,son2 $\leftarrow$ crossover(parent1,parent2);
    \STATE son1,son2 $\leftarrow$ mutation(son1,son2)
    \STATE offspring $\leftarrow$ offspring + son1 + son2
%   \ENDFOR
    \STATE {\em /* Replacement of the losers */}
    \STATE population $\leftarrow$ population - losers
    \STATE population $\leftarrow$ population + offspring

\ENDWHILE

\end{algorithmic}
\caption{Pseudocode of the proposed SurvivalBot.}
\label{alg:completealgorithm}
\end{algorithm}

% Antonio - falta darle un poco m�s de bombo a esto, parece una chumin�, pero en realidad... �ES LA POLLA! :D


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   EXPERIMENTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Experiments and Results}
\label{sec:experiments}

Several experiments have been conducted in order to study different issues of the proposed approaches, but having in mind that the main objective is the generation and improvements of bots using a co-evolutionary algorithm. The set of parameters considered in our co-evolutionary GP (Co-GP) algorithm is shown in Table \ref{tab:parameters}.
These parameters were previously used in \cite{GarciaGP14}, obtaining competitive bots. To do a fair comparison with the results obtained in that work the termination criteria has been also set to 8000 battles (therefore, 4000 generations). 30 executions of the Co-GP have been performed in order to obtain accurate statistics.   The population is randomly initialized creating trees of 3 levels. % completo la frase  [pedro]
% Antonio - cuidao que unas veces pones Co-GP y otras Co-GA FERGU: cambiado todo a Co-GP

%For each one of the presented approaches (replacement and selection policies),

\begin{table}
\begin{center}
\begin{tabular}{|c|p{2.5cm}|}
\hline
{\em Parameter Name} & {\em Value} \\\hline \hline
Population size & 32 \\\hline
Initialization & Random \\ \hline
Crossover type & Sub-tree crossover \\ \hline
Crossover rate & 0.5\\ \hline
Maximum number of turns per battle & 1000 \\\hline
Mutation  & 1-node mutation\\ \hline
Mutation step-size & 0.25 \\ \hline
Selection & 2-tournament \\ \hline
Replacement & Steady-state\\ \hline
Stop criterion & 4000 iterations \\ \hline
Maximum Tree Depth & 7  \\ \hline

Runs per configuration & 30 \\ \hline
Maps used in each evaluation & 1 random chosen among maps \#76 \#69 \#7 \#11 \#26\\ \hline
\end{tabular}
\caption{Parameter set used in the experiments.}     % [pedro]
\label{tab:parameters}
\end{center}
\end{table}

\subsection{Analysis of the executions}   % cambiar  executions  por  runs ?   % [pedro]
\label{subsec:analysisexecutions}

It is difficult to show the convergence of the populations using a implicit fitness evaluation, as the evaluated individuals (and therefore, the fitness of the average population) do not count with a numeric value to be plotted in time. In this paper, we propose a scoring method that takes into account the number of victories, turns to win and turns to be defeated % needed to be defeated? por que? - Jj FERGU: explicado debajo
 against a previously known bot. After the execution of our algorithm all the generated individuals during all the runs have been confronted versus the best GPbot obtained in \cite{GarciaGP14} (as it is the fairest opponent in terms of actions and parameters used) and scored using the next formula:
\begin{equation}
Score_{i}=\alpha+\beta+\gamma
\end{equation}

where

\begin{equation}
\alpha  = v, \alpha \in\left[0,N\right]
\end{equation}
% Antonio - no pongas esto como ecuacion. Es solo una aclaracion de un rango FERGU: no, tiene dos partes.

\begin{equation}
\begin{split}
\beta =N\times\frac{t_{win}+\frac{1}{N\times t_{MAX}+1}}{\frac{t_{win}}{v+1}+1},\\
\beta \in\left[0,N\right], \\
t_{win} \in\left[0,N\times t_{MAX}\right]
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\gamma  =\frac{t_{defeated}}{N \times t_{MAX}+1}, \\
\gamma \in\left[0,1\right], \\
t_{defeated} \in\left[0,N\times t_{MAX}\right]
\end{split}
\end{equation}

The terms used are: the number of battles ($N$) to test, the number of victories of the individual against GPbot ($v$), the total number of turns used to win GPbot ($t_{win}$), the total number of turns when the individual has been defeated by GPbot ($t_{defeated}$) and the maximum number of turns a battle lasts ($t_{MAX}$). This score aims to favour the victories against the turns to win and turns to be defeated, giving different ratios to each section. Therefore $\alpha$ has the highest ratio. The term $\beta$ add extra score taking into account the number of turns when the individual wins (lower numbers to win implies better bots), following a exponential curve.
Finally, the $\gamma$ term adds score from the turns to be beaten (higher number is better, as it is difficult to be beaten). The 1 in all denominators is used to avoid divide by 0.

Each individual has been tested 3 times in 10 different maps (the 5 used during evolution and other new 5 ones from the Google set), therefore $N=15$, and a the limit of turns is the default of the competition ($t_{MAX}=1000$). As previously said, this score has the same shortcomings that we are trying to avoid in this paper: it requires parametrization and an existing opponent. However, we will use this score as a way to measure the performance of our fitness-less approach.

Figure \ref{figura:Score_VS_GPBot} shows the boxplots of the score of the current population (of all 30 original runs) in different stages of the evolution in different maps (those used  with the evaluation and the new set). As can be seen, this score increases during the evolution; therefore there exist some kind of improvement during the execution of the Co-GP algorithm.



\begin{figure}[htb]
\tiny
\begin{center}
\includegraphics[clip=true,width=9cm]{./imags/score_vs_gpbot.eps}
\end{center}
%Es muy peque�o. �No se puede poner a dos columnas? - JJ FERGU: agrandado
\caption{Score confronting all SurvivalBots obtained during all the runs versus GPbot.}
%�Qu� diablos es "not training" Ser� "not included in training" - �Qu�
%diablos es score vs. gpbot? �La puntuaci�n se enfrenta con GPBOT?
%Ser� score SurvivalBot vs. gpbot, �no? - JJ FERGU: s�, es que era un
%copypaste de mi mail. Arreglado caption, tama�o y quitado t�tulo
% En mi versi�n sigue con el mismo t�tulo. El eje horizontal deber�a
% llamarse Number of simulations or accumulated simulations o algo.
\label{figura:Score_VS_GPBot}
\end{figure}





%\begin{figure}[htb]
%\tiny
%\begin{center}
%  \epsfig{file=./imags/score_vs_gpbot_all_maps,width=6cm}
%\end{center}
%\caption{Score vs GPBot (all maps). Casi igual que las anteriores pero en boxplots SIN outliers (por eso la del best individual parece que el m�ximo est� en 5, con lo cual no deber�a estar en 23 en la que son l�neas). Estas dos figuras puedes quitarlas si quieres.}
%\label{figura:Score_VS_GPBot_AllMaps}
%\end{figure}

Figure \ref{figura:convergence} shows the average score of all individuals, and the average score of all best individuals, obtained during the evolution of the fitness-less Co-GP.
% Antonio - cuidao, que no se ha dicho antes que sea un co-evolutivo y no se han usado esas siglas. fERGU: ya sí
This figure shows how there exist an increasing performance in the best individuals during the runs ANTONIO-TERMINA-ESTO

\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/convergence_graph,width=7cm}
\end{center}
\caption{Average score of the best and worst individuals and the
  population average from all executions of our fitness-less Co-GP,
  versus GPbot to show the convergence.} % Esto �por qu�? - JJ FERGU: to show...
\label{figura:convergence}
\end{figure}





Figure \ref{figura:Victories_VS_GPBot_AllMaps} shows the percentage of individuals (normalized  from 0 to 1) that wins a certain number of times (from 0 to 30) against GPBot in the initial and the final populations. As can be seen there exist differences in the number of victories ANTONIO-TERMINA-ESTO

\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/victories_vs_GPBot_allmaps,width=9cm}
\end{center}
\caption{Histogram of number of victories of all the populations (initial and final) vs GPBot (all 30 maps).}
% Este histograma deberiais redudirlo a 10 bins o asi. Hay reglas para
% el numero de bins segun lo que quieras representar en un histograma,
% son muchas - JJ
% Y seguis sin hacerme caso a este comentario. Ser� Histogram of
% number victories for the algorithm population o algo asi - JJ FERGU: lo de unirlo por batallas lo hicimos pero quedaba fatal. Cambiado de todas formas el caption.
\label{figura:Victories_VS_GPBot_AllMaps}
\end{figure}


Moreover, the age of the individuals can help to understand how the evolution is being
performed. Figure \ref{figura:age} shows the ages of the agents during
one run. It is interesting how the age has a limit and it is not increased during all the run, meaning that a truly good bot is not
generated at first and living during all the evaluation. ANTONIO-TERMINA-ESTO

\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/ageall,width=9cm}
\end{center}
\caption{Boxplots of the age (generations) of the population during one run.}

\label{figura:age}
\end{figure}


% Y enlazar con el siguiente, no me gusta nada el comienzo - JJ FERGU: he cambiado secciones de sitio


\subsection{Analysis of the noise}
\label{subsec:analysisnoise}

Figure \ref{figura:noise} shows the noise factor of GPBot and SurvivalBot. ANTONIO-TERMINA-ESTO
\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/noise_study,width=7cm}
\end{center}
\caption{Noise factor of the best 30 bots obtained using GPBot and SurvivalBot approaches, evaluated in 10 different maps (5 previously trained and 5 not previously trained), 30 times/battles per map. The value for each bot is the difference between the maximum and the minimum scores obtained in every map after the 30 battles.}
\label{figura:noise}
\end{figure}




% --------------------------------------------------------------

\subsection{Analysis of the generated bots}
\label{subsec:analysisbots}

Finally, all the SurvivalBots obtained at the end of the evaluations have been tested to other bots available in the literature. First, the {\em best} individual of each run has been obtained confronting an all members of the last generation among them (i.e. all vs. all). The individual who has won more times is considered the best of the run. This method has been chosen to avoid the usage of the score function (and therefore, the shortcomings we are trying to avoid). Thus, we have confronted the 30 bots obtained in each configuration again with several bots available in the literature, in the 100 maps provided by Google. This experiment has been used to validate if the obtained SurvivalBots can be competitive in terms of quality in maps not used for evaluation, and against bots not used for evaluation (as other approaches did). Table \ref{tab:literaturebots} shows the bots used as enemies. 

Figure \ref{figure:boxplot_mejores_contra_clasicos} shows the boxplots of the percentage of victories of the SurvivalBots. Note that it only shows the victories, not the draws. The most interesting result is that GPbot is clearly outperformed, even if the execution time (number of battles) has been the same to train SurvivalBot, as we predicted. Therefore, our method can create competitive bots without using existent ones. The HoFBot, which was also obtained using co-evolution, has also been beaten more than 50\% times by the most of the generated SurvivalBots. However, highly trained bots (GeneBot and ExpGenebot), which required 4 times more evaluations to be generated have been difficult to beat. It is interesting to mention that in \cite{GarciaGP14} GPbot was able to beat these two bots in a higher value, but because they were used to train GPbot, so GPbot is more focused only in beat them.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\em Bot Name} & {\& Reference} & {\em Simulations in training} & {\em Max. Turns} \\\hline \hline
BullyBot & Google AI Web & None & None \\ \hline
SurvivalBot & (this) & 8000 & 1000 \\ \hline
GeneBot & \cite{Genebot_JCST} & 32000 & 1000 \\ \hline
ExpGeneBot & \cite{Genebot_CIG2012} & 32000 & 1000 \\ \hline
HoFBot & \cite{NogueiraCoevolutionary14} & 180000 & 500 \\ \hline
GPBot & \cite{GarciaGP14} & 8000 & 1000 \\ \hline


\end{tabular}
\caption{Bots available in the literature used for measuring the quality of the SurvivalBots.}     % [pedro]
\label{tab:literaturebots}
\end{center}
\end{table}





\begin{figure}[htb]
\tiny
\begin{center}
    \includegraphics[width=9cm]{./imags/boxplot_mejores_contra_clasicos.eps}

\end{center}
\caption{Boxplots confronting best SurvivalBots against existing bots in the literature.}
\label{figure:boxplot_mejores_contra_clasicos}
\end{figure}



Although this is not the main contribution of this paper, we also show the results of the obtained SurvivalBots,
analysing the distribution of actions and decisions at the beginning
and at the end of the evolution to understand their behaviour. Figure \ref{figura:tarta_actions}
shows the final distribution of the different types of actions
(attack, reinforce and do nothing). This figure shows that %As it can be seen, %no dig�is
                                %nunca As it can be seen. Se prueba o
                                %no se prueba, pero nunca se
                                %ve. Adem�s, ni siquiera hab�is puesto
                                %una referencia - JJ FERGU: arreglado
 the percentage of the {\em do
  nothing} action in the obtained trees has been increased % la acci�n se ha incrementado?
                                % Qu� significa eso? Ser� que el
                                % n�meor de veces que aparece esa
                                % acci�n nos� d�nde lo ha hecho, �no?
                                % - JJ
at the end of the evolution, as
it seems logical that not all planets should attack in every turn
(they may be waiting to have a large fleet to send). Of course, and
because a player wins conquering the enemy planets, it is logical that
the attack action is more effective than reinforce.

\begin{figure}[htb]
\tiny
\begin{center}


    \includegraphics[trim=1cm 7cm 1cm 5.8cm, clip=true,width=4.5cm,angle=-90]{./imags/distribution_initial_action.eps}
    \includegraphics[trim=1cm 7cm 1cm 6cm, clip=true,width=4.5cm,angle=-90]{./imags/distribution_final_action.eps}
    %\includegraphics{file=./imags/distribution_final_reinforce,width=3cm,angle=-90}


\end{center}
\caption{Distribution of different types of actions (attack, reinforce and do nothing) of the generated bots. See section \ref{subsec:generationgp} for more information about actions.}
% Estos gr�ficos est�n fatal. Las etiquetas est�n cortadas. Y no est�n
% etiquetadas cada tarta qu� significa. - JJ FERGU: es verdad, arreglado y enlazado
\label{figura:tarta_actions}
\end{figure}

Figure \ref{figura:tarta_attacking} shows the strategy when a planet is attacking to other. It is clear that the generated bots have a predilection for nearest planets and the ones easiest to conquer. This make sense because ships flying to long destinations are not being used, so, using a {\em rush} strategy makes a good option to conquer and advance.
\begin{figure}[htb]
\tiny
\begin{center}

    \includegraphics[trim=1cm 7cm 0cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_initial_attack.eps}
    \includegraphics[trim=1cm 7cm 0cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_final_attack.eps}
    %\includegraphics{file=./imags/distribution_final_reinforce,width=3cm,angle=-90}

\end{center}
\caption{Condition of target planet when attacking: Nearest, Weakest, Wealthiest, Beneficious, Quickest, Base or Random. See section \ref{subsec:generationgp} for more information about attack actions.}
\label{figura:tarta_attacking}
\end{figure}

Also, the actions are more focused in attacking planets owned by the Enemy (as can be seen in Figure \ref{figura:tarta_attacking_who}). This can be explained because the bot is not only conquering planets, but also destroying enemy ships that will not be used against him.
\begin{figure}[htb]
\tiny
\begin{center}

    \includegraphics[trim=1cm 7cm 1cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_initial_target.eps}
    \includegraphics[trim=1cm 7cm 1cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_final_target.eps}
    %\includegraphics{file=./imags/distribution_final_reinforce,width=3cm,angle=-90}

\end{center}
\caption{Owners of target planets when attacking: Enemy, Neutral, NotMy. See section \ref{subsec:generationgp} for more information about planet's owner.} % Ser� notmine, �no? No notmy. FERGU: viene de NotMyPlanet
\label{figura:tarta_attacking_who}
\end{figure}


Figure \ref{figura:tarta_reinforcing} shows the target when a planet is reinforcing the player's planet. As in attack actions, previously explained, seems a good rule is to focus in reinforce closer planets, for the same reasons.
\begin{figure}[htb]
\tiny
\begin{center}

    \includegraphics[trim=1cm 7cm 1cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_initial_reinforce.eps}
    \includegraphics[trim=1cm 7cm 1cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_final_reinforce.eps}
    %\includegraphics{file=./imags/distribution_final_reinforce,width=3cm,angle=-90}

\end{center}
\caption{Destination of planets when reinforcing: Near, Base, Wealthiest or Weakest. See section \ref{subsec:generationgp} for more information about reinforcement actions.}
\label{figura:tarta_reinforcing}
\end{figure}

Finally, Figure \ref{figura:tarta_decissions} shows the final proportion of decisions. Surprisingly the most important variable to take into account is the ratio between flying and landed ships. This can be explained because it makes sense to find an equilibrium between the two states of ships, as all ships flying or waiting may be counterproductive. The {\em random} decision also has importance, as it is the one who gives weights to the branches of the tree.
\begin{figure}[htb]
\tiny
\begin{center}


    \includegraphics[trim=1cm 5.5cm 0cm 5.5cm, clip=true,width=4.5cm,angle=-90]{./imags/distribution_initial_condition.eps}
    \includegraphics[trim=1cm 5.5cm 0cm 5.5cm, clip=true,width=4.5cm,angle=-90]{./imags/distribution_final_condition.eps}



\end{center}
\caption{Percentage of decisions. See section \ref{subsec:generationgp} for more information about decisions.}
\label{figura:tarta_decissions}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Conclusions and Future Work}
\label{sec:conclusions}

This paper presents an implementation of a quite simple approach: to omit the fitness-based selection mechanism in an EA, simulating the evolution in a more natural way: performing real battles of the individuals. This has been applied over the improvement of the behavioral parameters and rules of the bot's AI in the RTS game Planet Wars. This approach has two main benefits: it leaves out existing bots for the evaluation, and the parametrization of the fitness function, and also reduces the noisy fitness evaluation.

The classic tournament selection mechanism has been modeled as a battle in the game (called joust) to ANTONIO-TERMINA-ESTO

In future work, more rules will be added to the proposed algorithm (for example, to analyze the planet distances) and more enemies will be used for scoring. Also, a $4vs4$ joust will be tested, as it can reduce the noise even more (the 2nd and 3rd contender are not removed, neither mated). Other competition games used in the area of computational intelligence in videogames, such as Unreal\texttrademark~ or RobotWars\texttrademark~ will be used.

% en las conclusiones hay que generalizar y el tema es el uso de
% din�micas de juego en metaheur�sticas y c�mo se podr�a
% generalizar. Lo que tenemos al final es un orden parcial y ruidoso
% que se podr�a usar, en general, en todo tipo de algoritmos
% evolutivos donde haya ese problema - JJ



%The results obtained in this study are very promising, but they inherit a flaw from previous works, which is the low flexibility level due to the predefined set of rules/states that the bots follow. This means that almost every bot will eventually behave well, and the diversity in the search loses its relevance.
%The consideration of a more flexible approach for defining the behavioural engine of the bots, such as a Genetic Programming one [REF GP Genebot], could yield more interesting results and conclusions about the value of the methods proposed in this work.

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
% Antonio - actualizar esto. He quitado EVORQ y MUSES, pero ANYSELF termin� y CANUBE tambi�n, �no?
This paper has been funded in part by Spanish National project TIN2011-28627-C04-02 (ANYSELF) and project GENIL PYR-2014-17, awarded by the CEI-BioTIC UGR.

\bibliographystyle{IEEEtran}
\bibliography{genebot}

\end{document}
