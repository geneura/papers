\documentclass[conference]{IEEEtran}
% If the IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it: e.g.,
% \documentclass[conference]{../sty/IEEEtran}

\usepackage[latin1]{inputenc}
\usepackage{graphicx,color,longtable,multirow,times,amsmath,url}
\usepackage[dvips]{epsfig}
\usepackage{textcomp}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fancyvrb}
\usepackage{subfigure}
% correct bad hyphenation here
\hyphenation{}

\IEEEoverridecommandlockouts    % to create the author's affliation portion
                % using \thanks

\textwidth 178mm    % <------ These are the adjustments we made 10/18/2005
\textheight 239mm   % You may or may not need to adjust these numbes again
\oddsidemargin -7mm
\evensidemargin -7mm
\topmargin -6mm
\columnsep 5mm

\setlength{\textfloatsep}{8pt plus 2pt minus 2pt}
\setlength{\intextsep}{8pt plus 2pt minus 2pt}

\begin{document}

%TODO LIST
% Acronimos GP y GA

%DUDAS PARA ANTARES
% En cada evaluación, cuantas batallas hay?
% Qué mapas se usan en cada evaluación?

% paper title: Must keep \ \\ \LARGE\bf in it to leave enough margin.
\title{\ \\ \LARGE\bf Struggling to Survive: Evolving RTS Bots without
  Explicit Evaluation}

%Usar palabras negativas como "without" en títulos no me acaba de
%gustar, porque no dice qué hace, sino qué no se hace. "using real
%tournaments" podía ser una alternativa. Y mejor si los llamárais
%"joust" o justas, "using joust tournaments" y llamar al sistema
%"joust evaluation", por ejemplo. 

% Antonio - Aclararnos bien con Explicit o Implicit. :D

% Tampoco me gusta "Fighting to survive": "Fight for life" o "Struggling to survive" está mejor dicho - JJ FERGU: Struggling, pues, que lo he visto en pelis

\author{A.J. Fernández-Ares, P. García-Sánchez, A.M. Mora, P. A. Castillo and J.J. Merelo \thanks{Department of Computer Architecture and Computer Technology, University of Granada, Spain, {\tt \{antares,pablogarcia\}@ugr.es, \{amorag,pedro,jmerelo\}@geneura.ugr.es}}}
% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership
% use only for invited papers
%\specialpapernotice{(Invited Paper)}

% make the title area
\maketitle

\begin{abstract}
This paper proposes an evolutionary algorithm for evolving game bots
that eschews an explicit fitness function for an actual match between
individuals, which we will call {\em joust}, % podía quedar hasta bien
                                % - JJ
implemented as a selection mechanism. Instead of measuring
fitness by making the bots 
perform certain tasks or fight against baseline bots, they fight with
each other and  the winner will  survive, passing  to the next
generation. Explicit evaluation is thus omitted and substituted by
explicit comparisons between bots. This algorithm has been designed as
an optimization to generate the behavioural engine of
bots for the RTS game Planet Wars using Genetic Programming. This algorithm has two
objectives: first, to  deal better with the noisy nature of the fitness
function (the evaluation for the same individual may vary from one
time to another, due to the stochastic component of the combats); and
second, obtaining more general bots than those evolved considering a
specific opponent, which are optimized to fight against it, and so,
they are specialized bots. In addition, avoiding the evaluation step
ideally will reduce the algorithm time consumption.
% Todas estas cosas tienes que probarlas.
%Different approaches are proposed and compared, namely %steady-state and generational implementations, and a pool-based approach in which a combat arena with potential individuals is considered. Each of them applies 1 vs
%1 bots combats. 
*** RESULTS show that...***

%They implement different exploration versus
%exploitation tradeoffs in order to decide the best balance between
%these factors.  
% ¿Por qué consideras todas estas cosas? ¿Por qué es
% importante la exploración frente a la explotación en
% este tipo de trabajos?
% Antonio - porque pienso que es importante en todos los algoritmos de optimización decidir el justo equilibrio entre exploración y explotación. Pero no queda muy adecuado aquí, asíq ue lo quito.
\end{abstract}


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introduction}
\label{sec:intro}
%
Evolutionary Algorithms (EAs) have been widely applied in a number of problems, including videogames area \cite{Ponsen_EvLearn_RTS,co-evol-rts2006,Su-EAs_StrategySel09,cooperativebots_CIG2010,Cook_Platforming2012}. This metaheuristic performs very well in most of cases, but the presence
 of the so-called $noise$ in the evaluation process can make them not
 working properly \cite{Genebot_JCST}. 

% citas sobre el tema, incluyendo los tuyso
                    % propios - JJ
This problem is quite common in the videogames scope, due to the
pseudo-stochasticity present in some factors, such as the game rules
or status, the opponents' behaviour or the random initial conditions,
which obviously have an influence on the score obtained by the agents,
which is usually the base (sometimes the only one) for giving an agent (or $bot$) a
fitness in an EA. 
This problem also arises when the opponents follow non-deterministic
Artificial Intelligence (AI) behavioural models, i.e. when they are
Non-Playing Characters (NPC) or $bots$, since their behaviour
considers stochastic factors which can influence the result of the
game, and can vary from time to time. % citas para todo. FERGU TODO 

Planet Wars, the game used in the Google AI Challenge
2010\footnote{http://planetwars.aichallenge.org/} is not an exception
when an EA is applied to improve bots for playing it
\cite{Genebot-IWANN2011,Genebot_CEC11,Genebot_CIG2012},  and presents
this problem in the fitness calculation phase \cite{Genebot_JCST}. Usually several matches are carried out for the same
individual (maybe in different maps or against different opponents)
and then its fitness value is computed as an average or sum of all of the obtained results. This way, a more accurate (less noisy) measure of the individual's quality is obtained, but it is not still a completely reliable solution because it depends on some values (number of victories or number of created ships, for example) or on the rival's performance, which could be a previously created bot.

% decid por qué no es perfecta FERGU: depende de parámetros y del oponente
% Antonio - he cambiado lo de 'perfect' por 'reliable', ya que no habrá solución perfecta, sino más o menos fiable/representativa

Even if we could obtain a statistically significant fitness
evaluation, the way this fitness is obtained might include an additional
bias due to opponent selection. This issue concerns the overfitting
of the population with respect the selected rival/s, i.e. the
individuals learn to play against it/them, and could behave poorly
against another type of enemy. 

% cita. Que se vea que no te lo has inventado. - JJ TODO
% Antonio - Sí, mejor con una cita

The present paper proposes a EA \cite{EAs_Back96} for improving bot's AI in Planet Wars by means of an implicit fitness evaluation, 
% Antonio - sería 'implicit', ¿no?
based in the \textit{survival of the individuals}. 
To this end the selection process is transformed into a \textit{tournament} (or \textit{joust}) in which just the winners will survive and become parents of the new offspring. This way, the fitness computation is omitted and thus, the influence of noise is reduced.  Moreover, it does not require the usual ad-hoc parameters, such as the number of battles or the score values.
This model is closer to the \textit{real natural selection process} which happens in nature \cite{darwin1859}, where just the fittest individuals survive.
% Antonio - no sé si explicar esto mejor. Buscar una cita???

A Genetic Programming (GP) \cite{GP_Koza92} approach has been implemented, due to the additional flexibility factor that this method offers with respect to a Genetic Algorithm (GA), i.e. GP is able to create new sets of behavioural rules meanwhile GA is devoted to optimise the parameters of previously designed rules. Moreover, this technique has yielded excellent results in previous works \cite{GarciaGP14,EsparciaGP2013}.
% Antonio - Poner cita a los trabajos anteriores de GP en Planet Wars (nuestros o de otros)

% Antonio - Dejar esto para luego (para las conclusiones o para los comentarios de los resulatdos) 
% It is not completely avoided due to the pseudo-stochasticity of the bots' behaviours.
 
Since the algorithm runs over Planet Wars, the tournament is modelled as a set of battles in the game. However, in order to deal with the still present noise (pseudo-stochasticity of the bots' behaviours) every battle consists in a set of matches. 
%TODO Borrar esto si al final se hace contra uno o varios (fergu)
Thus, the survivor of every battle (the one who wins more matches) 
passes to the next generation and also become a parent for the next
offspring.  

% Una cosa es que no hables en general de la selección
% natural y todo eso y otro que no la menciones. 
% Antonio - ya lo he puesto yo antes. ;)

In addition,  considerating all the individuals in the population as opponents, not a specific one, makes the training (evolution) more
general, and thus, the obtained individuals would, potentially, be able to face a wider amount of possible rivals.  
%Vale, a buenas horas...


%Two different Genetic Algorithms (GAs) have been implemented and
%studied in this work: the common steady-state \cite{Genitor_whitley},
%and generational models \cite{GAs_Goldberg89}. 1 vs 1 and 1 vs 3
%battles have been considered, getting four approaches with different
%levels of diversity, i.e. different exploitation/exploration
%factors. % Pero ¿por qué?
%TODO descomentar si se usan y justificar

% *** ¿¿¿Decir que es un tipo de co-evolución??? ¿No lo son todos los algoritmos que hemos hecho y no lo hemos dicho? ***

% *** Hablar de los experimentos y de las comparaciones que se harán ***

%*****Moreover, the avoidance of the fitness computation reduces the running time in a half

Summarizing, this paper tries to solve the next research questions:
\begin{itemize}
\item Is the implicit fitness evaluation proposed a feasible way to evaluate individuals?
\item Is this approach less sensible to noise than others?
\item How does affects not using a previously defined opponent?
\item How is the behaviour of the generated bots?
\end{itemize}

% Antonio -  Mejorar estos puntos y poner otros si hace falta

The paper is structured as follows. The background section reviews related work regarding the scope (videogames) and the implementation (EAs with no fitness computation or different selection mechanisms). It also describes the problem enclosed in the Planet Wars game.
Section \ref{sec:survival_bots} presents the proposed method, termed {Survival Bot}, and its different implementations. 
The experiments and results are described and discussed in Section \ref{sec:experiments}. Finally, the conclusions and future lines of research are presented in Section \ref{sec:conclusions}.

% Antonio - Arreglar esto con la distribución final. Candidato a ser borrado por falta de espacio. :D


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  STATE OF THE ART  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Background}

% Tenéis que introducir esto de alguna forma. ¿Cuál es el contexto?
% ¿No ponéis nada de evaluación implícita del fitness? ¿No sería el
% contexto más bien temas de ruido en la evaluación? Se puede hacer
% una intro general a CI en RTS, pero es un contexto demasiado amplio
% y ese no es el tema del trabajo - JJ  
\subsection{Computational intelligence in RTS games}
\label{subsec:soa}
% ¿A santo de qué esto?

% No os olvidéis de citar lo del Wilcoxon, que está publicado en
% figshare y está en geneura.bib - JJ
Evolutionary Computation (EC) has been applied in a wide variety of issues inside the videogames scope. One of the most profiting areas inside them is the parameter optimisation of behavioural engines \cite{Mora-Evo2010,cooperativebots_CIG2010,Genebot-IWANN2011}.
% Antonio - poner citas de otra gente que evolucione parámetros de bots
%
% or content generation \cite{LaraMaps14}, among others.
% Antonio - quito esto que no viene a cuento aquí. ;)
%
This is normally a step in the generation of an AI engine to control the Non-Player Characters (NPCs) that play the game. The first approach is to create a set of rules by a human expert, and then optimize the set of parameters which determine how this bot behaves. This kind of improvement has been previously performed in \cite{Genebot_CEC11,genebot-evo12,Genebot_CIG2012,CarSetup} by means of off-line (before the game) Evolutionary Algorithms. 
However the use of GP \cite{GarciaGP14,EsparciaGP2013} dispenses the human expert to define the set of rules, as these rules, along with their numerical parameters, are created and evolved automatically during the run.  Thus, it is a more flexible approach for defining the behavioural engine of the bots, which can find rules that a human expert cannot imagine at all. For this reason GP has been used in this paper to generate the engines.

%, and comparing if this approach can compete with bots whose set of rules have been defined by an human expert, and optimized using GAs.
% Antonio - esto no sé si es verdad

One of the shortcomings of previous works is that they depend on a baseline bot, taken as rival during the evolution, or an ad-hoc fitness function that involves some kind of parameterization (for example, number of matches or scoring the actions). To avoid this, co-Evolutionary Algorithms (CEAs) 
% Antonio - Cita???
have been previously used in this scope, as it is a natural choice to use in problems where the behaviour of one agent is related to the behaviour of others \cite{Coevolving13Samothrakis}. 

The co-evolutionary scheme was initially used in puzzle and board games such as Backgammon \cite{Pollack_Backgammon98}, or Go \cite{Runarsson_Go2005}.
The first work proposed a very simple hill-climbing algorithm to evolve a population of neural networks, playing among them as rivals, in a competitive co-evolutionary approach. The latter paper presented a co-evolutionary learning approach which performed well once the EA was correctly tuned, moreover, this method yields better players to solve small Go boards since every individual is evaluated against a diverse population of rivals.
In the same line, there are some other works in the card games area, such as \cite{Thompson_Poker2008}, aimed to create Poker agents, considering a co-evolution process in which the players are part of the learning process. This meant a difficult process to get robust strategies, due to the variation in opponents, but the results shown to fit with some recommended strategies according to experts.
The aim of the present work is to conduct a study implementing a similar co-evolutionary approach, being competitive in the fitness calculation, but cooperative since all the opponents are also part of the same learning process (same population).

In recent years, this type of EAs has been also applied to videogames, enclosed in the Computational Intelligence (CI) branch of AI.
For instance Togelius et al. \cite{Togelius_Cars2007} studied the co-evolution effects of some populations in car racing controllers, comparing the performance of a single population against various, implementing both generational and a steady-state approaches. Avery and Michalewicz introduced in \cite{Avery_Human2008} a co-evolutionary algorithm (for the game TEMPO) which used humans as rivals for the individuals in the evolutionary process. 
Cook et al. \cite{Cook_Platforming2012} presented a cooperative co-evolutionary approach for the automated design of levels in simple platform games. And recently Cardona et al. \cite{Cardona_MSPacman2013} studied the performance of a competitive algorithm for the simultaneous evolution of controllers to both Ms. PacMan and the Ghost Team which has to chase her.

Co-evolution has also been used in the RTS scope. Livingstone \cite{Livinstone_RTS2005} compared several AI-modelling approaches for RTS games, and proposed a framework to create new models by means of co-evolutionary methods. He considered two levels of learning in a hierarchical AI model (inside an own-created RTS), evolving at the same time different partners in different strategic levels, so it was a cooperative approach. It is different to the one proposed here, since in the present work the co-evolution occurs at the same level for all the individuals. 
The work by Smith et al. \cite{Smith_RTS_SpatialTactics2010} presents an analysis on how a co-evolutionary algorithm can be used for improving students' playing tactics in RTS games. Other authors proposed using co-evolution for evolving team tactics \cite{Avery_RTS_Team2010}. However, the problem is how tactics are constrained and parametrised and how the overall score is computed. 
Nogueira et al. \cite{Nogueira_HoF2013} considered in a recent
publication the use of a Hall of Fame as a set of rivals (in the
evaluation function) inside a co-evolutionary algorithm to create
autonomous agents for the RTS game {\em RobotWars}. An updated version of this algorithm was also applied to Planet Wars game \cite{NogueiraCoevolutionary14}. This approach is based in a self-learning algorithm as the one we are proposing, but focused in a subset of individuals (the elite) which can have a negative effect in the generalisation factor or the bots' knowledge. Moreover, they use a ad-hoc fitness function with specific parameters, taking into account several battles and extra score measurement. Also, using the evolution to a fixed set of players could not lead to strong players \cite{Coevolving13Samothrakis}.

The approach presented in this work implements a survival-based co-evolutionary scheme, which omits an explicit fitness computation. Instead, the agents or bots (individuals) compete against the rest in the so-called \textit{joust tournaments}. Thus, just the survivors will remain in the population and will reproduce to generate the next offspring.
This tries to minimize the influence of a \textit{noisy fitness function} \cite{Genebot_JCST} in the evolution of the individuals; i.e. a good fitness value could be assigned to a bad player by chance, and the other way round. Moreover the proposed scheme has two advantages with respect to previous works: not adding ad-hoc parameters (such as the number of victories), and not using a specific bot as rival during evolution, which would lead to a specialisation of the individuals to fight against it.


% Ya estamos con el este hizo este, este hizo el otro y el otor hizo
% el otro. y regarding lo otro, pues lo demás. Tienes que contar la
% historia del estado del arte (incluir también los papers de Carlos
% Cotta) y llegar al punto de ahora en el que vas a avanzar. FERGU: le he metido el problema que tienen los trabajos anteriores, y cómo lo resolvemos nosotros arriba y al final. 

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROBLEM DESCRIPTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\subsection{Problem Description} % esto deberías integrarlo a la
                              % introducción, para que tras la
                              % introducción fuera el SoA, que es lo
                              % clásico. FERGU: Movido a subsección Background, ya está después del SOA
\label{sec:problemDescription}

The game used here as testbed, Planet Wars, is a simplified version of the game
{\em Galcon}\footnote{http://www.galcon.com/}, 
% Antonio - poner un enlace mejor que este que he encontrado. ;)FERGU: hombre, este es el oficial, no? xD
which models turn-based space battles between two to four
contenders. This game is interesting in research in the RTS scope because it models a minimal RTS game: only one type of resources (planets), one type of unit (spaceships) and only one type of attack. Therefore, it has been used previously in different fields of computational intelligence in games, such as competitive bot creation \cite{ziolko2012automatic,NogueiraCoevolutionary14} or map generation \cite{LaraMaps14,LaraCabrera2014aesthetic}. 

 % ¿No hay ningún
                                % artículo científico? Hay un montón
                                % de Keldon y Carlos Cotta sobre
                                % generación de mapas - JJ FERGU: metidos.

\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/naves.eps,width=6cm}
\end{center}
\caption{Simulated screenshot of an early stage of a 1 vs 1 match in Planet Wars. White planets belong to the player (blue colour in the game), dark grey belong to the opponent (red in the game), and light grey planets belong to no player. The triangles are fleets, and the numbers (in planets and triangles) represent the ships. The planet size models the growth rate of the amount of ships in it (the bigger, the higher).}
\label{figura:PlanetWars1}
\end{figure}

A Planet Wars match takes place on a map (see Figure \ref{figura:PlanetWars1}) that contains several planets (neutral, enemies or owned), each one of them with a number assigned to it that represents the quantity of ships that the planet is currently hosting. 

The aim of the game is to defeat all the ships in the opponent's planets. Although Planet Wars is a RTS game, this implementation includes the concept of {\em turn} (1 second slot to decide the actions), and each player has a maximum number of turns to accomplish the objective. At the end of the match, the winner is the player that remains alive, or that who owns more ships if more than one survives. 

The problem in this paper is to create a bot's AI in order to win the game, i.e. able to defeat every possible opponent in a 1 vs 1 match.% (four independent bots fighting in the same map).  
 The bot must react according to the state of the map in each simulated turn (input), returning a set of actions to perform in order to fight the enemy, conquering its resources, and, ultimately, wining the game. 

There are two strong constraints which determine the possible methods to apply to design a bot: a simulated turn takes just one second (that is, the maximum time to decide next action is one second), and the bot is not allowed to store any kind of information about its former actions, about the opponent's actions or about the state of the game (i.e., the game's map).

%We start from a designed bot's AI \cite{Genebot_CEC11}, named Genebot. It was defined from scratch (by an expert player), so it consists in a predefined set of behavioural rules. These rules depend on a set of parameters, which model thresholds, probabilities and weights, and which in turn, define how the bot will behave.

Thus, the aim in this paper is to study the generation and improvement of that set of behavioural parameters and rules by means of some novel (in this scope) evolutionary approaches, based in the survival to evolve. These will be described in the following section.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   SURVIVAL BOT  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Survival Bots}
\label{sec:survival_bots}


% Joeves, usad comentarios!!!!

%***\\
%Contar la idea general:
%	- sustituir la evaluación y el fitness
%	- selección en base a supervivencia (y quizá a antigüedad)
%	- eliminación del ruido
%	- pormenores: 
%		   . 5 combates en 5 escenarios representativos
%			. rival dentro de la misma población (auto-aprendizaje???)
%***\\

%*** tipos de codificación, operadores ($BLX-\alpha$) ***

%A shape of co-evolution. They compete but the whole population (offspring) is improved.

%***All these approaches are novel, at least in the application to the present problem. The third one is completely new.***

% ------------------------------------------------------------------
%

This section describes the algorithm used in this work to generate the bots (called {\em SurvivalBots}), and the different configurations tested. A Genetic Programming algorithm to generate the agent's behaviour is combined with different policies of selection and replacement using implicit fitness.

\subsection{Bot generation using GP}
\label{subsec:generationgp}

To generate the bot behaviour the {\em GPBot} algorithm, presented in \cite{GarciaGP14}, will be used in conjunction with our new proposed selection and replacement mechanisms. GPBot, the Genetic Programming algorithm used evolves a set of rules which, in turn, models a Decision Tree. 
During the evolution, every individual in the population (a tree) must be evaluated. To do so, the tree is set as the behavioural engine of an agent, which is then placed in a map against a rival in a Planet Wars match. Depending on the obtained results, the agent (i.e. the individual) gets a fitness value, that will be considered in the evolutionary process as a measure of its validity. 
 
Thus, during the match the tree will be used (by the bot) in order to select the best strategy at every moment, i.e. for every planet a target will be selected along with the number of ships to send from one the other.

The  Decision Trees used are binary trees of expressions composed by
two different \textit{types of nodes}: 

\begin{itemize}
\item {\em Decision}: a logical expression formed by a variable, a less than operator ($<$), and a number between 0 and 1. It is the equivalent to a ``primitive'' in the field of GP.
\item {\em Action}: a leave of the tree (therefore, a ``terminal''). Each decision is the name of the method to call from the planet that executes the tree. This method indicates to which planet send a percentage of available ships (from 0 to 1). 
\end{itemize}

 The decisions are based in the values of different \textit{variables} which are computed considering some other variables in the game. They are defined by a human expert and defined in \cite{GarciaGP14}, and are shown next (with their acronyms, that will be used in the rest of the paper):

\begin{itemize}
\item {\em myShipsEnemyRatio} [?mSE]: Ratio between the player's ships and enemy's ships.
\item {\em myShipsLandedFlyingRatio} [?mSLF]: Ratio between the player's landed and flying ships.
\item {\em myPlanetsEnemyRatio} [?mPE]: Ratio between the number of player's planets and the enemy's ones.
\item {\em myPlanetsTotalRatio} [?mPT]: Ratio between the number of player's planet and total planets (neutrals and enemy included).
\item {\em actualMyShipsRatio} [?aMS]: Ratio between the number of ships in the specific planet that evaluates the tree and player's total ships.
\item {\em actualLandedFlyingRatio} [?maLF]: Ratio between the number of ships landed and flying from the specific planet that evaluates the tree and player's total ships.
\item {\em Random} [?R]: This decision was not included in \cite{GarciaGP14} and it has been added to add stochasticity to the agent to perform the study presented in this work. It is a probability added to select one branch or the other.
\end{itemize}

Finally, the possible \textit{actions} (with acronyms) are:

\begin{itemize}
\item {\em Attack Nearest (Neutral|Enemy|NotMy) Planet} [ANN, ANE, ANNm]: The objective is the nearest planet.
\item {\em Attack Weakest (Neutral|Enemy|NotMy) Planet} [AWkN, AWkE, AWkNm]: The objective is the planet with less ships.
\item {\em Attack Wealthiest (Neutral|Enemy|NotMy) Planet} [AWN, AWE, AWNm]: The objective is the planet with higher lower rate.
\item {\em Attack Beneficial (Neutral|Enemy|NotMy) Planet} [ABN, ABE, ABNm]: The objective is the  more beneficial planet, that is, the one with highest growth rate divided by the number of ships.
\item {\em Attack Quickest (Neutral|Enemy|NotMy) Planet} [AQN, AQE, AQNm]: The objective is the planet easier to be conquered: the lowest product between the distance from the planet that executes the tree and the number of  ships in the objective planet.
\item {\em Attack (Neutral|Enemy|NotMy) Base} [ANB, AEB, ANmB]: The objective is the planet with more ships (that is, the base).
\item {\em  Attack Random Planet} [AR].
\item {\em Reinforce Nearest Planet} [RN]: Reinforce the nearest player's planet to the planet that executes the tree.
\item {\em Reinforce Base} [RB]: Reinforce the player's planet with higher number of ships.
\item {\em Reinforce Wealthiest Planet} [RW]: Reinforce the player's planet with higher grown rate.
\item {\em Reinforce Weakest Planet} [RWk]: Reinforce the player's planet with less ships.
\item {\em Do nothing} [DN].

\end{itemize}

An example of a possible decision tree is presented in Figure \ref{fig:javatree}. This example tree has a total of 5 nodes, with 2 decisions and 3 actions, and a depth of 3 levels. 
The bot's behaviour is explained in Algorithm \ref{alg:turn}.   % esta frase queda como "descolgada"... no entiendo a qué viene nombrar el Alg. 1 aquí (al menos de esta forma).   [pedro]

\begin{figure}
% usar el estilo Algorithmic, esto queda feísimo - JJ FERGU: esto es código java, el algoritmic lo uso abajo

\begin{Verbatim}[frame=single,fontsize=\small]
 
if(myShipsLandedFlyingRatio < 0.696)
   if(actualMyShipsRatio < 0.421)
      attackWeakestNeutralPlanet(0.481);
   else
      attackNearestEnemyPlanet(0.913);
else
   attackNearestEnemyPlanet(0.891);

\end{Verbatim}

\label{fig:javatree}
\caption{Example of the code generated from a decision tree of an individual.}
% Antonio - más que un decision tree es un rule-based system, no?
% Antonio - Si queremos hablar de decision trees, se podría dibujar uno modelando esas reglas. ;) FERGU: arreglao
\end{figure}



\begin{algorithm}[htb]
\begin{algorithmic}


\STATE {\em /*At the beginning of the execution the agent receives the tree*/}
\STATE tree $\leftarrow$ readTree()
\WHILE{game not finished}
  \STATE {\em /* starts the turn */}
  \STATE calculateGlobalPlanets() {\em /* e.g. Base or Enemy Base */}
  \STATE calculateGlobalRatios()  {\em /* e.g. myPlanetsEnemyRatio */}
  \FOR{Each p in PlayerPlanets}
    \STATE calculateLocalPlanets(p) {\em /*e.g. NearestNeutralPlanet to p*}
    \STATE calculateLocalRatios(p)  {\em /* e.g actualMyShipsRatio */}
    \STATE executeTree(p,tree)  {\em /*Send a percentage of ships to destination*/}
   \ENDFOR
\ENDWHILE

\end{algorithmic}
\caption{Pseudocode of the proposed agent. The same tree is used during all the agent's execution}
\label{alg:turn}
\end{algorithm}


\subsection{Selection policy}
The algorithm presented in \cite{GarciaGP14} % ¿El algoritmo de la anterior
                              % generación? ¿Eso qué es? - JJ FERGU: cierto, arreglado
is combined with the implicit fitness evaluation for selection and
replacement. This implicit evaluation of fitness is performed via a
match of the individuals in a real-scenario (called {\em joust} to distinguish from the classical tournament in EAs). 
%which explains the title we have given to this paper.  % Esta frase casi que la quitaría  [pedro] FERGU: ala, fuera
Selection of the mating parents is performed in a battle: the winner is selected to mate % en lugar de crossover, pon "mate" [pedro] FERGU:  fale
and the loser will be removed from the population (as it will be explained later).

%Two different types of battles will be compared: {\em 1vs1} and {\em All vs All of 4 bots}, 
% Antonio - esto lo ponía yo mal también, no son combates 4 contra 4, sino 1vs3. ;D FERGU: arreglao.
%as they are the only possible match types in Planet Wars. 
%In both configurations the winner will be selected to mating and the loser (i.e. the first to be eliminated) will be removed from the population. Therefore, in the case of All vs All, the individuals ending in 2nd and 3rd position neither are selected for mating nor removed.

This implicit selection and replacement have been chosen because...  % ¡estoy en ascuas!  ¡¿por qué?!  ;)  [pedro]

\subsection{Replacement policy}
\label{subsec:replacement}

%The bots that have lost, and the offspring generated in the previously explained battles are removed/added to the population following one of the two next policies: steady-state or generational mechanism.

%The replacement policy chosen is the steady-state, an implementation based in the classical Steady-State EA approach \cite{Genitor_whitley}. 
A steady-state algorithm has been chosen as replacement policy. In this case, the classical Steady-State EA approach \cite{Genitor_whitley} has been implemented.   % reescribo la frase; la dejo comentada a continuación por si hay que restaurarla  ;)  [pedro]
%The replacement policy chosen is the steady-state, an implementation based in the classical Steady-State EA approach \cite{Genitor_whitley}. 
In this algorithm, the majority of the population remains the same in the following generation, and just a small subset of individuals are substituted (usually just the worst).
This method aimed to increase the exploitation factor in the EA, since just the worst individuals in the population are substituted by fitter ones. 
This is interesting for this problem because......   % ¡otra vez en ascuas!   :D

% And this is interesting for this problem because... - JJ

Thus, the proposed approach follows this idea and just performs two battles (or jousts) per generation. The contenders are randomly selected from all the population. Each two battles, both winners are the parents for the two new individuals (offspring). These two new individuals are inserted in the population after being created, substituting the bots that lose the battle against its parents.


This approach presents a higher stochastic component than the
original, due to the lack of a fitness value which can value every
individual with a simple number. The random selection of all the
individuals increases the diversity in the algorithm run, which, in
turn, could be a key factor in this problem resolution,

\textcolor{red}{AS WILL BE proved IN THE EXPERIMENTS}. 



%In the second policy to be tested, the {\em generational} replacement\cite{GAs_Goldberg89}, half of the population is substituted every generation, having a big diversification and thus exploration factor.
%As in the previous configuration, the individuals are paired randomly,
%conducting battles. The winners (half of the population)
%will be the parents of the next offspring. % ¿Y por qué se elimina el
                                % ruido? ¿No puede ser que gane uno
                                % por casualidad? - JJ
%Then, after applying the crossover and mutation, the rest of the population is generated (two descendants per couple). The parents are grouped with all the offspring and the bots which have lost are deleted at the same time, increasing this way the diversity/exploration of the algorithm.

Algorithm \ref{alg:completealgorithm} shows the combination of the Genetic Programming generation, in conjunction with the implicit evaluation, selection and replacement mechanisms.


\begin{algorithm}[htb]
\begin{algorithmic}

\STATE population $\leftarrow$ initializePopulation()
\WHILE{stop criterion not found}
 
  \STATE offspring,losers,selected $\leftarrow$ \{\}
%  \IF{type=steadystate}
%    \STATE N $\leftarrow$ 1
%  \ELSIF{}
%    \STATE N $\leftarrow$ population.size/4
%  \ENDIF

%  \FOR{1 to N}
    \STATE {\em /* 2 contenders */}%in 1vs1 and 4 contenders in All vs All*/}
% Antonio - lo del 4vs4 FERGU: comentado
    \STATE contenders $\leftarrow$ selectContenders(population-selected)
    \STATE {\em /*The contenders fight and the winner and loser are obtained */} %(in case of All vs All, the first to be defeated is the loser)*/}
    \STATE winner1,loser1 $\leftarrow$ battle(contenders)
    \STATE {\em /* Previously selected bots not participate again in the tournament*/}
    \STATE selected $\leftarrow$ selected + winner1 + loser1 
    \STATE contenders $\leftarrow$ selectContenders(population-selected)
    \STATE winner2,loser2 $\leftarrow$ battle(contenders)
% Antonio - poner más comentarios. Yo de hecho los suelo poner antes de cada instrucción (y más si son tan improtantes como estas)
% Antonio - Si no se quiere hacer dos pseudocódigos para 1vs1 y para 1vs3, se pueden poner cosas comentadas sobre quienes son winner1 y loser1 en cada caso, poner algunas líneas que sólo se ejecutarían en un caso o en otro, etc FERGU: puestos comentarios
    \STATE selected $\leftarrow$ selected + winner2 + loser2 
    \STATE losers $\leftarrow$ losers + loser1 + loser2
    \STATE son1,son2 $\leftarrow$ crossover(parent1,parent2);
    \STATE son1,son2 $\leftarrow$ mutation(son1,son2)
    \STATE offspring $\leftarrow$ offspring + son1 + son2
%   \ENDFOR
   \STATE population $\leftarrow$ population - losers
   \STATE population $\leftarrow$ population + offspring

\ENDWHILE

\end{algorithmic}
\caption{Pseudocode of the proposed algorithm.}
\label{alg:completealgorithm}
\end{algorithm}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   EXPERIMENTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Experiments and Results}
\label{sec:experiments}

Several experiments have been conducted in order to study different issues of the proposed approaches, but having in mind that the main objective is the generation and improvements of bots using a co-evolutionary algorithm. The set of parameters considered in the Co-GP algorithm is shown in Table \ref{tab:parameters}.
These parameters have been  previously used in \cite{GarciaGP14}, obtaining competitive bots. To do a fair comparison with the results obtained in that work the termination criteria has been also set to 8000 battles (therefore, 4000 generations). 30 executions of the Co-GA have been performed in order to obtain accurate statistics.    % completo la frase  [pedro]

%For each one of the presented approaches (replacement and selection policies), 

\begin{table}
\begin{center}
\begin{tabular}{|c|p{2.5cm}|}
\hline
{\em Parameter Name} & {\em Value} \\\hline \hline
Population size & 32 \\\hline
Crossover type & Sub-tree crossover \\ \hline
Crossover rate & 0.5\\ \hline
Maximum number of turns per battle & 1000 \\\hline
Mutation  & 1-node mutation\\ \hline
Mutation step-size & 0.25 \\ \hline
Selection & 2-tournament \\ \hline
Replacement & Steady-state\\ \hline
Stop criterion & 4000 iterations \\ \hline
Maximum Tree Depth & 7  \\ \hline 

Runs per configuration & 30 \\ \hline 
Maps used in each evaluation & 1 random chosen among maps \#76 \#69 \#7 \#11 \#26\\ \hline
\end{tabular}
\caption{Parameter set used in the experiments.}     % [pedro]
\label{tab:parameters}
\end{center}
\end{table}

\subsection{Analysis of the executions}   % ¿cambiar  executions  por  runs ?   % [pedro]
\label{subsec:analysisexecutions}

It is difficult to show the convergence of the populations using a implicit fitness evaluation, as the evaluated individuals (and therefore, the fitness of the average population) do not count with a numeric value to be plotted in time. In this paper, we propose a scoring method that takes into account the number of victories, turns to win and turns to be defeated % needed to be defeated? por qué? - Jj FERGU: explicado debajo
 against a previously known bot. After the execution of our algorithm all the generated individuals during all the runs have been confronted versus GPbot \cite{GarciaGP14} and scored using the next formula:
\begin{equation}
Score_{i}=\alpha+\beta+\gamma
\end{equation}

where

\begin{equation}
\alpha  = v, \alpha \in\left[0,N\right]
\end{equation}

\begin{equation}
\begin{split}
\beta =N\times\frac{t_{win}+\frac{1}{N\times t_{MAX}+1}}{\frac{t_{win}}{v+1}+1},\\ 
\beta \in\left[0,N\right], \\
t_{win} \in\left[0,N\times t_{MAX}\right]
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\gamma  =\frac{t_{defeated}}{N \times t_{MAX}+1}, \\ 
\gamma \in\left[0,1\right], \\
t_{defeated} \in\left[0,N\times t_{MAX}\right]
\end{split}
\end{equation}

The terms used are: the number of battles ($N$) to test, the number of victories of the individual against GPbot ($v$), the total number of turns used to win GPbot ($t_{win}$), the total number of turns when the individual has been defeated by GPbot ($t_{defeated}$) and the maximum number of turns a battle lasts ($t_{MAX}$). This score tries to favor the victories against the turns to win and turns to be defeated, giving different ratios to each section. Therefore $\alpha$ has the highest ratio. The term $\beta$ add extra score taking into account the number of turns when the individual wins (lower numbers to win implies better bots), following a exponential curve.
Finally, the $\gamma$ term adds score from the turns to be beaten (higher number is better, as it is difficult to be beaten). The 1 in all denominators is used to avoid divide by 0.

Each individual has been tested 3 times in 10 different maps (the 5 used during evolution and other new 5 ones from the Google set), therefore $N=15$, and a the limit of turns is the default of the competition ($t_{MAX}=1000$). As previously said, this score has the same shortcomings that we are trying to avoid in this paper: it requires parametrization and an existent opponent. However, we will use this score as a way to measure the performance of our fitness-less approach.

Figure \ref{figura:Score_VS_GPBot} shows the boxplots of the score of the current population (of all 30 original runs) in different stages of the evolution in different maps (the used ones with the evaluation and the new set). As can be seen, this score is being increased during the evolution, therefore there exist some kind of improvement during the execution of the Co-GP algorithm. 



\begin{figure}[htb]
\tiny
\begin{center}
\includegraphics[clip=true,width=9cm]{./imags/score_vs_gpbot.eps}
\end{center}
%Es muy pequeño. ¿No se puede poner a dos columnas? - JJ FERGU: agrandado
\caption{Score confronting all SurvivalBots obtained during all the runs versus GPbot.}
%¿Qué diablos es "not training" Será "not included in training" - ¿Qué
%diablos es score vs. gpbot? ¿La puntuación se enfrenta con GPBOT?
%Será score SurvivalBot vs. gpbot, ¿no? - JJ FERGU: sí, es que era un copypaste de mi mail. Arreglado caption, tamaño y quitado título
\label{figura:Score_VS_GPBot}
\end{figure}





%\begin{figure}[htb]
%\tiny
%\begin{center}
%  \epsfig{file=./imags/score_vs_gpbot_all_maps,width=6cm}
%\end{center}
%\caption{Score vs GPBot (all maps). Casi igual que las anteriores pero en boxplots SIN outliers (por eso la del best individual parece que el máximo está en 5, con lo cual no debería estar en 23 en la que son líneas). Estas dos figuras puedes quitarlas si quieres.}
%\label{figura:Score_VS_GPBot_AllMaps}
%\end{figure}

Figure \ref{figura:convergence} shows the average score of all individuals, and the average score of all best and worst individuals, obtained during the evolution of the fitness-less Co-GA.  This figure shows how there exist an increasing performance in the best individuals during the runs and...

\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/convergence,width=9cm}
\end{center}
\caption{Average score of the best and worst individuals and average of the population from all executions of our fitness-less Co-GA, versus GPbot.}
\label{figura:convergence}
\end{figure}

Figure \ref{figura:Victories_VS_GPBot_AllMaps} shows the percentage of individuals (normalized  from 0 to 1) that wins certain number of times (from 0 to 30) against GPBot in the initial and the final populations. As can be seen there exist differences in the number of victories...

\begin{figure}[htb]
\tiny
\begin{center}
  \epsfig{file=./imags/victories_vs_GPBot_allmaps,width=9cm}
\end{center}
\caption{Victories of all the populations (initial and final) vs GPBot (all maps).}
% Este histograma deberíais redudirlo a 10 bins o así. Hay reglas para
% el número de bins según lo que quieras representar en un histograma,
% son muchas - JJ
\label{figura:Victories_VS_GPBot_AllMaps}
\end{figure}


Moreover, other values, such as the age, tree depth and number of nodes can help to understand how the evolution is being performed. Figure \ref{} shows the average age of the agents during the run. As can be seen, the maximum age is TERMINAR










% --------------------------------------------------------------

\subsection{Analysis of the generated bots}
\label{subsec:analysisbots}
This subsection shows the results of the obtained SurvivalBots, analysing the distribution of actions and decisions at the beginning and at the end of the evolution. Figure \ref{figura:tarta_actions} shows the final distribution of the different types of actions (attack, reinforce and do nothing). As it can be seen, the {\em do nothing} action has been increased at the end of the evolution, as it seems logical that not all planets should attack in every turn (they may be waiting to have a large fleet to send). Of course, and because a player wins conquering the enemy planets, it is logical that the attack action is more effective than reinforce.

\begin{figure}[htb]
\tiny
\begin{center}


    \includegraphics[trim=1cm 7cm 1cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_initial_action.eps}
    \includegraphics[trim=1cm 7cm 1cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_final_action.eps}
    %\includegraphics{file=./imags/distribution_final_reinforce,width=3cm,angle=-90}


\end{center}
\caption{Distribution of different types of actions (attack, reinforce and do nothing) of the generated bots.}
\label{figura:tarta_actions}
\end{figure}

Figure \ref{tarta_attacking_action} shows the strategy when a planet is attacking to other. It is clear that the generated bots have a predilection for nearest planets and the ones easiest to conquer. This make sense because ships flying to long destinations are not being used, so, using a {\em rush} strategy makes...
\begin{figure}[htb]
\tiny
\begin{center}

    \includegraphics[trim=1cm 7cm 0cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_initial_attack.eps}
    \includegraphics[trim=1cm 7cm 0cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_final_attack.eps}
    %\includegraphics{file=./imags/distribution_final_reinforce,width=3cm,angle=-90}

\end{center}
\caption{Condition of target planet when attacking: Nearest, Quickiest, Weakest, Walthest, Base or Beneficious.}
\label{figura:tarta_attacking}
\end{figure}

Also, the actions are more focused in attacking planets owned by the Enemy (as can be seen in Figure \ref{figura:tarta_attacking_who}). This can be explained because the bot is not only conquering planets, but also destroying enemy ships that will not be used against him.
Figure \ref{tarta_attacking_who}
\begin{figure}[htb]
\tiny
\begin{center}

    \includegraphics[trim=1cm 7cm 1cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_initial_target.eps}
    \includegraphics[trim=1cm 7cm 1cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_final_target.eps}
    %\includegraphics{file=./imags/distribution_final_reinforce,width=3cm,angle=-90}

\end{center}
\caption{Owners of target planets when attacking: Enemy, Neutral, NotMy.}
\label{figura:tarta_attacking_who}
\end{figure}


Figure \ref{tarta_reinforcing} shows the target when a planet is reinforcing the player's planet. It seems a good rule is to center in reinforce [LOQUESEA] planets.
\begin{figure}[htb]
\tiny
\begin{center}

    \includegraphics[trim=1cm 7cm 1cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_initial_reinforce.eps}
    \includegraphics[trim=1cm 7cm 1cm 7cm, clip=true,width=5cm,angle=-90]{./imags/distribution_final_reinforce.eps}
    %\includegraphics{file=./imags/distribution_final_reinforce,width=3cm,angle=-90}

\end{center}
\caption{Destination of planets when reinforcing: Weakest, Wealthest, Base.}
\label{figura:tarta_reinforcing}
\end{figure}

Figure \ref{figura:tarta_decissions} shows the final proportion of decisions. Surprisingly the most important variable to take into account is the ratio between flying and landed ships. This can be explained because it makes sense to find an equilibrium between the two states of ships, as all ships flying or waiting may be counterproductive. The {\em random} decision also has importance, as it is the one who gives weights to the branches of the tree.
\begin{figure}[htb]
\tiny
\begin{center}


    \includegraphics[trim=1cm 5.5cm 0cm 5.5cm, clip=true,width=4.5cm,angle=-90]{./imags/distribution_initial_condition.eps}
    \includegraphics[trim=1cm 5.5cm 0cm 5.5cm, clip=true,width=4.5cm,angle=-90]{./imags/distribution_final_condition.eps}



\end{center}
\caption{Percentage of decisions.}
\label{figura:tarta_decissions}
\end{figure}

\subsection{Analysis of the generated bots vs. others}
\label{subsec:Analysisothers}

All the SurvivalBots obtained at the end of the evaluations have been tested to other bots available in the literature. First, the {\em best} individual of each run has been obtained confronting all versus all members of the last generation. The individuals who has won the most is considered the best of the run. This method has been chosen to avoid the usage of the score function (and therefore, the shortcomings we are trying to avoid). Thus, we have confronted the 30 bots obtained in each configuration again with several bots available in the literature, in the 100 maps provided by Google. This experiment has been used to validate if the obtained SurvivalBots can be competitive in terms of quality in maps not used for evaluation, and against bots not used for evaluation (as other works). Table \ref{tab:literaturebots} shows the bots used. Note that only shows the victories, not the draws.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\em Bot Name} & {\& Reference} & {\em Number of simulations} & {\em Turns} \\\hline \hline
SurvivaBot & (this) & 8000 & 1000 \\ \hline
GeneBot & \cite{Genebot_JCST} & 32000 & 1000 \\ \hline
ExpGeneBot & \cite{Genebot_CIG2012} & 32000 & 1000 \\ \hline
NacoBot & \cite{NogueiraCoevolutionary14} & 180000 & 500 \\ \hline
GPBot & \cite{GarciaGP14} & 8000 & 1000 \\ \hline


\end{tabular}
\caption{Bots available in the literature used for measuring the quality of the SurvivalBots.}     % [pedro]
\label{tab:literaturebots}
\end{center}
\end{table}



Figure \ref{figure:boxplot_mejores_contra_clasicos} shows the boxplots 

\begin{figure}[htb]
\tiny
\begin{center}
    \includegraphics[width=9cm]{./imags/boxplot_mejores_contra_clasicos.eps}
   
\end{center}
\caption{Boxplots confronting best SurvivalBots against existent bots in the literature.}
\label{figura:tarta_decissions}
\end{figure}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Conclusions and Future Work}
\label{sec:conclusions}

This paper presents an implementation of a quite simple approach: to omit the fitness-based selection mechanism in an EA, simulating the evolution in a more natural way: performing real battles of the individuals. This has been applied over the improvement of the behavioural parameters and rules of the bot's AI in the RTS game Planet Wars. This approach has two main benefits: it leaves out existent bots for the evaluation, and the parametrization of the fitness function.

The classic tournament selection mechanism has been modelled as a battle in the game (called joust) to ...

% en las conclusiones hay que generalizar y el tema es el uso de
% dinámicas de juego en metaheurísticas y cómo se podría
% generalizar. Lo que tenemos al final es un orden parcial y ruidoso
% que se podría usar, en general, en todo tipo de algoritmos
% evolutivos donde haya ese problema - JJ



%The results obtained in this study are very promising, but they inherit a flaw from previous works, which is the low flexibility level due to the predefined set of rules/states that the bots follow. This means that almost every bot will eventually behave well, and the diversity in the search loses its relevance.
%The consideration of a more flexible approach for defining the behavioural engine of the bots, such as a Genetic Programming one [REF GP Genebot], could yield more interesting results and conclusions about the value of the methods proposed in this work.

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
% Antonio - actualizar esto. He quitado EVORQ y MUSES, pero ANYSELF terminó y CANUBE también, ¿no?
This paper has been funded in part by Spanish National project TIN2011-28627-C04-02 (ANYSELF), and projects CEI2013-P-14 (CANUBE) and GENIL PYR-2014-17, both awarded by the CEI-BioTIC UGR.

\bibliographystyle{IEEEtran}
\bibliography{genebot}

\end{document}
