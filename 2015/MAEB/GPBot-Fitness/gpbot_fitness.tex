\documentclass[twocolumn]{maeb2015}
\usepackage[spanish]{babel}
\usepackage[latin1]{inputenc}
\usepackage{graphics}
\usepackage[dvips]{epsfig}
\usepackage{url}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{rotating}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{color}
\usepackage{alltt}
\usepackage{verbatim}
\usepackage{moreverb} 

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\begin{document}

\title{Diseño de Bots Competitivos para un Juego de Estrategia en Tiempo Real usando Programación Genética.\\ Análisis de Funciones de Fitness} %!PN

\author{A. Fernández-Ares, P. García-Sánchez, A.M. Mora, P.A. Castillo \thanks{Departamento de Arquitectura y Tecnología de Computadores. CITIC, ETSIIT,Universidad de Granada
E-mail: \{antares,amorag,pgarcia,pedro\}@geneura.ugr.es}}

\maketitle


\begin{abstract}
Diseñar el motor de Inteligencia Artificial (IA) de un agente autónomo (bot) en un juego es una tarea compleja, hecha normalmente por un jugador experto, el cual plasma en un modelo su propio conocimiento y comportamiento en el juego en base a su experiencia.
Este artículo presenta un algoritmo para automatizar (o facilitar en gran medida) esa tarea, mediante el uso de Programación Genética (PG).
Este método se aplica aquí para diseñar árboles de decisión que modelan la IA de un bot para jugar combates 1 contra 1 dentro del juego de estrategia Planet Wars.
De forma que con este algoritmo es posible crear un sistema basado en reglas (que modela de dicho árbol) en el que se definen decisiones o condiciones y acciones a realizar. Esta tarea se hará automáticamente y de forma muy distinta a cómo lo haría un diseñador humano, que tendría que implementarlo 'a mano' normalmente.
Dichas reglas se optimizarán mediante el algoritmo, en base al comportamiento y rendimiento obtenido por el bot en combates de evaluación. De modo que el conjunto de reglas obtenidas finalmente diferirá sensiblemente de las que pudiese crear el humano, pudiendo obtenerse bot mucho más competitivos.
Debemos considerar la naturaleza ruidosa del entorno en el que se ejecutará el algoritmo, es decir, los videojuegos, en los que un mismo individuo puede obtener muy buenos o muy malos resultados dependiendo de las acciones del rival y del propio juego, si éstos cuentan con algún componente estocástico.
Por este motivo se han implementado y probado tres funciones de evaluación (fitness) diferentes. Dos de ellas tienden a minimizar dicho factor considerando información adicional (y dinámica) sobre los combates de evaluación, en lugar de únicamente fijarse en el resultado final del ganador, como sí hace la tercera función.
Para comprobar su valía los mejores bots obtenidos con cada función se han probado contra un bot creado anteriormente en base a conocimiento experto y mejorado mediante un algoritmo genético.
Los experimentos muestran que las tres funciones generan bots más efectivos que el citado, siendo la función basada en área la más exitosa.
\end{abstract}

\begin{keywords}
AG, Algoritmo Genético, PG, Programación Genética, Bot, NPC, RTS, Inteligencia Computacional, Inteligencia Artificial, Reglas, Máquina de Estados Finitos
\end{keywords}

%-------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------------------------------------
\section{Introduction}
\noindent 

Real-Time Strategy (RTS) games are a sub-genre of strategy-based videogames in which the contenders struggle to control a set of resources, units and structures that are distributed in a playing arena. A proper control and a sound strategy and tactics for handling these units is essential for winning the game, which happens after the game objective has been fulfilled, normally eliminating all enemy units, but sometimes also when certain points or game objectives have been reached.

Los juetos de estrategia en tiempo real (Real-Time Strategy, RTS) son un subgénero de los videojuegos de estrategia en los que los participantes se enfrentan por controlar un conjunto de recursos, unidades y estructuras distribuidas en un escenario de juego. Es esencial contar con un buen control y distintas técnicas y estrategias para ganar el juego, que normalmente consiste en eliminar todas las unidades enemigas, o ganar cierto número de puntos.

Como su nombre indica, su principal característica es su naturaleza en tiempo real, es decir, el jugador no tiene por qué esperar el resultado de los movimientos del otro jugador, al contrario que en los juegos basados en turnos. Algunos ejemplos bien conocidos de este tipo de juegos son Command and Conquer\texttrademark, Starcraft\texttrademark, Warcraft\texttrademark~ y Age of Empires\texttrademark~.

En estos juegos normalmente se consideran dos niveles de Inteligencia Artificial (IA) \cite{ahlquist_game_ai08}: el primero, realizado por un Personaje No Jugador (PNJ), también llamado \textit{bot}, toma decisiones relativas al conjunto completo de unidades (trabajadores, soldados, máquinas, vehículos o incluso edificios); mientras que el segundo nivel implementa el comportamiento de cada una de esas pequeñas unidades. Estos dos niveles de acción, que pueden considerarse ({\em estratégicos} y {\em tácticos}), hacen que sea inherentemente dificil de diseñar por un humano, pero además, esta dificultad se aumenta por su naturaleza en tiempo real (normalmente debida por restringir el tiempo para tomar una decisión) y también por el enorme espacio de búsqueda.

Por estas razones, en este trabajo se presenta un método de Programación Genética (PG) como método automático para crear el motor de IA de agentes autónomos en un RTS. El objetivo de la PG es crear funciones o programas para resolver determinados problemas, siendo la representación de los individuos en forma de árbol, formado por operadores (o {\em primitivas}) y variables ({\em terminales}).

La meta de usar PG en este ámbito es la creación de motores de comportamiento basados en reglas siguiendo un proceso heurístico, algorítmico y automático. Así, en lugar de implementarlos desde cero por un humano (experto o no), este método definirá un conjunto de reglas que podrían ser más complejas (o incluso, más simples) que aquellas definidas por un humano. Durante la ejecución de este algoritmo se evaluará cada posible conjunto de reglas, asignando a cada conjunto un valor de acuerdo al rendimiento del bot (durante las batallas). 

Para implementar y testear esta proposición hemos considerado el juego {\em Planet Wars}, un RTS presenado en la competición Google AI Challenge 2010\footnote{\url{http://planetwars.aichallenge.org/}}. Este juego ha sido utilizado por varios autores para el estudio de técnicas de inteligencia computacional en juegos RTS \cite{Genebot_CEC11,ExpGenebot_CIG2012,Lara_PCG_PlanetWars14}, ya que es una simplicación (sólo un tipo de recurso y de unidad) de los elementos que forman un juego RTS comercial. Los jugadores controlan naves espaciales (o simplemente naves), y el objetivo es conquistar todo el conjunto de planetas de un mapa, contra un enemigo que intenta lo mismo. Los planetas pueden producir nuevas naves y las naves pueden destruirse estrellando las del otro jugador en el planeta.

En este paper, se presentan 3 diferentes funciones fitness: la primera es una variación del fitness basado en victorias presentado en \cite{Genebot_CEC11}, la cual evalua todos los individuos en la población jugando 5 diferentes batallas contra un bot que hace de \textit{sparring}. El objetivo de estas repeticiones es evitar el factor ruidoso presente estos entornos dinámicos \cite{Mora_noisy_jcst,wilcoxon:ga}. Debido a esto, el valor fitness de un individuo podría variar dramáticamente entre distintas batallas, ya que depende de las actiones pseudo-estocásticas del oponente y de sus propias decisiones no deterministas.

Las otras dos funciones fitness presentadas intentan reducir la influencia del ruido en la evolución, pero usando datos adicionales durante la ejecución del bot: el número de naves generadas por cada bot en lugar del número de turnos. La primera realiza una regresión linear (función \textit{slope}), basada en el porcentaje de naves con respecto al total, y la segunda la integral (\textit{función area}) de la función que representa esos números. Todas las funciones usadas consideran los resultados finales de cada individuo (bot) después de las cinco batallas (en media).

El trabajo se centra a continuación en probar el valor de los sistemas basados reglas evolucionados, realizando varios experimentos usando las funciones fitness presentadas para enfrentarse a un bot competitivo disponible en la literatura \cite{Genebot_CEC11}.

%The work is then focused on proving the value of these evolved rule-based control systems for the agents. To this end, several experiments have been conducted, considering the aforementioned fitness functions, and an evolutionary bot as rival. This bot, called GeneBot, was presented in a previous work \cite{Genebot_CEC11}. 




%-----------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% BACKGROUND %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------
\section{Background and problem description}

\subsection{Genetic Programming}

Genetic Programming (GP) \cite{GP_Koza92} is a kind of Evolutionary Algorithm (EA), that is, a probabilistic search and optimization algorithms gleaned from the model of darwinistic evolution, based on the idea that in nature structures undergo adaptation. EAs work on a population of possible solutions (individuals) for the target problem and use a selection method that favours better solutions and a set of operators that act upon the selected solutions. %coñe, si has dicho que es un tipo de, cómo no va a compartirlos. Lo cambio.  -JJ
% AntonioDEF - ok
However, GP is a structural optimisation technique where the individuals are represented as hierarchical structures (typically tree) and the size and shape of the solutions are not defined a priori as in other methods from the field of evolutionary computation, but they evolve along the generations. So, the main difference with respect to GAs is the individual representation and the genetic operators to apply, which are mainly focused on the management (and improvement) of this kind of structure.
The flow of a GP algorithm is the same as any other EA: a population is created at random, each individual in the population is evaluated using a fitness function, the individuals that performed better in the evaluation process have a higher probability of being selected as parents for the new population than the rest and a new population is created once the individuals are subjected to the genetic operators of crossover and mutation with a certain probability. The loop is run until a predefined termination criterion is met.

% ----------------------------------------------------------------------

\subsection{Planet Wars Game}
%FERGU2: quitar subsecciones si falta espacio. O vamos, quitarlas aunque haya.
In this paper we work with a %simplified
version of the game Galcon, aimed at performing bot's fights which was used as base for the Google AI Challenge 2010 (GAIC)\footnote{http://ai-contest.com}.

 \begin{figure}[ht]
 \begin{center}
   \epsfig{file=./imags/naves.eps,width=7cm}
 \end{center}
 \caption{}Simulated screenshot of an early stage of a run in Planet Wars. White planets belong to the player (blue colour in the game), dark grey belong to the opponent (red in the game), and light grey planets belong to no player. The triangles are fleets, and the numbers (in planets and triangles) represent the ships. The planet size means growth rate of the amount of ships in it (the bigger, the higher).
 \label{figura:PlanetWars1}
 \end{figure}

A Planet Wars match takes place on a map (see Fig. \ref{figura:PlanetWars1}) %FERGU: por qué estaba comentada la referencia a la figura?
that contains several planets (neutral, enemies or owned), each one of them with a number assigned to it that represents the quantity of ships that the planet is currently hosting. 

The aim of the game is to defeat all the ships in the opponent's planets. Although Planet Wars is a RTS game, this implementation has transformed it into a turn-based game, in which each player has a maximum number of turns to accomplish the objective. At the end of the match, the winner is the player that remains alive, or that which owns more ships if more than one survives. 

There are two strong constraints which determine the possible methods to apply to design a bot: a simulated turn takes \textit{just one second}, 
%FERGU: falso, es el máximo tiempo para calcular una acción
% Antonio - y qué es un turno entonces sino el tiempo para decidir una acción (o conjunto de acciones)?
and the bot is \textit{not allowed to store any kind of information} about its former actions, about the opponent's actions or about the state of the game (i.e., the game's map). 
%FERGU: i.e. no quiere decir "es decir", quiere decir "por ejemplo". Poner "that is,"
% Antonio - creo que te lias, i.e. significa "id est", que significa "esto es/es decir". Creo que te confundes con e.g. que sí significa "for example".
%FERGU2: es verdad xD

Therefore, the aim in this paper is to study the improvement of a bot according to the state of the map in each simulated turn (input), returning a set of actions to perform in order to fight the enemy, conquering its resources, and, ultimately, wining the game. 
%In the original game, only two bots are faced but in this paper it is studied what happen when we simulate 4 on 4 battles, i.e., when 4 bots are fighting in the same map. 
%FERGU: no se va a hacer 4 vs 4: quitadlo
% Antonio - Cierto es. se me había pasado. :D

%-----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% STATE OF THE ART %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------------
\section{State of the art}
\label{sec:soa}

RTS games have been used extensively in the computational intelligence area (see \cite{Lara2013review} for a survey). 

Among other techniques, Evolutionary Algorithms have been widely used as a  Computational Intelligence method in RTS games \cite{Lara2013review}. For example, for parameter optimization \cite{Esparcia10FPS}, learning \cite{Kenneth2005neuroevolution} or content generation \cite{Mahlmann2012MapGeneration}. 

One of these types, Genetic Programming, has been proved as a good tool for developing strategies in games, achieving results comparable to human, or human-based competitors \cite{Sipper2007gameplaying}. They also have obtained higher ranking than solvers produced by other techniques or even beating high-ranking humans \cite{Elyasaf2012FreeCell}. GP has also been used in different kind of games, such as board-games \cite{Benbassat2012Reversi}, or (in principle) simpler games such as Ms. Pac-Man \cite{Brandstetter2012PacMan} and Spoof \cite{Wittkamp2007spoof} and even in modern video-games such as First Person Shothers (FPS) (for example, Unreal\texttrademark~ \cite{Esparcia2013GPunreal}). With respect to RTS games, there are just a few applications on pathfinding \cite{pathfinding_GP_RTS} and definition of tactics in an abstract tactical game \cite{KeaveneyO09_GP_RTS}.
In this paper, the aim is to apply GP inside a modern RTS, in order to define the whole behavioural engine for an autonomous player (non-player character), trying to improve a rule-based system previously defined by a human expert.
% AntonioDEF - he puesto esto nuevo, que creo que queda bien. No decimos que no se haya hecho, pero sí que lo nuestro es más mejor (o al menos diferente)
% ... y eso nos hace pensar que puede dar buenos resultados en este caso, porque nunca se ha aplicado a este tipo de problema.
% ¿Cuál es el selling poing? ¿GP aplicado a RTS? ¿GP aplicado a Planet Wars? - JJ 
% Antonio - escrito, aunque no es una frase lapidaria porque no estoy seguro de que no se haya usado antes en RTSs. PEro el objetivo/selling point se explica en la intro ahora.
 
Planet Wars, the game used in this work, has also been used in other researches as an experimental framework for agent testing. 
%We have deeply worked in this environment \cite{Genebot-IWANN2011,Genebot_CEC11,genebot-evo12,ExpGenebot_CIG2012,Co-Genebot_EVO2014},
%FERGU: no usar el "hemos trabajado", sino "se ha trabajado", que parece que somos los únicos que lo hacen (aunque sea verdad)
% Antonio - pues yo lo veo peor así porque das a entender que se ha usado en varios trabajos (de gente diversa) y sólo citas los nuestros... XD
We have considered this game in some previous studies \cite{Genebot-IWANN2011,Genebot_CEC11,genebot-evo12,ExpGenebot_CIG2012,Co-Genebot_EVO2014}, mainly applying Genetic Algorithms for evolving (the parameters of) a behavioural engine previously defined by a human expert from scratch. Those works have respectively defined a first approach, compared different implementations, analysed the noise influence, defined expert bots, and implemented co-evolutionary approaches.

%For example, in
%\cite{Mora2012Genebot} the authors programmed the behaviour of a {\em bot} (a computer-controlled player) with a decision tree of 3 levels. Then, the values of these rules were optimized using a genetic algorithm to tune the strategy rates and percentages.  
%  Results showed a good performance confronting with other bots
%  provided by the Google AI Challenge. %In our next work
%  In \cite{FernandezAres2012adaptive} the authors improved this agent optimizing it in different types of maps and selecting the set of optimized
%  parameters depending on the map where the game was taking place,
%  using a tree of 5 levels. These results outperformed the previous
%  version of the bot with 87\% of victories. 

The present work means a new step in this research line, which tries to avoid the strict limitations that the initial bot had, i.e. since it was defined by a human expert, it had a fixed structure (a Finite State Machine) which just offers a few degrees of improvement, namely a set of eight parameters.
The use of GP here will provide us with a new tool for completely redefine the bot's AI engine, which could also get better results than previous bots.
Thus GP has been applied to create the Decision Tree that the bot will use to make decisions during the game.
In order to prove the method value, the resulting agents will be compared with a competitive bot previously presented: GeneBot \cite{Genebot_CEC11}, our initial bot improved by means of Genetic Algorithms. This bot also proved (in that work) to be better than a human-defined bot (AresBot).
% AntonioDEF - He completado esto un poco para justificar mejor el trabajo. Comento además que Genebot era mejor que uno definido por un humano, Aresbot.

%, and Exp-Genebot \cite{ExpGenebot_CIG2012} an enhanced agent which considers different sets of parameters depending on the type of battle map, which is previously analysed by the bot.
%*** Quitamos Exp-Genebot??? ***
%FERGU2: sí, quitado

%-------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% GP BOT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------------------------------
\section{GPBot}
\label{sec:agent}

The Genetic Programming-based bot or {\em GPBot} evolves a set of rules which, in turn, models a Decision Tree. 
%FERGU: quitado lo del turno. Cada turno no modela un árbol nuevo.
% Antonio - "in turn" significa "a su vez". ;)
During the evolution, every individual in the population (a tree) must be evaluated. To do so, the tree is set as the behavioural engine of an agent, which is then placed in a map against a rival in a Planet Wars match. Depending on the obtained results, the agent (i.e. the individual) gets a fitness value, that will be considered in the evolutionary process as a measure of its validity. 
%FERGU: i.e. -> that is
% Antonio - no. XD
 
Thus, during the match the tree will be used (by the bot) in order to select the best strategy at every moment, i.e. for every planet a target will be selected along with the number of ships to send from one the other.

\noindent The used Decision Trees are binary trees of expressions composed by two different \textit{types of nodes}:

\begin{itemize}
\item {\em Decision}: a logical expression formed by a variable, a less than operator ($<$), and a number between 0 and 1. It is the equivalent to a ``primitive'' in the field of GP.
\item {\em Action}: a leave of the tree (therefore, a ``terminal''). Each decision is the name of the method to call from the planet that executes the tree. This method indicates to which planet send a percentage of available ships (from 0 to 1). 
\end{itemize}

\noindent The decisions are based in the values of different \textit{variables} which are computed considering some other variables in the game. They are defined by a human expert, and are:
% estas variables, ¿de dónde salen? ¿Son todas las variables del juego? ¿Unas pocas? - JJ 
% Antonio - Las define un experto. Escrito. ;)

\begin{itemize}
\item {\em myShipsEnemyRatio}: Ratio between the player's ships and enemy's ships.
\item {\em myShipsLandedFlyingRatio}: Ratio between the player's landed and flying ships.
\item {\em myPlanetsEnemyRatio}: Ratio between the number of player's planets and the enemy's ones.
\item {\em myPlanetsTotalRatio}: Ratio between the number of player's planet and total planets (neutrals and enemy included).
\item {\em actualMyShipsRatio}: Ratio between the number of ships in the specific planet that evaluates the tree and player's total ships.
\item {\em actualLandedFlyingRatio}: Ratio between the number of ships landed and flying from the specific planet that evaluates the tree and player's total ships.
\end{itemize}

\noindent Finally, the possible \textit{decisions} are:

\begin{itemize}
\item {\em Attack Nearest (Neutral|Enemy|NotMy) Planet}: The objective is the nearest planet.
\item {\em Attack Weakest (Neutral|Enemy|NotMy) Planet}: The objective is the planet with less ships.
\item {\em Attack Wealthiest (Neutral|Enemy|NotMy) Planet}: The objective is the planet with higher lower rate.
\item {\em Attack Beneficial (Neutral|Enemy|NotMy) Planet}: The objective is the  more beneficial planet, that is, the one with highest growth rate divided by the number of ships.
\item {\em Attack Quickest (Neutral|Enemy|NotMy) Planet}: The objective is the planet easier to be conquered: the lowest product between the distance from the planet that executes the tree and the number of  ships in the objective planet.
\item {\em Attack (Neutral|Enemy|NotMy) Base}: The objective is the planet with more ships (that is, the base).
\item {\em  Attack Random Planet}.
\item {\em Reinforce Nearest Planet}: Reinforce the nearest player's planet to the planet that executes the tree.
\item {\em Reinforce Base}: Reinforce the player's planet with higher number of ships.
\item {\em Reinforce Wealthiest Planet}: Reinforce the player's planet with higher grown rate.
\item {\em Do nothing}.

\end{itemize}

\noindent An example of a possible decision tree is shown below. This example tree has a total of 5 nodes, with 2 decisions and 3 actions, and a depth of 3 levels.

\begin{verbatim}

if(myShipsLandedFlyingRatio < 0.796)
   if(actualMyShipsRatio < 0.201)
      attackWeakestNeutralPlanet(0.481);
   else
      attackNearestEnemyPlanet(0.913);
else
   attackNearestEnemyPlanet(0.819);

\end{verbatim}\\\\

\noindent The bot's behaviour is explained in Algorithm \ref{alg:turn}.

\begin{algorithm}[ht]
\begin{algorithmic}
%\SetAlgoLined
%\KwData{this text}
%\KwResult{how to write algorithm with \LaTeX2e }

\STATE // At the beginning of the execution the agent receives the tree
\STATE tree $\leftarrow$ readTree()
\WHILE{game not finished}
	\STATE // starts the turn
	\STATE calculateGlobalPlanets() // e.g. Base or Enemy Base
	\STATE calculateGlobalRatios() // e.g. myPlanetsEnemyRatio
	\FOR{Each p in PlayerPlanets}
		\STATE calculateLocalPlanets(p) // e.g. NearestNeutralPlanet to p
		\STATE calculateLocalRatios(p) //e.g actualMyShipsRatio
		\STATE executeTree(p,tree)  // Send a percentage of ships to destination
   \ENDFOR
\ENDWHILE

\end{algorithmic}
\caption{Pseudocode of the proposed agent. The same tree is used during all the agent's execution}
\label{alg:turn}
\end{algorithm}



%\COMMENT {In each turn}
%\LOOP
	
%	\STATE calculateGlobalPlanets()
%	\COMMENT{{\em for example Base, Enemy Base...}}
%	\STATE calculateGlobalRatios ()
%	\COMMENT {{\em for example myPlanetEnemyRatio, myShipsEnemyRatio...}}
%		\FOR{each Planet: p}
%			\STATE calculateLocalPlanets (p)
%			\COMMENT{{\em for example NearestNeutralPlanet to planet p}}
%			\STATE calculateLocalRatios (p)
%			\COMMENT{{\em for example actualMyShipsRatio}}
%			\STATE executeTree(p,tree)
%			\COMMENT{{\em Send a percentage of the ships to another planet}}
%		\ENDFOR
%\ENDLOOP

Next section explains one of the main components of the evolutionary process, i.e. the fitness function. As previously stated, three different functions have been implemented, which are used to evaluate the agent's performance during the matches. 
%FERGU: el "devoted to" no me mola xD Tampoco sé si el fitness es el "main component". Ah, no usar el "we"
% Antonio - arreglado
% la función de fitness es muy importante, pero ¿mucho mucho más que tipos de nodos que componen los árboles?
% ¿no quedaría mejor diciendo "...explains one of the main components of the..." ?     [pedro]
% AntonioDEF - hecho

%-----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% FITNESS FUNCTIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------------

\section{Fitness Functions}
\label{sec:fitness_functions}

% ---------------------------------------------------------------------

\subsection{Fitness based in Victories}
% "Turn based fitness"  [pedro] FERGU3: arreglado. Habría que llamarlo fitness basado en VICTORIAS, que es lo importante!!!!! (de hecho, el valor de las tablas muestra las VICTORIAS)
\label{subsec:fitness_turns}

%In previous works \cite{Genebot_CEC11}, a bot was evaluated always versus the same enemy (a reference bot), several times (in different maps), using a \textit{hierarchical fitness function}. FERGU4: comento esto que no sirve para nada.

This a variation of the hierarchical fitness considered in \cite{Genebot_CEC11}.
In this approach, an individual is better than another if it wins in a higher number of maps. In case of equality of victories, then the individual with more turns to be defeated (i.e. the stronger one) is considered as better. The maximum fitness in this work is, therefore, 5 victories and 0 turns. 
%FERGU: mover la ultima frase a cuando se hable de los parametros
% AntonioDEF - ¿cómo que 0 turns? ¿Cómo se va a ganar en 0 turnos?
For two bots, A and B, the fitness comparison (and therefore, their order inside the population) is defined as Algorithm \ref{alg:fitness_turns_positions} shows.

\begin{algorithm}[ht]
\begin{algorithmic}
        
\STATE $A,B \in Population$
\IF{A.victories $=$ B.victories}
	\IF{A.turns $>=$ B.turns}
		\STATE A is better than B
	\ELSE
		\STATE B is better than A
	\ENDIF
\ELSE
	\IF{A.victories $>$ B.victories}
		\STATE A is better than B
	\ELSE
		\STATE B is better than A
	\ENDIF
\ENDIF

\end{algorithmic}
\caption{Comparison between two individuals using hierarchical fitness.}
% AntonioDEF - ¿dejamos hierachical o victories?
\label{alg:fitness_turns_positions}
\end{algorithm}

% Antonio - Lo de 'is better' ¿qué significa a efectos de puntuación? ¿no sería mejor poner en lugar de "A is better than B" otra cosa?. Es que no queda claro cómo se calcula el fitness, que es un número con este algoritmo,que es más bien una comparativa entre individuos... ¿?
%FERGU2: esto sirve a la hora de ordenar los individuos, por ejemplo. He cambiado el pie de página y la frase de justo arriba.

In this fitness, we are only interested in the final result (position and number of turns). We do not include in the analysis how the bot has reached them. The problem of this function is that the consideration of two different terms makes it difficult the comparison between different evaluations. 

Thus, in this work two additional evaluation functions have been proposed in order to let easier and fairer comparison methods between bots, trying to add another factor in order to reduce the influence of noise \cite{Mora_noisy_jcst}.
%FERGU: he puesto un therefore para enlazar
% Antonio - lo he cambiado todo. XD
Both of them are based in the percentage of ships belonging to each player in every turn. They are normalized considering the total amount of ships in the game for that turn (including neutrals ships in neutral planets), so for each player, there is a different {$cloud$} of ships.
% as Fig.\ref{figura:nubecita} shows. 
%FERGU: explicar la figura, que no queda na claro.
% Antonio - Eso digo yo... la voy a quitar
%
%\begin{figure}[ht]
%\begin{center}
%  \epsfig{file=imags/nubecita.eps,width=8cm}
%\end{center}
%\caption{Representation of the number of ships of each bot in each turn} 
%\label{figura:nubecita}
%\end{figure}
Below, are described the two alternatives to deal with this cloud of points for the fitness function: the use of slopes and areas.

% ---------------------------------------------------------------------

\subsection{Fitness based in Slope}
%  "slope based fitness"   [pedro]
\label{subsec:fitness_slope}

% In this case, a square regression .......      [pedro]
In this case, a square regression analysis is computed in order to transform the cloud of points into a simple line. The line is represented as {$y = \alpha \times x + \beta $}, where {$\alpha$} and {$\beta$} are calculated as shown in Equations \ref{eq:alpha} and \ref{eq:beta}, computing a least squares regression. For every bot in the simulation we calculate $\alpha$ and ($slope$). This $slope$ is the fitness of every bot for that simulation. 

%A graphical example can be seen in Fig. \ref{figura:nubecita:pendiente}.

%%Antonio, pon esto junto si puedes, en la misma fila, que con subfigure no funciona :(
\begin{equation}
\label{eq:alpha}
        \alpha = \frac{\sum_{i=1}^{n}(X_{i} - \bar{X_{i}})(Y_{i} - \bar{Y_{i}})}{\sum_{i=1}^{n}(X_{i} - \bar{X_{i}})^{2}}
\end{equation}

\begin{equation}
\label{eq:beta}
        \beta = \bar{Y}-\alpha\bar{X}
\end{equation}


%\begin{figure}[h]
%\centering
%\hspace*{-1in}
%  \epsfig{file=imags/nubecita_pendiente.eps,width=0.6
%  \textwidth}
%\caption{Fitness based in Slope: number of ships of every bot in each turn}
%\label{figura:nubecita:pendiente}
%\end{figure}


%\begin{figure}[h]
%\centering
%\hspace*{-1in}
%\begin{subfigure}[H]{0.4\textwidth}
%	\large
%    \begin{equation}
%        \alpha = \frac{\sum_{i=1}^{n}(X_{i} - \bar{X_{i}})(Y_{i} - \bar{Y_{i}})}{\sum_{i=1}^{n}(X_{i} - \bar{X_{i}})^{2}}
%    \end{equation}
%    \begin{equation}
%        \beta = \bar{Y}-\alpha\bar{X}
%    \end{equation}
%    \caption{Least Squares Regression}
%    \label{equation:LeastSquares}
%\end{subfigure}
%\hfill
%\hspace*{0.2in}
%\begin{subfigure}[H]{0.7\textwidth}
%\begin{center}
%  \epsfig{file=imagenes/nubecita_pendiente.eps,width=1.1\textwidth}
%\end{center}
%\caption{Number of ships of every bot in each turn} %Maribel, cambio if por of
%\label{figura:nubecita:pendiente}
%\end{subfigure}
%\caption{Fitness based in Slope}
%\end{figure}

Theoretical maximum and minimum values are set for this fitness. An optimum bot that wins in the first turn, has an ideal slope of {$1$}, so this is the maximum value of our fitness. On the other hand, a bot that loses in the first turn,  has a slope of {$-1$}. Thus, if we calculate the $slope$, we know if the bot {$WINs$} ({$slope>0$}) or {$LOSEs$} {$slope<0$}. 
The values of the different battles are summed to compute the global $slope$. %FERGU: por qué las mayúsculas? Explicar mejor esta última frase
Then, the bot with the highest value will be the best is each turn or battle. 

%Several evaluations in different maps was using, so it's need operate with fitness. In that case, only sum the slope of all the evaluations of the bot. Maribel, esto ya lo has dicho antes y además lía más la cosa así que lo he eliminado. Además expresiones como "was using" están mal, qué quieres decir? fue usando? eso en inglés no se dice.

% ---------------------------------------------------------------------

\subsection{Fitness based in Area}
% "area based fitness"    [pedro]
\label{sec:fitness}

In this function, the integral of the curve of the bot's live-line is used for calculating the area that is `covered' by the fitness cloud of points (see Equation \ref{eq:area}). This {$area$} is normalized considering the number of turns, and thus it represents the average percentage of ships during the battle for each player. 
%An example is shown in Fig. \ref{figura:nubecita:area}.

\begin{equation}
        area=\frac{\int_{0}^{t}\%ships(x)dx}{t}
    \label{eq:area}
\end{equation}

% \begin{figure}[h]
% \begin{center}
%   \epsfig{file=imagenes/nubecita_integral.eps,width=0.7\textwidth}
% \end{center}
% \caption{Fitness based in Area. Example of area under the live-line curve.}
% \label{figura:nubecita:area}
% \end{figure}

%\begin{figure}[h]
%\centering
%\hspace*{-1in}
%\begin{subfigure}[H]{0.4\textwidth}
%	\large
%    \begin{equation}
%        area=\frac{\int_{0}^{t}\%ships(x)dx}{t}
%    \end{equation}
%    \caption{Calculus of the area}
%    \label{equation:area}
%\end{subfigure}
%\begin{subfigure}[H]{0.6\textwidth}
%\begin{center}
%  \epsfig{file=imagenes/nubecita_integral.eps,width=0.6\textwidth}
%\end{center}
%\caption{Example of area under the live-line curve.} 
%\label{figura:nubecita:area}
%\end{subfigure}
%\caption{Fitness based in Area}
%\end{figure}

As in previous case, maximum and minimum values has been set for this fitness. If an optimal bot wins in the first turn, the area of each live-line is close to {$1$}, so this is the maximum value of the fitness. Otherwise, if a bot loses in the first turn, its live-line area is close to {$0$}. In this case, we do not extract additional about which bot wins the battle, because the area of the live-line is not related with the winner of the battle. Thus, we are losing some information. 
%FERGU: y por lo tanto... blablabla. No usar el we.
% AntonioDEF - completar esto (ANTARES escribe y FERGU revisa) :D

%-----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% EXPERIMENTAL SETUP %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------------

\section{Experimental Setup}
\label{sec:experiments}

Sub-tree crossover and 1-node mutation evolutionary operators have been used, following other researchers' proposals that have used these operators obtaining good results \cite{Esparcia2013GPunreal}. In this case, the mutation randomly changes the decision of a node or mutate the value with a step-size of 0.25 (an adequate value empirically tested). Each configuration is executed 30 times, with a population of 32 individuals and a 2-tournament selector for a pool of 16 parents.

To test each individual during the evolution, a battle with a previously created bot is performed in 5 different (but representative) maps provided by Google is played. 
%Hierarchical fitness is used, as proposed in \cite{Genebot_CEC11}. Thus, an individual is better than another if it wins in a higher number of maps. In case of equality of victories, then the individual with more turns to be defeated (i.e. the stronger one) is considered better. The maximum fitness is, therefore 5 victories and 0 turns. 
Also, as proposed by \cite{Genebot_CEC11}, and due to the noisy fitness effect, all individuals are re-evaluated in every generation.


A publicly available bot has been chosen for our experiments\footnote{It can be downloaded from \url{https://github.com/deantares/genebot}}. The bot to confront is {\em GeneBot}, proposed in \cite{Genebot_CEC11}. As stated, this bot was trained using a GA to optimize the 8 parameters that conforms a set of hand-made rules, obtained from an expert human player experience.  Table \ref{tab:parameters} summarizes all the parameters used.

\begin{table*}
\begin{center}
\begin{tabular}{|c|c|}
\hline
{\em Parameter Name} & {\em Value} \\\hline \hline
Population size & 32 \\\hline
Crossover type & Sub-tree crossover \\ \hline
Crossover rate & 0.5\\ \hline
Mutation  & 1-node mutation\\ \hline
Mutation step-size & 0.25 \\ \hline
Selection & 2-tournament \\ \hline
Replacement & Steady-state\\ \hline
Stop criterion & 50 generations \\ \hline
Maximum Tree Depth & 7  \\ \hline %FERGU: quitados distintos tamaños
Runs per configuration & 30 \\ \hline
Evaluation & Playing versus GeneBot \cite{Genebot_CEC11}  \\ \hline 
%FERGU: Antares, confirmalo
% Antonio - Al final creo que sólo Genebot %FERGU2: quitado
Maps used in each evaluation & map76, map69, map7, map11, map26 
% AntonioDEF - los mapas han sido esos, ¿no?
\\ \hline
\end{tabular}
\caption{Parameters used in the experiments.}
\label{tab:parameters}
\end{center}
\end{table*}

% ¿cómo se han obtenido los valores de esos parámetros?   [pedro]
% AntonioDEF - habla un poco de los parámetros Fergu. Dí que se ahn obtenido por systematic experimentation y justifica la profundidad de 7, pero sin decir nada del artículo del EVO*. XD
% que es un valor habitual, que dado el número de antecedentes es un valor justo, no sé...
 

After all the executions we have evaluated the obtained best individuals in all runs confronting to the other bots in a larger set of maps to study the behaviour of the algorithm and how good are the obtained bots versus enemies and maps that have not been used for training.

%FERGU3: ANTARES, confirma como has hecho esto ANTARES: Confirmo, se ha hecho la prueba en 4 mapas adicionales desconocidos y frente a oponentes desconocidos a su vez (los que han generado los métodos)


The used framework is OSGiLiath, a service-oriented evolutionary framework. %\cite{Garcia13Service}. 
The generated tree is compiled in real-time and injected in the agent's code using Javassist \footnote{\url{www.javassist.org}} library. All the source code used in this work is available under a LGPL V3 License in \url{http://www.osgiliath.org}.


%-------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% RESULTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------------------------------
\section{Results}

Table \ref{tab:results3config} shows the obtained results after executing 20 times each approach, i.e. GP algorithm using every fitness implementation. 
% AntonioDEF - aclaro un poco más
Although these fitness are not comparable, as they obviously apply different metrics, the \textit{Victory-based} fitness achieves values near to the optimum (5) at the end of the run (look at the best individual and average population values). The \textit{Slope} and \textit{Area} fitness yield results under their theoretical optimum, 
% AntonioDEF - ¿cuál es su óptimo teórico?
as they depend on more information ranges (variation in the number of ships).
\textit{Area} obtains slightly better values than \textit{Slope}.

% NI IDEA PORQUE NO LOS ENTIENDO XDD (FERGU3). ARREGLADLO
% AntonioDEF - yo tampoco sabría justificarlo. ANTARES dí algo y lo traduce Fergu. ;D


\begin{table*}
\centering{
\begin{tabular}{|c|c|c|} \hline            
		& Average best fitness	&	Average population fitness	\\ \hline \hline
Victory	& 4,761 $\pm$	0,624	&	4,345	$\pm$	0,78 \\ \hline
Slope	& 2,296	$\pm$	0,486	&	2,103	$\pm$	0,486 \\ \hline
% AntonioDEF - la misma desv. típica? huele a copy-paste mal hecho. XD
Area	& 2,838	$\pm$	1,198	&	2,347	$\pm$	0,949 \\ \hline


\end{tabular}
\caption{Average results obtained for each approach at the end of the runs. Everyone has been tested 20 times.}
\label{tab:results3config}
}
\end{table*}

Even though the \textit{Victory-based} fitness yields better results (near optimal), to do a fair comparison, we have confronted the 20 best bots obtained with each configuration (one per run) against GeneBot. However these matches have been performed in 9 different maps than those where the bots were trained (evolved). These maps, provided by Google, are considered as representative, because they have different features to promote a wide set of strategies, i.e. different distributions of planets, sizes and number of initial ships. 

This experiment has been conducted in order to validate if the bots obtained by the proposed approaches can be competitive in terms of quality in maps not used for evolution/evaluation. Results are shown in Figure \ref{figura:boxplotvictoriesgenebot}. As it can be seen, again the \textit{Victory-based} fitness achieves significantly better results than the other methods.


\begin{figure}[ht]
 \begin{center}
   \includegraphics[width=7cm]{imags/boxplot_victoria_bot_por_metodo_contra_genebot.eps}
 \end{center}
 \caption{}Boxplot of average percentage of victories of the bots obtained by each approach vs. GeneBot. 9 different matches have been performed per bot in different maps.
% AntonioDEF - ¿average percentage o sólo percentage? (ANTARES)
% Toy un poco dormío ya para pensar. :/
 \label{figura:boxplotvictoriesgenebot}
 \end{figure}

Finally, an additional experiment has been conducted, proposing a direct comparison between the three methods. To this end each of the best individuals obtained per approach has been tested competing against all the rest (in a vs 1 battles) in 9 matches per pair of bots, one per representative map.
% AntonioDEF - ¿en cuantos mapas? o sea, ¿cuántos combates hacía cada pareja?
% revisad si está bien así. (ANTARES)

This allows a comparison with a wider number of bots, and also, allows the analysis of their behavior against rivals not previously used during training (as in the experiment above).
The boxplots of the average percentage of victories from the best bots obtained by each method are shown in Figure \ref{figura:boxplotvictories}. It is clear from the image that the \textit{Slope} fitness does not get good results with respect to the other methods. This can be explained because the this fitness loses information during the run, in comparison with the others, obtaining bots less aggressive. 
% AntonioDEF - ¿Qué información pierde?¿Esto de dónde os lo habeis sacado?¿Cómo lo sabéis? (ANTARES)
The \textit{Victory-based} fitness achieves better results in average, being also more robust (small standard deviation). However, looking at the \textit{Area} fitness results, they outperform several times the \textit{Victory} results, obtaining more victories.
%.... NO SE COMO JUSTIFICAR QUE ESTO ES MEJOR :/// (FERGU3). ç

%FERGU3 ANTARES, CUANTAS EJECUCIONES POR BOT Y EN QUE MAPAS?????  
%Los mapas son & 7 & 11 & 13 & 26 & 32 & 64 & 69 & 76 & 87 \ y os explico un poco las cuentas.
%Se han hecho 35721 batallas en 9 mapas con 63 bots (21 por método).
%Cada bot se ha enfrentado con los otros bots en 9 mapas (63 enfrentamientos) más luego él mismo ha servido como rival para los otros bots (otros 63 combates)
% AntonioDEF - esto supongo que es de aquí

 \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=7cm]{imags/boxplot_victoria_bot_por_metodo.eps}
 \end{center}
 \caption{}Boxplot of average percentage of victories of the best bots obtained by each method vs. the rest.
% Los boxplot son para comparar. Los tenéis que meter a los tres en el mismo. Si no vais a comparar, no tenéis que hacer ningún boxplot. - JJ FERGU4: lo que te dije, Antares. 
 \label{figura:boxplotvictories}
 \end{figure}


There is still a final remark, which concerns to the percentage of draw matches.
As it can be seen in Table \ref{tab:ties}. The \textit{Victory-based} fitness achieves more draws against bots of the same type than the other approaches. This can be explained because they use less information to perform the evolution, obtaining quite similar behaviours. This is also reinforced by previous results in which the bots of this fitness seemed to perform similarly, obtaining close number of victories and thus, getting small standard deviation values.
% AntonioDEF - a ver si os mola esta conclusión. (ANTARES)

\begin{table}
\centering{
\begin{tabular}{|c|c|c|c|} \hline            
		& Victory		&	Area 		& Slope	\\ \hline \hline
Victory & 62.06\%	& 23.15 \% 	&	12.47 \% \\ \hline
Slope	& 	-		& 19.07 \%	&	12.50 \%		\\ \hline
Area	& 	-		&	-		&	11.87 \% \\ \hline


\end{tabular}
\caption{Percentage of draw matches between bots per fitness approach.}
\label{tab:ties}
}
\end{table}



%FERGU: he borrado las tablas, que aquí no se usan. Dejo el resto comentado, por si queréis "inspiraros". Vamos, que no se reutiliza nada del trabajo del Evostar

%As can be seen, the average population fitness versus Genebot is nearest to the optimum than versus Exp-Genebot, even with the lowest depth. Highest performance in the population is also with the depth of 3 levels. On the contrary, confronting with Exp-Genebot the configuration with unlimited depth achieves better results. This make sense as more decisions should be taken because the enemy can be different in each map.

%In the second experiment, we have confronted the 30 bots obtained in each configuration again with Genebot and Exp-Genebot, but in the 100 maps provided by Google. This experiment has been used to validate if the obtained individuals of the proposed method can be competitive in terms of quality in maps not used for evaluation. Results are shown in Table \ref{tab:allmaps} and boxplots in Figure \ref{fig:victories}. %FERGU: si se va a hacer esto, esta frase se puede usar

%It can be seen that in average, the bots produced by the proposed algorithm perform equal or better than the best obtained by the previous authors. Note that, even obtaining individuals with maximum fitness (5 victories) that have been kept in the population several generations (as presented before in Tables \ref{tab:resultsGenebot} and \ref{tab:resultsExpgenebot}) cannot be representative of a extremely good bot in a wider set of maps that have not been used for training. As the distributions are not normalized, a Kruskal-Wallis test has been used, obtaining significant differences in turns for the experiment versus Genebot (p-value = 0.0028) and victories in Exp-genebot (p-value = 0.02681). Therefore, there are differences using a maximum depth in the generation of bots. In both configurations, the trees created with 7 levels of depth as maximum have obtained the better results.

%To explain why results versus Genebot (a weaker bot than Exp-Genebot) are slightly worse than versus Exp-Genebot, even when the best individuals produced by the GP have higher fitness, it is necessary to analyse how the best individual and the population are being evolved. Figure \ref{fig:gens} shows that best individual using Genebot reaches the optimal before Exp-Genebot, and also the average population converges quicker. This could lead to over-specialization: the generated bots are over-trained to win in the five maps. This is due because these individuals are being re-evaluated, and therefore, they are still changing after they have reached the optimal.



%\begin{table*}
%\centering{
%\begin{tabular}{|c|c|c|c|c|c|c|} \hline
   
%{\em Configuration}     &    {\em Average maps won}  &    {\em Average turns}     \\ \hline
%                   \multicolumn{3}{|c|}{Versus Genebot}    \\ \hline
% Depth 3          &   47.033 $\pm$ 10.001 &   133.371 $\pm$   16.34    \\ \hline
% Depth 7          &   48.9 $\pm$ 10.21    &   \textbf{141.386} $\pm$  15.54   \\ \hline
% Unlimited Depth  &   50.23 $\pm$ 11.40   &   133.916   $\pm$   10.55    \\ \hline
%       \multicolumn{3}{|c|}{Versus Exp-Genebot}                          \\ \hline              
% Depth 3          &   52.367 $\pm$ 13.39 &  191.051 $\pm$ 67.79 \\ \hline
% Depth 7          &   \textbf{58.867} $\pm$ 7.35  &  174.694$\pm$ 47.50 \\ \hline
% Unlimited Depth  &   52.3 $\pm$ 11.57   &  197.492 $\pm$ 72.30 \\ \hline 

%\end{tabular}


%\caption{Results confronting the 30 best bots attained from each configuration in the 100 maps each.}
%\label{tab:allmaps}
%}
%\end{table*}

%\begin{figure}[htb]
%\centering

%\subfigure[Victories]{
%   \includegraphics[scale =0.30] {imags/victories.eps}
%   \label{fig:subfig1}
% }
%\subfigure[Turns]{
%   \includegraphics[scale =0.30] {imags/turns.eps}
%   \label{fig:subfig2}
% }
%\caption{Average of executing the 30 best bots in each configuration (3, 7 and U) versus Genebot (G) and Exp-Genebot (E).}

%\label{fig:victories}
%\end{figure}

%\begin{figure}[htb]
%\centering
%\includegraphics[scale =0.60] {imags/generations.eps}
%\caption{Evolution of the best individual and the average population during one run for depth 7 versus Genebot and Exp-Genebot.}
%\label{fig:gens}
%\end{figure}


%-----------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% CONCLUSIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------- 
\section{Conclusions}
\label{sec:conclusion}

%This work presents a Genetic Programming algorithm that generates agents for playing the Planet Wars game. A number of possible actions to perform and decision variables have been presented. A competitive bot available in the literature (Genebot) has been used to calculate the fitness of the generated individuals. This bot was the best obtained from several runs and the behaviour to be optimized was extracted from human expertise. %FERGU: confirmarlo %FERGU2: quitado exp-genebot y reescrito a continuación

%Genetic programming is a method that can help to create competitive bots for RTS games. % ¿Eso es una conclusión? ¿Lo habéis concluido en el paper? Empezad las conclusiones por "en este trabajo nos proponíamos probar..." - JJ
The objective of this work is to validate if using Genetic Programming can create competitive bots for RTS games, and study the behaviour of different fitness functions, as they can affect directly to the creation of these bots. Three different fitness functions have been compared to generate bots for the Planet Wars game. A competitive bot available in the literature (GeneBot) has been used to evaluate the generated individuals (fighting against it). This bot was the best one obtained in an evolutionary process which optimized different parameters inside a human-designed behavioural engine. 

Different information of the run of the game is taken into account in these functions to obtain a metric to guide the evolution. %Although a turn-based fitness behaves more robustly than other methods, the area-based fitness achieves better percentage of victories in more produced robots. % robots o bots? - JJ
%FERGU3: esto ultimo lo estoy escribiendo a las 3 de la mañana y no sé que pongo
% Por lo que más queráis, revisad el inglés - JJ FERGU4: ya, vaya horror.
The results show differences depending on the fitness used: a victory-based fitness that prioritizes the number of victories generate better bots in average than fitness that take into account the number of spaceships during all the run of the battle. This can be explained because this fitness exploits the individuals to generate more aggressive bots. However, their performance decreases confronting versus different types of bots.

% La conclusión tiene que estar relacionada con el título. ¿Qué habéis concluido con respecto a GP? ¿Qué tipo de GP? ¿Qué población? ¿No decís nada más que del fitness? - JJ FERGU

%Three different maximum depth for the trees have been used: 3, 7 and unlimited. Results show that the best individuals outperform these agents during the evolution in all configurations. These individuals have been tested against a larger set of maps not previously used during the evolution, obtaining equivalent or better results than Genebot and Exp-Genebot. FERGU: estos no son los resultados

%Results show that it is important to choose carefully the fitness that is going to be used to evaluate the evolved bots and to validate it using some kind of benchmark. % Some kind? Which kind? - JJ
% ¿Eso es lo único que muestran? - JJ FERGU4: joer, menuda mierda de frase he escrito...

In future work, other rules will be added to the proposed algorithm (for example, ones analysing the map) and more competitive enemies will be used. In addition, the approach could be implemented and tested in more complex RTS games, such as Starcraft, or even in different videogames like Unreal\texttrademark~ or Super Mario\texttrademark~.

%******************************************************************************

\section*{Agradecimientos}
This work has been supported in part by FPU research grant AP2009-2942 and projects SIPESCA (G-GI3000/IDIF, under Programa Operativo FEDER de Andalucía 2007-2013), CANUBE (CEI2013-P-14), ANYSELF (TIN2011-28627-C04-02) and PYR-2014-17 included in GENIL - CEI BIOTIC (Granada).

\nocite{*}
\bibliographystyle{maeb2015}

\begin{thebibliography}{1}

\bibitem{ahlquist_game_ai08}
Ahlquist, J.B., Novak, J.:
\newblock Game Artificial Intelligence.
\newblock Game Development Essentials. Thompson Learning, Canada (2008)

\bibitem{Genebot_CEC11}
Fernández-Ares, A., Mora, A.M., Merelo, J.J., García-Sánchez, P., Fernandes,
  C.:
\newblock Optimizing player behavior in a real-time strategy game using
  evolutionary algorithms.
\newblock In: IEEE Congress on Evolutionary Computation (CEC 2011). (2011)
  2017--2024

\bibitem{ExpGenebot_CIG2012}
Fern{\'a}ndez-Ares, A., Garc\'{\i}a-S{\'a}nchez, P., Mora, A.M., Merelo, J.J.:
\newblock Adaptive bots for real-time strategy games via map characterization.
\newblock In: 2012 IEEE Conference on Computational Intelligence and Games, CIG
  2012, IEEE (2012)  417--721

\bibitem{Lara_PCG_PlanetWars14}
Lara-Cabrera, R., Cotta, C., Leiva, A.J.F.:
\newblock On balance and dynamism in procedural content generation with
  self-adaptive evolutionary algorithms.
\newblock Natural Computing \textbf{13}(2) (2014)  157--168

\bibitem{Mora_noisy_jcst}
Mora, A.M., Fern{\'a}ndez-Ares, A., Merelo, J.J., Garc\'{\i}a-S{\'a}nchez, P.,
  Fernandes, C.M.:
\newblock Effect of noisy fitness in real-time strategy games player behaviour
  optimisation using evolutionary algorithms.
\newblock J. Comput. Sci. Technol. \textbf{27}(5) (2012)  1007--1023

\bibitem{wilcoxon:ga}
Guervós;, J.J.M.:
\newblock Using a wilcoxon-test based partial order for selection in
  evolutionary algorithms with noisy fitness.
\newblock Technical report, GeNeura group, university of Granada (2014)

\bibitem{GP_Koza92}
Koza, J.R.:
\newblock Genetic Programming: On the programming of computers by means of
  natural selection.
\newblock MIT Press, Cambridge, MA (1992)

\bibitem{Lara2013review}
Lara-Cabrera, R., Cotta, C., Fern{\'a}ndez-Leiva, A.J.:
\newblock A review of computational intelligence in rts games.
\newblock In: FOCI, IEEE (2013)  114--121

\bibitem{Esparcia10FPS}
Esparcia-Alc{\'a}zar, A.I., Mart\'{\i}nez-Garc\'{\i}a, A.I., Mora, A.M.,
  Merelo, J.J., Garc\'{\i}a-S{\'a}nchez, P.:
\newblock Controlling bots in a first person shooter game using genetic
  algorithms.
\newblock In: IEEE Congress on Evolutionary Computation, IEEE (2010)  1--8

\bibitem{Kenneth2005neuroevolution}
Stanley, K.O., Bryant, B.D., Miikkulainen, R.:
\newblock Real-time neuroevolution in the nero video game.
\newblock IEEE Transactions on Evolutionary Computation (2005)  653--668

\bibitem{Mahlmann2012MapGeneration}
Mahlmann, T., Togelius, J., Yannakakis, G.N.:
\newblock Spicing up map generation.
\newblock In: Proceedings of the 2012t European conference on Applications of
  Evolutionary Computation. EvoApplications'12, Berlin, Heidelberg,
  Springer-Verlag (2012)  224--233

\bibitem{Sipper2007gameplaying}
Sipper, M., Azaria, Y., Hauptman, A., Shichel, Y.:
\newblock Designing an evolutionary strategizing machine for game playing and
  beyond.
\newblock IEEE Transactions on Systems, Man and Cybernetics Part C:
  Applications and Reviews \textbf{37}(4) (2007)  583--593

\bibitem{Elyasaf2012FreeCell}
Elyasaf, A., Hauptman, A., Sipper, M.:
\newblock Evolutionary design of freecell solvers.
\newblock IEEE Transactions on Computational Intelligence and AI in Games
  \textbf{4}(4) (2012)  270--281

\bibitem{Benbassat2012Reversi}
Benbassat, A., Sipper, M.:
\newblock Evolving both search and strategy for reversi players using genetic
  programming.
\newblock (2012)  47--54

\bibitem{Brandstetter2012PacMan}
Brandstetter, M., Ahmadi, S.:
\newblock Reactive control of ms. pac man using information retrieval based on
  genetic programming.
\newblock (2012)  250--256

\bibitem{Wittkamp2007spoof}
Wittkamp, M., Barone, L., While, L.:
\newblock A comparison of genetic programming and look-up table learning for
  the game of spoof.
\newblock (2007)  63--71

\bibitem{Esparcia2013GPunreal}
Esparcia-Alc{\'a}zar, A.I., Moravec, J.:
\newblock Fitness approximation for bot evolution in genetic programming.
\newblock Soft Computing \textbf{17}(8) (2013)  1479--1487

\bibitem{pathfinding_GP_RTS}
Strom, R.:
\newblock Evolving pathfinding algorithms using genetic programming.
\newblock Gamasutra Webpage (Accessed on 25/05/2014)
  URL=http://www.gamasutra.com/view/feature/131147/ evolving\_pathfinding\_algorithms\_.php?print=1.

\bibitem{KeaveneyO09_GP_RTS}
Keaveney, D., O'Riordan, C.:
\newblock Evolving robust strategies for an abstract real-time strategy game.
\newblock In Lanzi, P.L., ed.: CIG, IEEE (2009)  371--378

\bibitem{Genebot-IWANN2011}
Fernández-Ares, A., Mora, A.M., Merelo, J.J., García-Sánchez, P., Fernandes,
  C.M.:
\newblock Optimizing strategy parameters in a game bot.
\newblock In: Proc. 11th International Work-Conference on Artificial Neural
  Networks, IWANN 2011, Springer, LNCS, vol. 6692 (2011)  325--332

\bibitem{genebot-evo12}
Mora, A.M., Fernández-Ares, A., Merelo, J.J., García-Sánchez, P.:
\newblock Dealing with noisy fitness in the design of a {RTS} game bot.
\newblock In: Proc. Applications of Evolutionary Computing: EvoApplications
  2012, Springer, LNCS, vol. 7248 (2012)  234--244

\bibitem{Co-Genebot_EVO2014}
Fernández-Ares, A.J., Mora, A.M., Arenas, M.G., García-Sánchez, P., Merelo,
  J.J., Castillo, P.A.:
\newblock Co-evolutionary optimization of autonomous agents in a real-time
  strategy game.
\newblock In: Proc. Applications of Evolutionary Computing: EvoApplications
  2014, Springer (2014)  In Press

\end{thebibliography}
\end{document}
